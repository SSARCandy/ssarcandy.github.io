<!DOCTYPE html><html><head><meta charset="utf-8"><title>Deep CORAL: Correlation Alignment for Deep Domain Adaptation | SSARCandy&#39;s Blog</title><meta name="google-site-verification" content="uUti8Shw9zIz5j2Rs_nwYT9VusmIOXijBuf2bBNYm78"><meta http-equiv="Cache-control" content="public"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#673AB7"><meta name="keywords" content="paper,python,pytorch,machine learning"><meta name="description" content="雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 doma"><meta property="og:type" content="article"><meta property="og:title" content="Deep CORAL: Correlation Alignment for Deep Domain Adaptation"><meta property="og:url" content="https://ssarcandy.tw/2017/10/31/deep-coral/index.html"><meta property="og:site_name" content="SSARCandy's Blog"><meta property="og:description" content="雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 doma"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/1.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/2.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/3.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/4.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/5.png"><meta property="og:updated_time" content="2019-03-15T00:11:01.037Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Deep CORAL: Correlation Alignment for Deep Domain Adaptation"><meta name="twitter:description" content="雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 doma"><meta name="twitter:image" content="https://ssarcandy.tw/img/2017-10-31/1.png"><link rel="alternative" href="/atom.xml" title="SSARCandy&#39;s Blog" type="application/atom+xml"><meta name="summary" content="&lt;p&gt;雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。&lt;br&gt;Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 domain 的資料上。&lt;/p&gt;
&lt;p&gt;這篇論文&lt;sup&gt;[1]&lt;/sup&gt; (Deep CORAL: Correlation Alignment for Deep Domain Adaptation, B Sun, K Saenko, ECCV 2016) 提出一個 CORAL loss，通過對 source domain 和 target domain 進行線性變換來將他們各自的的二階統計量對齊 (minimizing the difference between source/target correlations).&lt;/p&gt;"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/style.css"><link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"><link rel="manifest" href="/web-app-manifest.json"><style type="text/css">#menu,#menu .avatar{box-shadow:0 2px 5px 0 rgba(0,0,0,.16),0 2px 10px 0 rgba(0,0,0,.12)}#main,#menu,body,html{min-height:100%}.icon{display:inline-block;font-family:FontAwesome;font-size:1em;text-rendering:auto;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.icon-lg{font-size:24px;line-height:.75em;vertical-align:-15%}.icon-2x{font-size:2em}.icon-eye:before{content:"\f06e"}.icon-chevron-up:before{content:"\f077"}.icon-github:before{content:"\f09b"}.icon-rss-square:before{content:"\f143"}*,::after,::before{-webkit-box-sizing:border-box;box-sizing:border-box}::-webkit-scrollbar{width:5px;height:5px}::-webkit-scrollbar-track{background-color:transparent}::-webkit-scrollbar-thumb{background-color:#b6b6b6}::-webkit-scrollbar-thumb:hover{background-color:#727272}::-webkit-scrollbar-thumb:active{background-color:#727272}html{background-color:#fff;color:#212121;font-size:15px;line-height:1.5}body,html{position:relative;height:100%}body,input{font:400 1em Roboto,'Helvetica Neue',Helvetica,'Microsoft Yahei',Arial}.material-icons,i{font-style:normal}body,h1,h4,h5,input,p{margin:0}ol,p,ul{margin:12px 0}input{padding:0}ol,ul{padding:0 0 0 32px}ol ol{margin-top:0;margin-bottom:0}img{max-width:100%;border:none}img,input{vertical-align:middle}h1,h4,h5{font-weight:400}h1{font-size:44px;line-height:48px}h4{font-size:20px;line-height:28px}h5{font-size:16px;line-height:24px}a{background-color:transparent;background-image:none;color:#673ab7;text-decoration:none}.ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.flex-row,.flex-row-vertical{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex}.flex-row-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column}.flex-row>*{display:block}.flex-col{-webkit-box-flex:1;-webkit-flex:1;-ms-flex:1;flex:1}#loading{position:fixed;top:0;left:-100%;z-index:70;width:100%;height:3px;background:#e040fb}#main,.container,.waves-effect{position:relative}#loading.active{-webkit-animation:loading-anim 1s ease-in-out infinite;animation:loading-anim 1s ease-in-out infinite}@-webkit-keyframes loading-anim{to{-webkit-transform:translateX(200%);transform:translateX(200%)}}@keyframes loading-anim{to{-webkit-transform:translateX(200%);transform:translateX(200%)}}.waves-effect{display:inline-block;overflow:hidden}#menu .avatar,.waves-block{display:block}.waves-circle{-webkit-transform:translateZ(0);-moz-transform:translateZ(0);-ms-transform:translateZ(0);-o-transform:translateZ(0);transform:translateZ(0);text-align:center;width:2.5em;height:2.5em;line-height:2.5em;border-radius:50%}#menu,.mask{position:fixed;top:0}#menu-off{position:absolute;top:0;left:100%}#menu{left:0;bottom:0;z-index:66;width:240px;background:#fff}#menu.hide{-webkit-transform:translateX(-100%);transform:translateX(-100%)}#menu.hide #menu-off{-webkit-transform:scale(0);transform:scale(0)}#menu.hide+#main{padding-left:0}#menu .inner{position:relative;height:100%}#menu .brand-wrap{background:url(/img/brand.png);background-size:100% 100%}#menu .brand{padding:40px 20px 2em;background:rgba(103,58,183,.5)}#menu .avatar{width:80px;height:80px;border:2px solid #fff;border-radius:50%;overflow:hidden}#menu .introduce{margin:1em 0 0;color:#fff}#menu .mail{display:inline-block;padding-top:4px;color:#d1c4e9;font-size:13px}#menu .nav{margin:0;padding:12px 0;list-style:none;line-height:44px;overflow-y:auto}#menu .nav li{padding:0 20px;display:block}#menu .nav li .icon{position:absolute;top:0;left:20px;line-height:44px}#menu .nav a{display:block;padding-left:48px;height:44px;font-weight:500;color:#727272}#menu .footer{position:relative;width:100%;font-size:12px;font-family:inherit;padding:10px 20px;border-top:1px solid #dadada}#menu .footer .rss{position:absolute;right:20px;bottom:20px;color:#e64a19}#main{padding-left:240px;background:#fbfbfe}.body-wrap{padding:30px 0 40px}.container{width:960px;margin:0 auto}.mask{left:0;bottom:0;z-index:88;width:100%;height:100%;background:#000;opacity:.5;display:none}#gotop,.top-header{z-index:30;position:fixed}.content-header,.top-header{color:#fff;background:#673ab7}@media screen and (max-width:1240px){.container{width:760px}#menu-off{display:none}#menu{z-index:99;box-shadow:none;-webkit-transform:translateX(-100%);transform:translateX(-100%)}#main{padding-left:0}}#gotop,.search-panel{box-shadow:0 2px 5px 0 rgba(0,0,0,.16),0 2px 10px 0 rgba(0,0,0,.12)}@media screen and (max-width:760px){.container{width:100%;padding:20px 0;overflow-x:hidden}#menu .brand{padding-top:20px;padding-bottom:1em}#menu .nav{line-height:36px}#menu .nav a{height:36px}}.header-icon,.header-title{line-height:56px;text-align:center}.top-header{left:0;top:0;width:100%;height:56px}#gotop,.header-icon{width:56px;height:56px}.header-icon{color:#fff}.header-title{visibility:hidden;font-weight:400;font-size:16px;white-space:nowrap}.content-header{margin-left:-240px;padding:104px 16px 48px 256px}.content-header .subtitle{padding-top:6px;color:#d1c4e9}@media screen and (max-width:1240px){.header-title{text-align:left}}@media screen and (max-width:760px){.content-header{margin:0;padding:72px 16px 20px}.content-header .author{font-size:24px;line-height:30px}.content-header .subtitle{font-size:14px;line-height:20px}}.article-tag-list{overflow:hidden;margin:0 5px 0 0;padding:0}.article-tag-list li{display:inline-block;margin:0 8px 5px 0}.article-tag-list-link{display:inline-block;padding:0 10px;line-height:20px;font-size:13px;color:#727272;background:#e8e8e8;border-radius:5px}#gotop{right:16px;bottom:30px;line-height:56px;text-align:center;color:#fff;background:#e040fb;border-radius:50%;opacity:0;-webkit-transform:translateX(200%);transform:translateX(200%)}@media screen and (max-width:760px){#gotop{width:40px;height:40px;line-height:40px}}.post-widget{float:right;margin-left:40px;width:calc((100vw - 960px)/ 2 - 15px);min-height:1px}.post-toc-wrap{position:fixed;overflow-x:hidden;width:20%;margin-top:20px}.post-toc-link,.post-toc>.post-toc-item{position:relative}.post-toc-wrap ol{list-style:none;margin:0}.post-toc-wrap h4{padding:0 0 10px 20px;font-size:15px;font-weight:600;color:#727272}.post-toc-child{padding-left:10px}.post-toc{display:inline-block;padding:0;font-size:13px}.post-toc-item{font-weight:400;color:#727272}.post-toc-link{z-index:2;display:block;padding:3px 20px;line-height:1.5rem;color:inherit;word-break:break-all}.post-toc-link:after,.post-toc-link:before{content:'';position:absolute;z-index:1;top:0;right:0;width:100%;height:100%}.post-body{padding:30px 0 20px}.post-main .post-content{min-height:100px}.post-content{line-height:1.8}.post-content h1{color:#673ab7;padding-top:56px;font-size:32px;font-weight:500}.post-content h1,.post-content p,.post-content ul{margin-bottom:8px;margin-top:8px}.post-content p:first-child{margin-top:0}.post-content ul{font-size:13px;line-height:2em}.post-content img{display:block;max-width:100%;margin:0 auto;background:#fff;overflow:hidden}.post-content .headerlink{visibility:hidden;margin-left:-1em;padding-right:5px;color:#d1c4e9}.post-content .headerlink:before{content:"#"}@media screen and (max-width:760px){.article .post-meat,.article h1,.article p{margin-left:16px;margin-right:16px}}@media screen and (max-width:1440px){.post-widget{display:none}}@media screen and (min-width:1440px) and (max-width:1680px){main>#post-widget{opacity:1}}.search-wrap{position:relative;min-width:56px}.search-wrap .search-input{position:relative;z-index:2;width:0;height:36px;margin:10px 10px 0 0;border:none;background:0 0;color:#fff;border-bottom:2px solid #fff;opacity:0}.search-wrap .search-input::-webkit-input-placeholder{color:#d1c4e9}.search-wrap .header-icon{position:absolute;top:2px;z-index:3}.search-wrap #search{right:0}.search-wrap #back{display:none;left:0}.search-panel{position:fixed;top:50px;right:65px;z-index:36;width:400px;max-height:70%;overflow-y:auto;background:#fff;border-radius:2px;opacity:0;visibility:hidden;-webkit-transform:translateY(-50%) scale(.5);transform:translateY(-50%) scale(.5)}.search-result{margin:0;padding:0}@media screen and (max-width:760px){.search-panel{z-index:28;top:0;right:0;padding-top:56px;width:100%;height:100%;max-height:100%;border-radius:0;box-shadow:none}}.on{display:block}.material-icons{font-family:'Material Icons';font-weight:400;font-size:24px;line-height:1;letter-spacing:normal;text-transform:none;display:inline-block;white-space:nowrap;word-wrap:normal;direction:ltr}</style></head><body><div id="loading" class="active"></div><nav id="menu" class="hide"><div class="inner flex-row-vertical"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off"><i class="icon icon-lg material-icons">close</i></a><div class="brand-wrap"><div class="brand"><a href="/about" class="avatar"><img src="/img/logo.png"></a><hgroup class="introduce"><h5 class="nickname">許書軒</h5><a href="mailto:ssarcandy@gmail.com" title="ssarcandy@gmail.com" class="mail">ssarcandy@gmail.com</a></hgroup></div></div><ul class="nav flex-col"><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 10px"><a href="/about"><i class="material-icons icon icon-lg">person</i> <span>About</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 10px"><a href="/"><i class="material-icons icon icon-lg">description</i> <span>Posts</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 10px"><a href="/archives"><i class="material-icons icon icon-lg">view_list</i> <span>Archives</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 10px"><a href="/projects"><i class="material-icons icon icon-lg">code</i> <span>Projects</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 10px"><a href="https://github.com/SSARCandy" target="_blank"><i class="icon icon-lg icon-github"></i> <span>Github</span></a></li></ul><footer class="footer"><p>Total visitors: 34,909</p><p>SSARCandy &copy; 2016 - 2019</p><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-2x icon-rss-square"></i></a></footer></div></nav><main id="main"><header class="top-header" id="header"><div class="flex-row"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle"><i class="icon icon-lg material-icons">menu</i></a><div class="flex-col header-title ellipsis">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</div><div class="search-wrap" id="search-wrap"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg material-icons">navigate_before</i> </a><input type="text" id="key" class="search-input" autocomplete="off" placeholder="Keywords"> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search"><i class="icon icon-lg material-icons">search</i></a></div></div></header><header class="content-header"><div class="container"><h1 class="author">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</h1><h5 class="subtitle"><time datetime="2017-10-31T13:18:50.000Z" itemprop="datePublished" class="page-time">Oct 31 2017</time></h5></div></header><aside class="post-widget" id="post-widget"><nav class="post-toc-wrap" id="post-toc"><h4>Contents</h4><ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Introduction"><span class="post-toc-text">Introduction</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Architecture"><span class="post-toc-text">Architecture</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#CORAL-Loss"><span class="post-toc-text">CORAL Loss</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Forward"><span class="post-toc-text">Forward</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Backward-gradient"><span class="post-toc-text">Backward (gradient)</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Experiment"><span class="post-toc-text">Experiment</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Implementation"><span class="post-toc-text">Implementation</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Update"><span class="post-toc-text">Update</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#References"><span class="post-toc-text">References</span></a></li></ol></nav></aside><div class="container body-wrap" id="body-wrap"><article id="post-deep-coral" class="article article-type-post" itemprop="blogPost"><div class="post-meat flex-row"><ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/">paper</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/">pytorch</a></li></ul></div><div class="post-meat" style="color:grey;font-size:13px"><i class="icon icon-sm icon-eye" aria-hidden="true"></i> 1,787</div><div class="post-body"><div class="post-main"><div class="post-content" id="post-content" itemprop="postContent"><p>雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。<br>Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 domain 的資料上。</p><p>這篇論文<sup>[1]</sup> (Deep CORAL: Correlation Alignment for Deep Domain Adaptation, B Sun, K Saenko, ECCV 2016) 提出一個 CORAL loss，通過對 source domain 和 target domain 進行線性變換來將他們各自的的二階統計量對齊 (minimizing the difference between source/target correlations).</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>作者引入了 CORAL<sup>[2]</sup> 這一方法，並且將其擴展成一 differentiable loss function。</li><li>作者藉由 CORAL loss 來做 unsupervised learning，並達到了 state-of-the-art 的 performance。</li><li>CORAL loss 十分簡單並且可以輕易地整合至一般的神經網路中。</li></ul><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><div><img src="/img/2017-10-31/1.png" alt="作者將 CORAL loss 加進一分類問題的網路架構中。 <sup>[1]</sup>" data-action="zoom"><span class="image-caption">作者將 CORAL loss 加進一分類問題的網路架構中。 <sup>[1]</sup></span></div><p>作者將 Deep CORAL 應用在一般的分類問題上，整個神經網路架構如圖。從中間 <code>cov1</code> ~ <code>fc8</code> 其實就是一般的 AlexNet，只是稍作修改改成有兩個 input (source data &amp; target data) 以及兩個 output。</p><p>在訓練的過程中，每個 batch 都包含了 source data &amp; target data，其中 source data 是包含 label 資料的；而 target data 則完全沒有 label 資料。<br>source data &amp; target data 各自經過一 shared weight 的 networks 之後會有兩個 output，其中:</p><ul><li>source task 會算一個 classification loss</li><li>source 的 <code>fc8</code> 及 target 的 <code>fc8</code> 會再拿來算 CORAL loss</li></ul><p>而總和 loss 為兩者相加:<br>$$<br>loss = l_{CLASS} + \lambda l_{CORAL}<br>$$</p><h1 id="CORAL-Loss"><a href="#CORAL-Loss" class="headerlink" title="CORAL Loss"></a>CORAL Loss</h1><p>作者提出的 CORAL loss 是在計算 source &amp; target covariance matrix 之間的 distance。</p><blockquote><p>We define the CORAL loss as the distance between the second-order statistics<br>(covariances) of the source and target features.</p></blockquote><h2 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h2><p>而這個 loss function 定義如下:</p><p>$$l_{CORAL} = \frac {1} {4d^{2}} \lVert C_S - C_T \rVert ^{2}_F$$</p><p>其中，<br>\(C_S\), \(C_T\) 為 source, target 的 covariance matrix，其定義如下:</p><p>$$\ C_S = \frac{1}{n_S - 1}(D^{\intercal}_S D_S-\frac {1} {n_S} (\textbf{l}^{\intercal}D_S )^\intercal(\textbf{l}^\intercal D_S))$$</p><p>$$\ C_T = \frac{1}{n_T - 1}(D^{\intercal}_T D_T-\frac {1} {n_T} (\textbf{l}^{\intercal}D_T )^\intercal(\textbf{l}^\intercal D_T))$$</p><p>詳細符號定義可以參考 paper<sup>[1]</sup> section 3.1</p><h2 id="Backward-gradient"><a href="#Backward-gradient" class="headerlink" title="Backward (gradient)"></a>Backward (gradient)</h2><p>至於 gradient 可以由 chain rule 算出來，如下:</p><p>$$<br>\ \frac{\partial l_{CORAL}}{\partial D^{ij}_S}=\frac {1}{d^{2}(n_S-1)} (D^{\intercal}_S-\frac{1}{n_S}((\textbf{l}^{\intercal}D_S )^\intercal\textbf{l}^{\intercal})^{\intercal}(C_S - C_T))^{ij}<br>$$</p><p>$$<br>\ \frac{\partial l_{CORAL}}{\partial D^{ij}_T}=-\frac {1}{d^{2}(n_T-1)} (D^{\intercal}_T-\frac{1}{n_T}((\textbf{l}^{\intercal}D_T )^\intercal\textbf{l}^{\intercal})^{\intercal}(C_S - C_T))^{ij}<br>$$</p><p>注意 target 那邊是有個負號的，當初在實作時忘記這個負號而搞半天…</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>作者做的實驗也是在分類問題上，架構如同上面提及的神經網路架構圖。<br>實驗採用 Office31 dataset<sup>[3]</sup>，這是一個專門拿來做 domain adaption 的資料集，裡面有三種不同 domain 的影像: Amazon, DSLR, and Webcam<br>裡面都有相同的 31 種類別，也就是說這三大類唯一不同的點就是圖片的樣貌:</p><ul><li>Amazon 就是去背的圖片(背景都是白色的)</li><li>DSLR 就是用單眼拍的圖片(背景就是真實場景的背景)</li><li>Webcam 跟 DSLR 很相近，差別比較大的部分是 webcam 的畫質比較差，有的還有色偏</li></ul><p>在實驗進行過程中，source data 會有 label；而 target data 則沒有。<br>且在開始之前會先預載 ImageNet pre-trained model。</p><p>由於 Office31 有三種 domain data，所以作者就做了所有 domain adaption 的組合，以下是結果圖:</p><div><img src="/img/2017-10-31/2.png" alt="各種方法比較圖。螢光的是作者的方法。<sup>[1]</sup>" data-action="zoom"><span class="image-caption">各種方法比較圖。螢光的是作者的方法。<sup>[1]</sup></span></div><p>可以看到 D-CORAL 在大部分的 domain adaption tasks 中都取得了最好的成績。</p><p>再來看看其中一個實驗 Amazon → Webcam 的詳細結果:</p><div><img src="/img/2017-10-31/3.png" alt="Amazon → Webcam 的詳細結果圖。<sup>[1]</sup>" data-action="zoom"><span class="image-caption">Amazon → Webcam 的詳細結果圖。<sup>[1]</sup></span></div><p>圖 (a) 比較了有 CORAL loss 與沒有 CORAL loss 的差別，可以看到當加入CORAL loss 之後，target (test) task 有顯著的提升，而且並未使得 source (training) task 的準確率下降太多。</p><p>圖 (b) 則可以看出，classification loss 跟 CORAL loss 其實是扮演互相抗衡的腳色，隨著訓練的進行會讓兩者到達一穩定的狀態。</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>我也試著用 PyTorch 實做了此篇論文的方法，最重要的其實就是新增一 loss function 到整個網路架構中，其中 forward and backward 的算法剛好也有詳細說明。</p><p>Forward 的部分大概如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, source, target)</span>:</span></div><div class="line">    d = source.shape[<span class="number">1</span>]</div><div class="line">    ns, nt = source.shape[<span class="number">0</span>], target.shape[<span class="number">0</span>]</div><div class="line">    cs = feature_covariance_mat(ns, source)</div><div class="line">    ct = feature_covariance_mat(nt, target)</div><div class="line">    self.saved = (source, target, cs, ct, ns, nt, d)</div><div class="line">    res = forbenius_norm(cs - ct)**<span class="number">2</span>/(<span class="number">4</span>*d*d)</div><div class="line">    res = torch.FloatTensor([res])</div><div class="line">    <span class="keyword">return</span> res</div></pre></td></tr></table></figure><p>Backward 則如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></div><div class="line">    source, target, cs, ct, ns, nt, d = self.saved</div><div class="line">    ones_s_t = torch.ones(ns).view(<span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">    ones_t_t = torch.ones(nt).view(<span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">    s_gradient = (source.t() - (ones_s_t.matmul(source).t().matmul(ones_s_t)/ns)).t().matmul(cs - ct) / (d*d*(ns - <span class="number">1</span>))</div><div class="line">    t_gradient = (target.t() - (ones_t_t.matmul(target).t().matmul(ones_t_t)/nt)).t().matmul(cs - ct) / (d*d*(nt - <span class="number">1</span>))</div><div class="line">    t_gradient = -t_gradient</div><div class="line">    <span class="keyword">return</span> s_gradient*grad_output, t_gradient*grad_output</div></pre></td></tr></table></figure><p>寫起來公式的部分又臭又長 XD</p><p>我也實際跑了 Amazon → Webcam 的例子，做了個圖:</p><div><img src="/img/2017-10-31/4.png" alt="我做出來的 Amazon → Webcam 的詳細結果圖。" data-action="zoom"><span class="image-caption">我做出來的 Amazon → Webcam 的詳細結果圖。</span></div><p>可以看出有 CORAL loss 的確使得 target task 的準確率提升一些。<br>不過我做出來的整體準確率並沒有與論文上的一樣有 60% 左右，而是大概在 50% 左右，不知道為甚麼… QQ</p><h1 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h1><p>經過 <a href="https://github.com/redhat12345" target="_blank" rel="external">redhat12345</a> 的建議後，修正了一下 CORAL Loss 的算法，終於使 Target accuracy 提升到原論文的程度。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">CORAL</span><span class="params">(source, target)</span>:</span></div><div class="line">    d = source.data.shape[<span class="number">1</span>]</div><div class="line">    <span class="comment"># source covariance</span></div><div class="line">    xm = torch.mean(source, <span class="number">1</span>, keepdim=<span class="keyword">True</span>) - source</div><div class="line">    xc = torch.matmul(torch.transpose(xm, <span class="number">0</span>, <span class="number">1</span>), xm)</div><div class="line">    <span class="comment"># target covariance</span></div><div class="line">    xmt = torch.mean(target, <span class="number">1</span>, keepdim=<span class="keyword">True</span>) - target</div><div class="line">    xct = torch.matmul(torch.transpose(xmt, <span class="number">0</span>, <span class="number">1</span>), xmt)</div><div class="line">    <span class="comment"># frobenius norm between source and target</span></div><div class="line">    loss = torch.mean(torch.mul((xc - xct), (xc - xct)))</div><div class="line">    loss = loss/(<span class="number">4</span>*d*d)</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p></p><div><img src="/img/2017-10-31/5.png" alt="修正過後的結果。" data-action="zoom"><span class="image-caption">修正過後的結果。</span></div><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] Sun, B., Saenko, K.: Deep CORAL: Correlation Alignment for Deep Domain Adaptation. In: ECCV (2016)<br>[2] Sun, B., Feng, J., Saenko, K.: Return of frustratingly easy domain adaptation. In: AAAI (2016)<br>[3] <a href="https://people.eecs.berkeley.edu/~jhoffman/domainadapt/#datasets_code" target="_blank" rel="external">Domain Adaptation Project</a></p></div><nav class="post-nav"><div class="waves-block waves-effect prev fl"><a href="/2017/12/06/javascript-prototype-chain/" id="post-prev" class="post-nav-link"><div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div><h4 class="title">搞懂 JavaScript 原型鍊</h4></a></div><div class="waves-block waves-effect next fr"><a href="/2017/09/27/using-pytorch-in-windows/" id="post-next" class="post-nav-link"><div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div><h4 class="title">Using PyTorch in Windows 10</h4></a></div></nav><section id="comments"><div id="disqus_thread"></div><script type="text/javascript">var disqus_shortname="ssarcandy";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></section></div></div></article><script src="/js/zoom.js"></script><script type="application/ld+json">{
  "@context": "http://schema.org",
  "@type": "Article",
  "mainEntityOfPage": "https://ssarcandy.tw/2017/10/31/deep-coral/",
  "headline": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
  "datePublished": "Tue Oct 31 2017 21:18:50 GMT+0800",
  "dateModified": "Tue Oct 31 2017 21:18:50 GMT+0800",
  "author": "SSARCandy",
  "image": {
    "@type": "ImageObject",
    "url": "https://ssarcandy.tw"
  },
  "publisher":{ 
    "@type" : "Organization",
    "name": "SSARCandy's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ssarcandy.tw/img/logo.png"
    }
  }     
}</script></div></main><div class="mask" id="mask"></div><a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a><script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script><script src="/js/main.js"></script><div class="search-panel" id="search-panel"><ul class="search-result" id="search-result"></ul></div><script type="text/template" id="search-tpl"><li class="item">
    <a href="/{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li></script><script src="/js/search.js"></script><script type="text/javascript">!function(e,a,t,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=a.createElement(t),s=a.getElementsByTagName(t)[0],o.async=1,o.src="//www.google-analytics.com/analytics.js",s.parentNode.insertBefore(o,s)}(window,document,"script",0,"ga"),ga("create","UA-81178870-1","auto"),ga("send","pageview")</script><script type="text/x-mathjax-config">MathJax.Hub.Config("");</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>