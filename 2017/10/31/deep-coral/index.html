<!DOCTYPE html><html lang="zh"><head><link rel="manifest" href="/manifest.json"><meta charset="utf-8"><title>Deep CORAL: Correlation Alignment for Deep Domain Adaptation | SSARCandy's Blog</title><meta name="google-site-verification" content="uUti8Shw9zIz5j2Rs_nwYT9VusmIOXijBuf2bBNYm78"><meta http-equiv="Cache-control" content="public"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#673AB7"><meta name="keywords" content="paper,python,pytorch,machine learning"><meta name="description" content="雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 doma"><meta property="og:type" content="article"><meta property="og:title" content="Deep CORAL: Correlation Alignment for Deep Domain Adaptation"><meta property="og:url" content="https://ssarcandy.tw/2017/10/31/deep-coral/index.html"><meta property="og:site_name" content="SSARCandy's Blog"><meta property="og:description" content="雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 doma"><meta property="og:locale" content="zh_TW"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/1.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/2.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/3.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/4.png"><meta property="og:image" content="https://ssarcandy.tw/img/2017-10-31/5.png"><meta property="article:published_time" content="2017-10-31T13:18:50.000Z"><meta property="article:modified_time" content="2020-02-03T05:33:45.212Z"><meta property="article:author" content="許書軒"><meta property="article:tag" content="paper"><meta property="article:tag" content="python"><meta property="article:tag" content="pytorch"><meta property="article:tag" content="machine learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://ssarcandy.tw/img/2017-10-31/1.png"><link rel="alternative" href="/atom.xml" title="SSARCandy's Blog" type="application/atom+xml"><meta name="summary" content="雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 domain 的資料上。
這篇論文[1] (Deep CORAL: Correlation Alignment for Deep Domain Adaptation, B Sun, K Saenko, ECCV 2016) 提出一個 CORAL loss，通過對 source domain 和 target domain 進行線性變換來將他們各自的的二階統計量對齊 (minimizing the difference between source/target correlations)."><meta name="Description" content="ssarcandy's blog. There is always more than one solution."><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/style.css"><link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"><link rel="manifest" href="/web-app-manifest.json"><meta name="generator" content="Hexo 4.1.1"></head><body><div id="loading" class="active"></div><nav id="menu" class="hide"><div class="inner flex-row-vertical"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off"><i class="icon icon-lg material-icons">close</i></a><div class="brand-wrap"><div class="brand"><a href="/about" class="avatar"><img alt="ssarcandy" src="/img/logo.png"></a><hgroup class="introduce"><h5 class="nickname">許書軒</h5><div class="mail">There is always more than one solution.</div></hgroup></div></div><ul class="nav flex-col"><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 0"><a href="/about"><i class="material-icons icon icon-lg">person</i> <span>About</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 0"><a href="/"><i class="material-icons icon icon-lg">description</i> <span>Posts</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 0"><a href="/archives"><i class="material-icons icon icon-lg">view_list</i> <span>Archives</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 0"><a href="/projects"><i class="material-icons icon icon-lg">code</i> <span>Projects</span></a></li><li class="waves-block waves-effect" style="line-height:44px;height:44px;padding:0 0"><a href="https://github.com/SSARCandy" target="_blank" rel="noopener"><i class="icon icon-lg icon-github"></i> <span>Github</span></a></li></ul><footer class="footer"><p>Total visitors: 46,505</p><p>SSARCandy © 2016 - 2020</p><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-2x icon-rss-square"></i></a></footer></div></nav><main id="main"><header class="top-header" id="header"><div class="flex-row"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle"><i class="icon icon-lg material-icons">menu</i></a><div class="flex-col header-title ellipsis">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</div><div class="search-wrap" id="search-wrap"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg material-icons">navigate_before</i> </a><input type="text" id="key" class="search-input" aria-label="search-box" autocomplete="off" placeholder="Keywords"> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search"><i class="icon icon-lg material-icons">search</i></a></div></div></header><header class="content-header"><div class="container"><h1 class="author">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</h1><h5 class="subtitle"><time datetime="2017-10-31T13:18:50.000Z" itemprop="datePublished" class="page-time">Oct 31 2017</time></h5></div></header><aside class="post-widget" id="post-widget"><nav class="post-toc-wrap" id="post-toc"><h4>Contents</h4><ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Introduction"><span class="post-toc-text">Introduction</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Architecture"><span class="post-toc-text">Architecture</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#CORAL-Loss"><span class="post-toc-text">CORAL Loss</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Forward"><span class="post-toc-text">Forward</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Backward-gradient"><span class="post-toc-text">Backward (gradient)</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Experiment"><span class="post-toc-text">Experiment</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Implementation"><span class="post-toc-text">Implementation</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Update"><span class="post-toc-text">Update</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#References"><span class="post-toc-text">References</span></a></li></ol></nav></aside><div class="container body-wrap" id="body-wrap"><article id="post-deep-coral" class="article article-type-post" itemprop="blogPost"><div class="post-meat flex-row"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/" rel="tag">paper</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul></div><div class="post-meat" style="color:grey;font-size:13px"><i class="icon icon-sm icon-eye" aria-hidden="true"></i> 3,042</div><div class="post-body"><div class="post-main"><div class="post-content" id="post-content" itemprop="postContent"><p>雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。<br>Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 domain 的資料上。</p><p>這篇論文<sup>[1]</sup> (Deep CORAL: Correlation Alignment for Deep Domain Adaptation, B Sun, K Saenko, ECCV 2016) 提出一個 CORAL loss，通過對 source domain 和 target domain 進行線性變換來將他們各自的的二階統計量對齊 (minimizing the difference between source/target correlations).</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>作者引入了 CORAL<sup>[2]</sup> 這一方法，並且將其擴展成一 differentiable loss function。</li><li>作者藉由 CORAL loss 來做 unsupervised learning，並達到了 state-of-the-art 的 performance。</li><li>CORAL loss 十分簡單並且可以輕易地整合至一般的神經網路中。</li></ul><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><div><img src="/img/2017-10-31/1.png" alt="作者將 CORAL loss 加進一分類問題的網路架構中。 <sup>[1]</sup>" data-action="zoom" class="photozoom"> <span class="zoom-initial-caption">作者將 CORAL loss 加進一分類問題的網路架構中。 <sup>[1]</sup></span></div><p>作者將 Deep CORAL 應用在一般的分類問題上，整個神經網路架構如圖。從中間 <code>cov1</code> ~ <code>fc8</code> 其實就是一般的 AlexNet，只是稍作修改改成有兩個 input (source data & target data) 以及兩個 output。</p><p>在訓練的過程中，每個 batch 都包含了 source data & target data，其中 source data 是包含 label 資料的；而 target data 則完全沒有 label 資料。<br>source data & target data 各自經過一 shared weight 的 networks 之後會有兩個 output，其中:</p><ul><li>source task 會算一個 classification loss</li><li>source 的 <code>fc8</code> 及 target 的 <code>fc8</code> 會再拿來算 CORAL loss</li></ul><p>而總和 loss 為兩者相加:<br>$$<br>loss = l_{CLASS} + \lambda l_{CORAL}<br>$$</p><h1 id="CORAL-Loss"><a href="#CORAL-Loss" class="headerlink" title="CORAL Loss"></a>CORAL Loss</h1><p>作者提出的 CORAL loss 是在計算 source & target covariance matrix 之間的 distance。</p><blockquote><p>We define the CORAL loss as the distance between the second-order statistics<br>(covariances) of the source and target features.</p></blockquote><h2 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h2><p>而這個 loss function 定義如下:</p><p>$$l_{CORAL} = \frac {1} {4d^{2}} \lVert C_S - C_T \rVert ^{2}_F$$</p><p>其中，<br>\(C_S\), \(C_T\) 為 source, target 的 covariance matrix，其定義如下:</p><p>$$\ C_S = \frac{1}{n_S - 1}(D^{\intercal}_S D_S-\frac {1} {n_S} (\textbf{l}^{\intercal}D_S )^\intercal(\textbf{l}^\intercal D_S))$$</p><p>$$\ C_T = \frac{1}{n_T - 1}(D^{\intercal}_T D_T-\frac {1} {n_T} (\textbf{l}^{\intercal}D_T )^\intercal(\textbf{l}^\intercal D_T))$$</p><p>詳細符號定義可以參考 paper<sup>[1]</sup> section 3.1</p><h2 id="Backward-gradient"><a href="#Backward-gradient" class="headerlink" title="Backward (gradient)"></a>Backward (gradient)</h2><p>至於 gradient 可以由 chain rule 算出來，如下:</p><p>$$<br>\ \frac{\partial l_{CORAL}}{\partial D^{ij}_S}=\frac {1}{d^{2}(n_S-1)} (D^{\intercal}_S-\frac{1}{n_S}((\textbf{l}^{\intercal}D_S )^\intercal\textbf{l}^{\intercal})^{\intercal}(C_S - C_T))^{ij}<br>$$</p><p>$$<br>\ \frac{\partial l_{CORAL}}{\partial D^{ij}_T}=-\frac {1}{d^{2}(n_T-1)} (D^{\intercal}_T-\frac{1}{n_T}((\textbf{l}^{\intercal}D_T )^\intercal\textbf{l}^{\intercal})^{\intercal}(C_S - C_T))^{ij}<br>$$</p><p>注意 target 那邊是有個負號的，當初在實作時忘記這個負號而搞半天…</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>作者做的實驗也是在分類問題上，架構如同上面提及的神經網路架構圖。<br>實驗採用 Office31 dataset<sup>[3]</sup>，這是一個專門拿來做 domain adaption 的資料集，裡面有三種不同 domain 的影像: Amazon, DSLR, and Webcam<br>裡面都有相同的 31 種類別，也就是說這三大類唯一不同的點就是圖片的樣貌:</p><ul><li>Amazon 就是去背的圖片(背景都是白色的)</li><li>DSLR 就是用單眼拍的圖片(背景就是真實場景的背景)</li><li>Webcam 跟 DSLR 很相近，差別比較大的部分是 webcam 的畫質比較差，有的還有色偏</li></ul><p>在實驗進行過程中，source data 會有 label；而 target data 則沒有。<br>且在開始之前會先預載 ImageNet pre-trained model。</p><p>由於 Office31 有三種 domain data，所以作者就做了所有 domain adaption 的組合，以下是結果圖:</p><div><img src="/img/2017-10-31/2.png" alt="各種方法比較圖。螢光的是作者的方法。<sup>[1]</sup>" data-action="zoom" class="photozoom"> <span class="zoom-initial-caption">各種方法比較圖。螢光的是作者的方法。<sup>[1]</sup></span></div><p>可以看到 D-CORAL 在大部分的 domain adaption tasks 中都取得了最好的成績。</p><p>再來看看其中一個實驗 Amazon → Webcam 的詳細結果:</p><div><img src="/img/2017-10-31/3.png" alt="Amazon → Webcam 的詳細結果圖。<sup>[1]</sup>" data-action="zoom" class="photozoom"> <span class="zoom-initial-caption">Amazon → Webcam 的詳細結果圖。<sup>[1]</sup></span></div><p>圖 (a) 比較了有 CORAL loss 與沒有 CORAL loss 的差別，可以看到當加入CORAL loss 之後，target (test) task 有顯著的提升，而且並未使得 source (training) task 的準確率下降太多。</p><p>圖 (b) 則可以看出，classification loss 跟 CORAL loss 其實是扮演互相抗衡的腳色，隨著訓練的進行會讓兩者到達一穩定的狀態。</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>我也試著用 PyTorch 實做了此篇論文的方法，最重要的其實就是新增一 loss function 到整個網路架構中，其中 forward and backward 的算法剛好也有詳細說明。</p><p>Forward 的部分大概如下:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, source, target)</span>:</span></span><br><span class="line">    d = source.shape[<span class="number">1</span>]</span><br><span class="line">    ns, nt = source.shape[<span class="number">0</span>], target.shape[<span class="number">0</span>]</span><br><span class="line">    cs = feature_covariance_mat(ns, source)</span><br><span class="line">    ct = feature_covariance_mat(nt, target)</span><br><span class="line">    self.saved = (source, target, cs, ct, ns, nt, d)</span><br><span class="line">    res = forbenius_norm(cs - ct)**<span class="number">2</span>/(<span class="number">4</span>*d*d)</span><br><span class="line">    res = torch.FloatTensor([res])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></tbody></table></figure><p>Backward 則如下:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">    source, target, cs, ct, ns, nt, d = self.saved</span><br><span class="line">    ones_s_t = torch.ones(ns).view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    ones_t_t = torch.ones(nt).view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    s_gradient = (source.t() - (ones_s_t.matmul(source).t().matmul(ones_s_t)/ns)).t().matmul(cs - ct) / (d*d*(ns - <span class="number">1</span>))</span><br><span class="line">    t_gradient = (target.t() - (ones_t_t.matmul(target).t().matmul(ones_t_t)/nt)).t().matmul(cs - ct) / (d*d*(nt - <span class="number">1</span>))</span><br><span class="line">    t_gradient = -t_gradient</span><br><span class="line">    <span class="keyword">return</span> s_gradient*grad_output, t_gradient*grad_output</span><br></pre></td></tr></tbody></table></figure><p>寫起來公式的部分又臭又長 XD</p><p>我也實際跑了 Amazon → Webcam 的例子，做了個圖:</p><div><img src="/img/2017-10-31/4.png" alt="我做出來的 Amazon → Webcam 的詳細結果圖。" data-action="zoom" class="photozoom"> <span class="zoom-initial-caption">我做出來的 Amazon → Webcam 的詳細結果圖。</span></div><p>可以看出有 CORAL loss 的確使得 target task 的準確率提升一些。<br>不過我做出來的整體準確率並沒有與論文上的一樣有 60% 左右，而是大概在 50% 左右，不知道為甚麼… QQ</p><h1 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h1><p>經過 <a href="https://github.com/redhat12345" target="_blank" rel="noopener">redhat12345</a> 的建議後，修正了一下 CORAL Loss 的算法，終於使 Target accuracy 提升到原論文的程度。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CORAL</span><span class="params">(source, target)</span>:</span></span><br><span class="line">    d = source.data.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># source covariance</span></span><br><span class="line">    xm = torch.mean(source, <span class="number">1</span>, keepdim=<span class="literal">True</span>) - source</span><br><span class="line">    xc = torch.matmul(torch.transpose(xm, <span class="number">0</span>, <span class="number">1</span>), xm)</span><br><span class="line">    <span class="comment"># target covariance</span></span><br><span class="line">    xmt = torch.mean(target, <span class="number">1</span>, keepdim=<span class="literal">True</span>) - target</span><br><span class="line">    xct = torch.matmul(torch.transpose(xmt, <span class="number">0</span>, <span class="number">1</span>), xmt)</span><br><span class="line">    <span class="comment"># frobenius norm between source and target</span></span><br><span class="line">    loss = torch.mean(torch.mul((xc - xct), (xc - xct)))</span><br><span class="line">    loss = loss/(<span class="number">4</span>*d*d)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></tbody></table></figure><div><img src="/img/2017-10-31/5.png" alt="修正過後的結果。" data-action="zoom" class="photozoom"> <span class="zoom-initial-caption">修正過後的結果。</span></div><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] Sun, B., Saenko, K.: Deep CORAL: Correlation Alignment for Deep Domain Adaptation. In: ECCV (2016)<br>[2] Sun, B., Feng, J., Saenko, K.: Return of frustratingly easy domain adaptation. In: AAAI (2016)<br>[3] <a href="https://people.eecs.berkeley.edu/~jhoffman/domainadapt/#datasets_code" target="_blank" rel="noopener">Domain Adaptation Project</a></p></div><nav class="post-nav"><div class="waves-block waves-effect prev fl"><a href="/2017/12/06/javascript-prototype-chain/" id="post-prev" class="post-nav-link"><div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div><h4 class="title">搞懂 JavaScript 原型鍊</h4></a></div><div class="waves-block waves-effect next fr"><a href="/2017/09/27/using-pytorch-in-windows/" id="post-next" class="post-nav-link"><div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div><h4 class="title">Using PyTorch in Windows 10</h4></a></div></nav><section id="comments"><div id="disqus_thread"></div><script>!function(){var t=document,e=t.createElement("script");e.src="https://ssarcandy.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript></section></div></div></article><script type="application/ld+json">{
  "@context": "http://schema.org",
  "@type": "Article",
  "mainEntityOfPage": "https://ssarcandy.tw/2017/10/31/deep-coral/",
  "headline": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
  "datePublished": "Tue Oct 31 2017 21:18:50 GMT+0800 (CST)",
  "dateModified": "Tue Oct 31 2017 21:18:50 GMT+0800 (CST)",
  "author": "SSARCandy",
  "image": {
    "@type": "ImageObject",
    "url": "https://ssarcandy.tw"
  },
  "publisher":{ 
    "@type" : "Organization",
    "name": "SSARCandy's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ssarcandy.tw/img/logo.png"
    }
  }     
}</script></div></main><div class="mask" id="mask"></div><a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light" aria-label="go to top"><span class="icon icon-lg icon-chevron-up"></span></a><script src="/js/global.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/node-waves/0.7.6/waves.min.js"></script><script src="/js/main.js"></script><div class="search-panel" id="search-panel"><ul class="search-result" id="search-result"></ul></div><script type="text/template" id="search-tpl"><li class="item">
    <a href="/{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li></script><script src="/js/search.js"></script><script type="text/javascript">!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject=g,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script",0,"ga"),ga("create","UA-81178870-1","auto"),ga("send","pageview")</script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js?t=1580708107544").then(function(){console.log("ServiceWorker Register Successfully.")}).catch(function(e){console.error(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config("");</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">/**
 * Pure JavaScript implementation of zoom.js.
 *
 * Original preamble:
 * zoom.js - It's the best way to zoom an image
 * @version v0.0.2
 * @link https://github.com/fat/zoom.js
 * @license MIT
 *
 * This is a fork of the original zoom.js implementation by @fat.
 * Copyrights for the original project are held by @fat. All other copyright
 * for changes in the fork are held by Nishanth Shanmugham.
 *
 * Copyright (c) 2013 @fat
 * The MIT License. Copyright © 2016 Nishanth Shanmugham.
 */
!function(){"use strict";var t=function(){function t(t,e){for(var n=0;n<e.length;n++){var i=e[n];i.enumerable=i.enumerable||!1,i.configurable=!0,"value"in i&&(i.writable=!0),Object.defineProperty(t,i.key,i)}}return function(e,n,i){return n&&t(e.prototype,n),i&&t(e,i),e}}();function e(t,e){if(!(t instanceof e))throw new TypeError("Cannot call a class as a function")}!function(t){var e={};function n(i){if(e[i])return e[i].exports;var o=e[i]={i:i,l:!1,exports:{}};return t[i].call(o.exports,o,o.exports,n),o.l=!0,o.exports}n.m=t,n.c=e,n.i=function(t){return t},n.d=function(t,e,i){n.o(t,e)||Object.defineProperty(t,e,{configurable:!1,enumerable:!0,get:i})},n.n=function(t){var e=t&&t.__esModule?function(){return t.default}:function(){return t};return n.d(e,"a",e),e},n.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)},n.p="",n(n.s=3)}([function(t,e,n){n.d(e,"a",function(){return i}),n.d(e,"b",function(){return o}),n.d(e,"c",function(){return r}),n.d(e,"d",function(){return a});var i=function(){return document.documentElement.clientWidth},o=function(){return document.documentElement.clientHeight},r=function(t){var e=t.getBoundingClientRect(),n=document.documentElement,i=window;return{top:e.top+i.pageYOffset-n.clientTop,left:e.left+i.pageXOffset-n.clientLeft}},a=function(t,e,n){t.addEventListener(e,function t(i){i.target.removeEventListener(e,t),n()})}},function(t,e,n){var i=n(2),o=n(0);n.d(e,"a",function(){return g});var r=null,a=-1,s=-1,c=function(t){document.body.classList.contains("zoom-overlay-open")||(t.metaKey||t.ctrlKey?window.open(t.target.getAttribute("data-original")||t.target.src,"_blank"):t.target.width>=n.i(o.a)()-80||(u(!0),(r=new i.a(t.target,80)).zoom(),l()))},u=function(t){null!=r&&(t?r.dispose():r.close(),d(),r=null)},l=function(){document.addEventListener("scroll",m),document.addEventListener("keyup",f),document.addEventListener("touchstart",h),document.addEventListener("click",v,!0)},d=function(){document.removeEventListener("scroll",m),document.removeEventListener("keyup",f),document.removeEventListener("touchstart",h),document.removeEventListener("click",v,!0)},m=function(){-1==a&&(a=window.pageYOffset),Math.abs(a-window.pageYOffset)>=40&&u()},f=function(t){27==t.keyCode&&u()},h=function(t){var e=t.touches[0];null!=e&&(s=e.pageY,t.target.addEventListener("touchmove",p))},p=function t(e){var n=e.touches[0];null!=n&&Math.abs(n.pageY-s)>10&&(u(),e.target.removeEventListener("touchmove",t))},v=function(){u()},g=Object.create(null);g.setup=function(t){t.addEventListener("click",c)}},function(n,i,o){var r=o(0),a=function t(n,i){e(this,t),this.w=n,this.h=i},s=function(){function n(t,i){e(this,n),this.img=t,this.preservedTransform=t.style.transform,this.wrap=null,this.caption=null,this.overlay=null,this.offset=i}return t(n,[{key:"forceRepaint",value:function(){this.img.offsetWidth}},{key:"zoom",value:function(){var t=new a(this.img.naturalWidth,this.img.naturalHeight);this.wrap=document.createElement("div"),this.wrap.classList.add("zoom-img-wrap"),this.img.parentNode.insertBefore(this.wrap,this.img),this.wrap.appendChild(this.img),this.img.classList.add("zoom-img"),this.img.setAttribute("data-action","zoom-out"),this.overlay=document.createElement("div"),this.overlay.classList.add("zoom-overlay"),this.caption=document.createElement("span"),this.caption.innerHTML=this.img.getAttribute("alt"),this.caption.classList.add("zoom-caption"),this.overlay.appendChild(this.caption),document.body.appendChild(this.overlay),this.forceRepaint();var e=this.calculateScale(t);this.forceRepaint(),this.animate(e),document.body.classList.add("zoom-overlay-open")}},{key:"calculateScale",value:function(t){var e=t.w/this.img.width,n=o.i(r.a)()-this.offset,i=o.i(r.b)()-this.offset,a=t.w/t.h,s=n/i;return t.w<n&&t.h<i?e:a<s?i/t.h*e:n/t.w*e}},{key:"animate",value:function(t){var e=o.i(r.c)(this.img),n=window.pageYOffset,i=o.i(r.a)()/2,a=n+o.i(r.b)()/2,s="scale("+t+")",c="translate3d("+(i-(e.left+this.img.width/2))+"px, "+(a-(e.top+this.img.height/2))+"px, 0px)";this.img.style.transform=s,this.wrap.style.transform=c}},{key:"dispose",value:function(){null!=this.wrap&&null!=this.wrap.parentNode&&(this.img.classList.remove("zoom-img"),this.img.setAttribute("data-action","zoom"),this.wrap.parentNode.insertBefore(this.img,this.wrap),this.wrap.parentNode.removeChild(this.wrap),document.body.removeChild(this.overlay),document.body.classList.remove("zoom-overlay-transitioning"))}},{key:"close",value:function(){var t=this;document.body.classList.add("zoom-overlay-transitioning"),this.img.style.transform=this.preservedTransform,0===this.img.style.length&&this.img.removeAttribute("style"),this.wrap.style.transform="none",o.i(r.d)(this.img,"transitionend",function(){t.dispose(),document.body.classList.remove("zoom-overlay-open")})}}]),n}();i.a=s},function(t,e,n){Object.defineProperty(e,"__esModule",{value:!0});var i=n(1);document.addEventListener("DOMContentLoaded",function(){for(var t=document.querySelectorAll("img[data-action='zoom']"),e=0;e<t.length;e++)i.a.setup(t[e])})}])}();</script><style>img[data-action=zoom]{cursor:pointer;cursor:-webkit-zoom-in;cursor:-moz-zoom-in}.zoom-img,.zoom-img-wrap{position:relative;z-index:666;-webkit-transition:all .3s;-o-transition:all .3s;transition:all .3s}img.zoom-img{cursor:pointer;cursor:-webkit-zoom-out;cursor:-moz-zoom-out}.zoom-overlay{z-index:420;text-align:center;background:#fff;position:fixed;top:0;left:0;right:0;bottom:0;pointer-events:none;filter:"alpha(opacity=0)";opacity:0;-webkit-transition:opacity .3s;-o-transition:opacity .3s;transition:opacity .3s}.zoom-overlay-open .zoom-overlay{filter:"alpha(opacity=100)";opacity:1}.zoom-overlay-open,.zoom-overlay-transitioning{cursor:default}.body-wrap-zoom{width:100vw}.zoom-caption{z-index:667;bottom:10px;position:fixed;left:10px;right:10px}.zoom-initial-caption{color:#999;display:block;font-size:.9em;margin-top:.5em;position:relative;text-align:center;}</style></body></html>