{"meta":{"title":"SSARCandy's Blog","subtitle":"","description":"ssarcandy's blog. There is always more than one solution.","author":"John Doe","url":"https://ssarcandy.tw","root":"/"},"pages":[{"title":"","date":"2025-10-22T00:51:26.809Z","updated":"2025-10-22T00:51:26.809Z","comments":true,"path":"web-app-manifest.json","permalink":"https://ssarcandy.tw/web-app-manifest.json","excerpt":"","text":"{\"name\":\"ssarcandy\",\"start_url\":\"/\",\"display\":\"standalone\",\"orientation\":\"natural\"}"},{"title":"About","date":"2025-10-22T00:51:26.606Z","updated":"2025-10-22T00:51:26.606Z","comments":false,"path":"about/index.html","permalink":"https://ssarcandy.tw/about/index.html","excerpt":"","text":"I am Andy Hsu, graduated student of CMLab, National Taiwan University.Interested in programming and actively share various projects on GitHub, trying to make the world better (or just make my life easier).Also interested in Astronomy⭐, Machine Learning💻, Web Development🌐, and Photographic📸."},{"title":"Projects","date":"2025-10-22T00:51:26.809Z","updated":"2025-10-22T00:51:26.809Z","comments":false,"path":"projects/index.html","permalink":"https://ssarcandy.tw/projects/index.html","excerpt":"","text":"Here listed my open source projects."},{"title":"","date":"2025-10-22T00:51:26.604Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"GAv3-pageview.json","permalink":"https://ssarcandy.tw/GAv3-pageview.json","excerpt":"","text":"{\"visitor_count\":92046,\"pv_map\":{\"travian-article-submission\":81,\"setting-up-opencv-using-cmake-gui\":948,\"hello-world\":2,\"bot-battle\":410,\"hexo-auto-deploy\":490,\"vim-plugin-manager\":4202,\"python-note\":723,\"pbrt-heightfield\":525,\"debug-using-vdb\":286,\"pbrt-realistic-camera\":739,\"cml-fb-bot\":624,\"pbrt-mediancut-environment-light\":297,\"policyd\":966,\"msauto\":4583,\"pbrt-light-field-camera\":685,\"from-react-to-react-native\":864,\"prevent-mail-server-sending-spam\":2963,\"setup-x11-forwarding-on-debian\":990,\"high-dynamic-range-imaging\":3392,\"migrate-to-gmail-using-migration-tool\":410,\"migrate-to-gmail-migration-strategy\":361,\"panorama-image-stitching\":4553,\"coherent-line-drawing\":2518,\"about-taking-case\":453,\"multi-server-gpu-status-monitor\":1361,\"using-pytorch-in-windows\":1876,\"deep-coral\":5359,\"javascript-prototype-chain\":536,\"use-pytorch-to-solve-fizzbuzz\":2176,\"mbpr2015-repair\":1671,\"parallel-computing-using-slurm\":5,\"setup-slurm-cluster\":1030,\"monitoring-from-handcraft-to-prometheus\":275,\"ways-to-access-internal-network\":245,\"game-design-from-perspective-of-hacker\":550,\"timing-attack\":432,\"develop-hexo-plugin\":503,\"build-a-high-performance-computing-cluster-on-gcp\":547,\"photographing-solar-eclipse-with-only-a-smartphone\":358,\"setup-couchdb-using-docker-swarm\":536,\"share-storage-comparison\":457,\"optimizing-compile-time\":126}}"}],"posts":[{"title":"Run Unix-like Server on Android","slug":"run-unix-server-on-android","date":"2023-04-09T09:56:43.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2023/04/09/run-unix-server-on-android/","link":"","permalink":"https://ssarcandy.tw/2023/04/09/run-unix-server-on-android/","excerpt":"手機的硬體作為伺服器其實相當夠用。","text":"手機的硬體作為伺服器其實相當夠用。 最近正好汰換下來一台 Pixel 4a，正在想辦法將他的剩餘價值發揮到最大化。前篇 把舊的 Pixel 改造成無限照片上傳機 已經發揮了一些價值，但是不夠！這次我要用這個手機來取代掉我長久以來租的雲端虛擬機，把月租費省下來。 選擇安裝方法要用手機來取代雲端虛擬機器，其實還是有一些困難，首先家裡並沒有固定 IP，也就是說用手機來當作伺服器的話沒辦法在外面存取，如果有架設網站的需求的話那這肯定是行不通。不過我的情況剛好不需要固定 IP，我要的其實是一個開發機兼永不停機的伺服器讓我可以在上面跑一些定時的工作或者是跑一些需要長時間運作的程式。 那接下來就是要來思考我需要什麼，主流在安卓設備上面跑 Unix Server 有兩種方式，一種是使用 Termux[1]，另一種是直接安裝完整的系統在手機上[2]。Termux 的好處是安裝十分容易也不需要 root，壞處是他雖然是 Unix-like 的系統，但是算是閹割版，可能有少數一些指令或者 package 是沒辦法使用的；反之，直接安裝完整的系統那當然可以獲得最像是 Ubuntu 的體驗，但相對來講比較難裝較複雜，可能也免不了需要取得 root。 考量到基本上我只需要有可以開發及部屬 Python, node.js, C++ 的應用程式，所以我選擇使用 Termux 就好了。 我的需求我的需求可以簡單列出： 可以透過 ssh 存取 有 Python, node.js, C++ 開發環境 可以用 vscode 開發 Termux 真的十分強大，大部分的指令都跟在使用 Ubuntu 一樣簡單，但我還是遇到了一些坑，特此紀錄一下。 sshTermux 安裝好後，還需要安裝 openssh 等東西才能使我可以從外部 ssh 進去。 1234$ pkg install openssh$ vim $&#123;PREFIX&#125;/etc/ssh/sshd_config# PasswordAuthentication yes$ sshd sshd 也可以用 screen 等等的指令執行背景，就不會占用前景。另外也可以用 ssh key 的方式登入，就不需要每次打密碼。而 $PREFIX 是 Termux 的根目錄，一般常見的 /usr, /etc 等等路徑在 Termux 底下就會被對應至 $PREFIX/usr, $PREFIX/etc Dev Environments12345678# python$ pkg install python3# node.js$ pkg install nodejs-lts# C++$ pkg install glib cmake gdb 三行指令就安裝完絕大部分需要的工具。另外開發 C++ 時經常需要 libboost，很可惜沒有辦法直接透過 pkg install ，但可以用原始碼編譯： 123456789$ wget https://boostorg.jfrog.io/artifactory/main/release/1.81.0/source/boost_1_81_0.tar.gz$ tar xvf boost_1_81_0.tar.gz$ cd boost_1_81_0$ ./bootstrap.sh --prefix=$PREFIX$ ./b2# Copy header &amp; library to right place$ cp -a boost /data/data/com.termux/files/usr/include$ cp -a stage/lib/* /data/data/com.termux/files/usr/lib/ 安裝完後就可以用 cmake 找到 libboost. vscodevscode 就相當多坑了…首先，vscode 無法透過 Remote-SSH 套件連線到 Termux，因為缺少必要的一些東西 (libstdc++, glibc 是閹割版)[3][4]。所以可以改安裝 code-server。code-server 是一個 Web 版的 vscode，從介面到使用方式都與原生 vscode 十分相似，所以用這個也是完全可以接受的解決方式。 但要安裝 code-server 十分之困難，我後來找的一個全是韓文的影片才順利安裝成功： 1234567$ pkg install nodejs-lts python yarn binutils$ v=$(node -v); v=$&#123;v#v&#125;; v=$&#123;v%%.*&#125;;$ FORCE_NODE_VERSION=&quot;$v&quot; yarn global add code-server@4.6.0 --ignore-engines;# after installation done, launch it in screen$ screen -S code-server$ code-server --bind-addr 0.0.0.0:8080 --disable-telemetry 然後就可以打開瀏覽器 PRIVATE_IP:8080 即可看見熟悉的 vscode 畫面。 但是事情沒有這麼簡單…這 vscode 壞掉的地方可不少，他 built-in Terminal 打不開，搜尋功能無法使用，Source Control 裝死….經過一系列研究及嘗試後[5]，終於把全部都解決了… 1234567891011121314151617### bulit-in Terminal cannot open# [IPC Library: Pty Host] Unknown channel: ptyHost# rejected promise not handled within 1 second: Unknown channel: Channel name &#x27;ptyHost&#x27; timed out after 1000ms$ sed -i -e &#x27;s|switch(process.platform)|switch(&quot;linux&quot;)|&#x27; /data/data/com.termux/files/home/.config/yarn/global/node_modules/code-server/lib/vscode/out/vs/platform/terminal/node/ptyHostMain.js### Search not working# ...@vscode/ripgrep/bin/rg&#x27;: No such file or directory$ pkg install ripgrep$ cp /data/data/com.termux/files/usr/bin/rg .config/yarn/global/node_modules/code-server/lib/vscode/node_modules/@vscode/ripgrep/bin/rg.config/yarn/global/node_modules/code-server/lib/vscode/node_modules/@vscode/ripgrep/bin/rg### Source Control not working# The folder currently open doesn&#x27;t have a git repository$ pkg install git$ vim ~/.local/share/code-server/Machine/settings.json&#123; &quot;git.path&quot;: &quot;/data/data/com.termux/files/usr/bin/git&quot;&#125; 這些改完以後，記得重啟 code-server。 其他有用的訣竅Monitor CPU Loadinghtop 在 non-root Termux 是半殘的，看不到 CPU loading，這是由於 Android 系統級別的設定，讓一般應用程式 (Termux 也算應用程式) 無法存取到 /proc。所以可以改安裝 pkg install neofetch，他可以正確展示 CPU 使用量 (如本文首圖)，不知怎辦到的 哈哈。 Turn off System Task Killer在安裝好所有東西以後，隔天我就發現無法 ssh 到手機上了，一看才發現被系統殺掉了。現在比較新版本的 Android OS (12+) 都有嚴格的執行緒限制，應該是 32 左右，這數字太小了會導致 Termux 經常被殺掉。所以需要透過 adb 去把這個東西關掉[6]： 12345# For Android 12L &amp; Android 13:./adb shell &quot;settings put global settings_enable_monitor_phantom_procs false&quot;# For Android 12：./adb shell &quot;/system/bin/device_config set_sync_disabled_for_tests persistent; /system/bin/device_config put activity_manager max_phantom_processes 2147483647&quot; Adjust System TimeServer 通常都要是 UTC 時間會比較方便，結果這其實不需要在 Termux 內用指令調整，只需要去手機 設定&gt;時間&gt;選擇時區 即可。 Bind MAC address to Private IP由於手機是透過 WiFi 連上家用網路，private IP 是透過 DHCP 分配，但如果 IP 經常改變很不方便。可以通過家用 router 去設定 mac address 綁定分配的 private IP 來避免 IP 跑掉。(ASUS router 有這功能[7]，不確定其他廠牌有沒有) Compare with Cloud VM我原先在用的是 Nanode 1 GB (1 shared CPU + 1GB RAM)。記憶體只有 1GB 有時候會 Out of Memory 很不開心，效能方面我倒是覺得堪用。價格方面則是一個月 $5 USD，是 Linode 提供的虛擬機中最便宜的選擇[9]。做了一些簡單的效能測試，看看 Nanode 1 GB v.s. Pixel 4a 是誰贏？ 1234567[Task1. Compile libboost v1.81.0]Nanode = 705.27s user 48.27s system 96% cpu 12:57.18 totalPixel4a= 4236.25s user 218.47s system 691% cpu 10:44.36 total[Task2. Python check prime number]Nanode = 19.27s user 0.00s system 99% cpu 19.305 totalPixel4a= 25.62s user 0.01s system 99% cpu 25.681 total Task1 測試的所有 CPU 都滿載時的效能，很合理的 Pixel4a 有多顆 CPU 自然會贏過 Nanode;Task2 是用 Python 寫的 is_prime script[10] ，用於測試單一 CPU 的性能，就可以發現其實手機的 CPU 還是偏爛，有差到 30% References Termux Wiki 連筆電都懶得帶? 那就在 Android 上跑 VS Code 吧! | Termux , PRoot , VS Code Server Remote host prerequisites SSH to Termux not working. [Bug]: Terminal is not working on Termux. Fix [Process completed (signal 9) - press Enter] for Termux on Android 12+ devices [Wireless Router] How to manually assign IP around the DHCP list? Akamai’s Cloud Computing Services: Pricing Update Linode Pricing List Python check prime number h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"},{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"}]},{"title":"把舊的 Pixel 改造成無限照片上傳機","slug":"make-old-pixel-as-photo-uploader","date":"2023-04-08T07:48:18.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2023/04/08/make-old-pixel-as-photo-uploader/","link":"","permalink":"https://ssarcandy.tw/2023/04/08/make-old-pixel-as-photo-uploader/","excerpt":"自從 Google Photos 養套殺，取消高畫質無限上傳以後[1]，對於我這種習慣使用 Google Photos 的人無疑是重大悲劇，空間用完指日可待。目前 Google Photos 的高畫質無限免費上傳是專門給 Pixel 2~5 的優待，只要是由這些型號的裝置上傳的照片，就可以享有這項福利，並不限制照片需使用這些裝置拍攝所得。為了蹭 Pixel 舊裝置可以以“高畫質”無限免費上傳 Google Photos，即使換了新手機，也應該想辦法由舊裝置來上傳相片。","text":"自從 Google Photos 養套殺，取消高畫質無限上傳以後[1]，對於我這種習慣使用 Google Photos 的人無疑是重大悲劇，空間用完指日可待。目前 Google Photos 的高畫質無限免費上傳是專門給 Pixel 2~5 的優待，只要是由這些型號的裝置上傳的照片，就可以享有這項福利，並不限制照片需使用這些裝置拍攝所得。為了蹭 Pixel 舊裝置可以以“高畫質”無限免費上傳 Google Photos，即使換了新手機，也應該想辦法由舊裝置來上傳相片。 這邊所謂 ”高畫質” (Storage Saver) 其實是有壓縮的，目前只有 Pixel 1 可以免費 ”原始畫質” (Original) 上傳，但對我而言有免費高畫質就很好了，直接省下訂閱 Google One 的費用。 如何達成這個任務呢？基本上的流程會是以下三個步驟： 新手機拍攝照片 新手機將照片同步至舊手機 舊手機自動備份至 Google Photos 但在這裡，需要注意的是，新手機不要開啟 Google Photos 備份功能，以免佔用雲端空間。 接下來，我們需要尋找一個雲端相片同步服務來當作中繼站，讓新手機拍攝的照片同步至舊手機的暫存空間，以利後續的備份。在我的實作中，我選擇了 Mega 和 MegaSync。 流程圖。 上面流程圖簡單的展示了整個流程，接下來就是把需要的東西準備好： Pixel 5 or an earlier device Google Photos 需安裝於新、舊手機 Mega 需安裝於新、舊手機 MegaSync 需安裝於舊手機 Mega 是一個雲端儲存服務，提供免費的 50GB 儲存空間[2]，註冊且下載至新手機以後以後，只要開啟 Camera Upload [3] 功能即可。空間方面也不用擔心耗盡，一方面是他有 50 GB，另一方面是我只是要將它當作一個暫存中繼站，只要後續備份完成，這邊的照片即可清空。 而 MegaSync 是一個第三方 app，功能類似於 rsync ，可以將 Mega 上面的指定資料夾下的檔案同步至手機地端，所以只要設定好要同步 Camera Upload 資料夾，就可以利用這個機制去下載我新手機所拍攝的照片到舊手機上。 至此，新手機拍攝的相片已經同步至舊手機，這時候只需要在舊手機的 Google Photos 開啟 “高畫質” 備份選項，一切就大功告成。 另外為了維持整個流程穩定，最好要把 舊手機上將 MegaSync 的電池最佳化關閉， 新手機上將 Mega 的電池最佳化關閉 才不會被系統限制導致無法在背景執行這些程式。 總結來說，透過這樣的改造，我們可以輕鬆地享有 Google Photos 的高畫質無限免費上傳功能，並且也能賦予舊裝置一個新的功能而非擺在抽屜閒置。只要按照以上的步驟進行操作，就可以輕鬆地完成一台無限照片上傳機的改造。 References Google 相簿儲存空間政策更新 2020 年以後註冊的新帳戶，免費空間改為 20GB，可透過一些任務升級至 50GB How to use camera uploads? h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"},{"name":"automation","slug":"automation","permalink":"https://ssarcandy.tw/tags/automation/"}]},{"title":"Optimizing C++ Compile Time","slug":"optimizing-compile-time","date":"2022-06-11T10:26:10.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2022/06/11/optimizing-compile-time/","link":"","permalink":"https://ssarcandy.tw/2022/06/11/optimizing-compile-time/","excerpt":"編譯是靜態語言不可避免的步驟。對於開發者而言，編譯是個又愛又恨的東西，好處是他可以幫助在編譯時期找出部分的錯誤又可以幫忙最佳化，但是壞處則是編譯要時間，當專案越來越大時，小小改個檔案可能就要花數分鐘去等編譯。 source: https://xkcd.com/303/","text":"編譯是靜態語言不可避免的步驟。對於開發者而言，編譯是個又愛又恨的東西，好處是他可以幫助在編譯時期找出部分的錯誤又可以幫忙最佳化，但是壞處則是編譯要時間，當專案越來越大時，小小改個檔案可能就要花數分鐘去等編譯。 source: https://xkcd.com/303/ 隨著 C++ 的發展，現在 modern C++ 如 C++14, 17 等等，新增了更多方式讓開發者在編譯時期完成更多事情，比如說更方便的if contexpr等等功能。而這其實也是被鼓勵的，因為能在編譯時期就處理完的話就可以讓 runtime 執行得更快！ 但當大量使用 template 或引用更多的 library 也讓 compiler 的工作越來越多，而如果每改幾行就要等待編譯幾分鐘才能知道執行結果的話，對於一天要編譯數百次的開發者而言實在是太浪費生命了。 本篇文章就要來探討各種加速 C++ Compile Time 的方式，大部分的方法都是 Stack Overflow 搜刮來，然後由我自行實測。測試環境如下： Ubuntu 18.04 LTS GCC 9 CMake 3.23 Ninja 1.8 Project LOC ~20k Use ccache引入 ccache 絕對是效益最高的加速方式，完全不用改程式就可以減少大量的編譯時間。ccache 是一個全域的 compiler cache，藉由快取編譯的中繼檔來節省重新編譯的時間。安裝好以後只要在 CMakeLists.txt 中加入: 12# CMakeLists.txtSET_PROPERTY(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ccache) 即可使用ccache，如果專案沒使用 build tools 的話，則是直接在gcc指令前加上ccache 1234# before$ /usr/bin/gcc main.cpp# after$ ccache /usr/bin/gcc main.cpp 使用 ccache 之後整體編譯速度大約可以提升兩倍以上，十分讚！ Use forward declaration as more as possibleC++ 的 #include 關鍵字其實就是複製貼上，所以當你在 A.h include 了 B.h，在預處理階段編譯器會把 B.h 內容複製到 A.h，而如果不幸的 B.h 又 include 一堆檔案，那也會通通展開。所以如果引用太多檔案，除了會造成預處理之後檔案肥大以外，也會造成檔案之間相依性混亂，間接導致每次編譯要重新編譯不必要的檔案。 除了將沒用的 include 清乾淨以外，還可以更激進的避免在 header include 東西，那就是利用 forward declaration。 include tree 試想以上情境，當你變更 A.h 時，A B C 都必須重新編譯，因為內容改變了，但實際上 C 並未使用到 A，其實應該可以避免重新編譯 C。 由於 C 會重新編譯是因為 B.h 內容改變了，而 B.h 內容改變的原因則是因為 A.h 更新了。這時候可以檢視為甚麼 B.h 需要引用 A.h，看看是否可以避免引用。 12345678/// B.h#include &quot;A.h&quot;class B &#123; // ...skip private: const A&amp; a;&#125;; 以上是常見的使用情境，B 存了一個 class A 的參考A&amp; a。 我們可以改寫成這樣，將 include 移至 B.cpp 實作檔中。這是因為A&amp;, A*等這類東西的大小是固定的，所以在定義時不需要知道實際 class A 的大小，只需先告知 compiler 有這個 class 即可。 12345678910/// B.hclass A; //&lt; forward declare !class B &#123; // ...skip private: const A&amp; a;&#125;;/// B.cpp#include &quot;A.h&quot; 如此一來，當你變更 A.h 時，B.h 內容並不會改變，也就不會觸發 C 需要重新編譯拉，可喜可賀~ 在大量使用這個技巧以後，我所測試的專案進步幅度也是非常明顯，更動 A.h 原本會牽動 54 個檔案需要重編譯，改完以後則只會牽動 29 個檔案，自然編譯速度也就變快了。 12345# before use fwd v.s. after use fwd# -j 6 incremental build, w/o ccache, unit in second[touch A.h]before = 303 (trigger 54 files rebuild)after = 178 (trigger 29 files rebuild) Unity BuildUnity build 又稱 Jumbo build, Mega build，其原理是透過將*.cpp彙整成一個all.cpp再一起執行編譯，這樣就是省下 N 個檔案的編譯時間 (具體而言是省下如 template 展開等原本每個 Translate Unit 都要做的事情)。 CMake v3.16 開始就支援 Unity Build 的設定，他支援將 batch size 個檔案先匯總成all_x.cpp之後再進行編譯。 不過這方法會遇到一些問題，由於這方法之原理說白了就是cat *.cpp &gt; all.cpp如此暴力，如果專案本身常常使用全域變數的話，這會很容易導致 ODR (One definition rule) 錯誤。所以也有可能不容易引入 Unity Build。 這個技巧我認為也是 CP 值十分之高的方法，幾乎不用改程式 (如果專案用太多全域變數就要改很多😅) 卻可以獲得大幅的進步。我測試的結果如下，可以看到無論是 incremental build 還是 clean build 都取得 50% 以上的進步。 123456789# w/o unity build v.s. with unity build (batch_size=8)# using -j 6, w/o ccache, unit in seconds[touch A.h]before = 242 # 38 tasksafter = 167 # 18 tasks[clean build]before = 420 # 111 tasksafter = 224 # 47 tasks Better linker編譯的最後階段是 linking，這部分可以替換成比較厲害的 linker，市面上目前有三種較有名的 linker ld (gcc default) gold lld 要替換使用 linker 只需要在 compile flag 加上fuse-ld=&lt;linker_name&gt;即可。詳細可參考 gcc document。而我實測不同 linker 表現如下， 123456# rebuild using single thread, unit in second# [1/1] Linking CXX executable main[linker]ld = 25.4gold = 11.6lld = 5.8 使用更強的 linker 雖然使 linking time 進步許多，但對整個專案的 compile time 而言其實佔比不是很大，相較於前面幾個章節算是進步較小的技巧。(但 CP 值也是很高，只要改一個 compile flag) Disable var-tracking for huge variable object我們可以透過 gcc flag -ftime-report來剖析編譯各個階段的耗時，然後針對各個耗時大的改善。 我測試的專案中，有一個 auto-generate 的unordered_map，該檔案動輒數萬行，每次編譯該檔案都會成為瓶頸。從-ftime-report得知編譯該檔案耗時最大的部分是 var-tracking，var-tracking 是讓 debug info 有更多資訊的功能，但當專案中有巨大的變數時，這會讓 compiling 速度大幅變慢。 在對我那個數萬行的unordered_map檔案拿掉 var-tracking 之後 (針對該檔案加上一個-fno-var-trackingflag) 結果如下， 123456789101112131415161718192021222324252627# gcc -ftime-report auto_gen.cpp# with var-tracking v.s. without var-tracking, sorted by usr time[before]Time variable usr sys wall GGC phase opt and generate : 122.95 ( 92%) 2.30 ( 35%) 125.26 ( 89%) 924305 kB ( 46%) var-tracking dataflow : 71.39 ( 53%) 0.15 ( 2%) 71.57 ( 51%) 3714 kB ( 0%) expand vars : 17.55 ( 13%) 0.03 ( 0%) 17.56 ( 12%) 8583 kB ( 0%) phase parsing : 8.09 ( 6%) 3.46 ( 53%) 11.55 ( 8%) 794986 kB ( 40%) alias stmt walking : 6.11 ( 5%) 0.08 ( 1%) 6.40 ( 5%) 678 kB ( 0%) template instantiation : 4.35 ( 3%) 1.58 ( 24%) 6.03 ( 4%) 443040 kB ( 22%) phase lang. deferred : 2.30 ( 2%) 0.72 ( 11%) 3.02 ( 2%) 232700 kB ( 12%) var-tracking emit : 2.87 ( 2%) 0.02 ( 0%) 2.95 ( 2%) 20420 kB ( 1%) |overload resolution : 3.18 ( 2%) 1.26 ( 19%) 4.50 ( 3%) 330116 kB ( 16%) TOTAL : 134.16 6.54 140.82 2005866 kB[after]Time variable usr sys wall GGC phase opt and generate : 44.61 ( 80%) 1.41 ( 27%) 46.03 ( 76%) 724840 kB ( 41%) expand vars : 18.45 ( 33%) 0.02 ( 0%) 18.46 ( 30%) 8567 kB ( 0%) phase parsing : 8.32 ( 15%) 3.12 ( 59%) 11.45 ( 19%) 794986 kB ( 45%) alias stmt walking : 6.39 ( 12%) 0.11 ( 2%) 6.52 ( 11%) 678 kB ( 0%) template instantiation : 4.38 ( 8%) 1.44 ( 27%) 5.93 ( 10%) 443040 kB ( 25%) |overload resolution : 3.27 ( 6%) 0.97 ( 18%) 4.51 ( 7%) 330116 kB ( 19%) phase lang. deferred : 2.27 ( 4%) 0.70 ( 13%) 2.97 ( 5%) 232700 kB ( 13%) parser (global) : 1.89 ( 3%) 0.90 ( 17%) 3.05 ( 5%) 211250 kB ( 12%) tree SSA incremental : 1.58 ( 3%) 0.01 ( 0%) 1.55 ( 3%) 259 kB ( 0%) TOTAL : 55.49 5.27 60.81 1761326 kB 結果是從原本耗時 134 秒降低至耗時 55 秒，減少超過 50% 的時間。這也使得該檔案不會再是整個專案的瓶頸。 Summary本文嘗試了許多技巧來加速編譯所需的時間，總結各點如下列： Use ccache [big improvement] Use forward declaration as more as possible [big improvement] Unity Build [big improvement] Use LLVM linker [good improvement] Disable var-tracking for huge variable object [good improvement] Pre-compiled headers [no improvement] Explicit template instantiation [no improvement] 在爬文時網友提及 pre-compiled headers 以及 explicit (extern) template 也對減少編譯時間有幫助，但實測並未有顯著差異，故本文未提及，也許實際上是有用只是剛好不適用於我的環境之類的。 References Improving Compilation Time of C&#x2F;C++ Projects “variable tracking” is eating my compile time! CMake Unity Build h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"cmake","slug":"cmake","permalink":"https://ssarcandy.tw/tags/cmake/"}]},{"title":"Share Storage Comparison","slug":"share-storage-comparison","date":"2021-05-10T07:36:52.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2021/05/10/share-storage-comparison/","link":"","permalink":"https://ssarcandy.tw/2021/05/10/share-storage-comparison/","excerpt":"Share storage，諸如家用 NAS 或工作站常有的網路硬碟(NFS)，是一個很方便又隨處可見的一種存儲空間，方便的點在於可以從不同機器存取或修改同樣的資料、避免需要維護重複的東西在不同的機器上；常見的使用情境像是會將 /home 家目錄放置於 Share storage 中，然後再掛載至所有工作站的機器上，這樣就可以讓使用者無論是登入哪一台工作站機器，都可以保持同樣的家目錄環境，十分方便。","text":"Share storage，諸如家用 NAS 或工作站常有的網路硬碟(NFS)，是一個很方便又隨處可見的一種存儲空間，方便的點在於可以從不同機器存取或修改同樣的資料、避免需要維護重複的東西在不同的機器上；常見的使用情境像是會將 /home 家目錄放置於 Share storage 中，然後再掛載至所有工作站的機器上，這樣就可以讓使用者無論是登入哪一台工作站機器，都可以保持同樣的家目錄環境，十分方便。 除了前言提到的使用情境以外，其實常見的使用方式也有像是拿來做為平行運算寫入的位置，或者用於存放大量資料集(dataset)給需要的人方便讀取(像是之前 CML 就是這樣用)。而諸如此類的應用其實就很考驗 Share storage 的讀寫性能，若性能不佳可能常常會壞掉，導致所有有掛載的機器都會陷入無法存取的窘境。 而本篇目的就是想要測測看市面上常見的 share storage 解決方案以及他們各自的寫入性能，做個大PK。底下列舉的就是我打算要測試的幾種 share storage (由於純讀取通常效能都很好，所以這邊只測寫入效能) Gluster file system v7 using SSD Ceph file system v13.2 using SSD Google Filestore (2T Premium Tier) IBM File Storage (6T Type Endurance) AWS EFS 其中前兩個(Gluster, Ceph)需要自建，後三個(GCP, AWS, IBM)則是直接用雲端解決方案。 測試的方式我就用原始又有效的 dd 指令來試啦，基本上腳本長這樣： 1234#!/bin/bashf=`hostname`rand=`head /dev/urandom | tr -dc A-Z | head -c 5 ; echo &#x27;&#x27;`sync &amp;&amp; dd if=/dev/zero of=/mnt/testfile_$&#123;f&#125;_$&#123;rand&#125; bs=10M count=100 oflag=direct 2&gt;&amp;1 | cat 這腳本有幾個參數可以調整： bs - block size 指單一寫入區塊的大小 count - 寫幾個區塊 of - 寫到哪裡，/mnt/ 就是我們要測試的 share storage 的目錄 bs*count 就是總寫入大小，我這邊就統一使用 bs&#x3D;10MB, count&#x3D;100, 總計寫入 1GB。 而這個腳本執行的結果會像是這樣: 123456$ bash test.sh100+0 records in100+0 records out1048576000 bytes (1.0 GB, 1000 MiB) copied, 4.59822 s, 128 MB/s # ^^^^^^^^# 這就是寫入速度 但這樣還不夠，要測試到極限的話，需要平行化同時執行一堆這個腳本，寫爆 share storage！ Gluster Ceph GCP IBM AWS 100x10MB, 10 jobs 43 12 67 76 13 100x10MB, 20 jobs 12 6 29 66 6 100x10MB, 40 jobs 9 6 21 33 2 Gluster Ceph GCP IBM AWS 100x10MB, 10 jobs 43.6 12.1 67.2 76.5 13.6 100x10MB, 20 jobs 12.0 6.2 29.3 66.1 6.1 100x10MB, 40 jobs 9.0 6.0 21.1 33.9 2.8 ▲ 不同 storage 寫入效能比較[1][2] 結果如上圖所示，自建的系統其實表現都遜於雲端方案(尤其是同時平行寫入很多時)，這不排除是因為我安裝 Gluster &#x2F; Ceph 時基本上都是用預設的設定，沒有特別研究怎樣最佳化😱；而 AWS EFS 表現奇差，這有原因，底下詳述…。除此之外又穩又快的就是 GCP Filestore 以及 IBM File Storage 了，其中 IBM File Storage 在同時平行寫入很多時表現比較穩定，所以在這邊是 IBM 表現最佳🏆。 AWS EFS AWS 的 EFS 的讀寫流量是有做限制的，基本上就是如果存越多資料在 EFS 裡面，則讀寫流量越高 (bursting mode)，這也造成基礎流量超低，如果用的儲存空間只有 100G，則讀寫流量大概落在 5 MB&#x2F;s，每增加 100GB 的空間用量會增加 5 MB&#x2F;s 的速度[3]，是個很妙的設計。在我這次的測試中，由於 EFS 基本上是空的，所以讀寫效能自然低下。 References 單位為 MB&#x2F;s jobs 意指同時執行幾個寫入腳本，故若同時執行 10jobs，平均寫入可有 20MBps，則瞬時寫入速度為 200 MBps AWS EFS - Scale and performance table { font-size: 14px; } #mychart { margin-bottom: 30px; } #mychart .data { position: relative; top: -20px; } #mychart tr { height: 250px; margin: 0 auto; } #mychart .legend { flex-direction: row; justify-content: space-between; border: initial; } .charts-css.column.show-labels { --labels-size: 3rem; }","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"cloud","slug":"cloud","permalink":"https://ssarcandy.tw/tags/cloud/"}]},{"title":"Setup CouchDB Cluster using Docker Swarm","slug":"setup-couchdb-using-docker-swarm","date":"2021-01-25T19:36:10.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2021/01/25/setup-couchdb-using-docker-swarm/","link":"","permalink":"https://ssarcandy.tw/2021/01/25/setup-couchdb-using-docker-swarm/","excerpt":"CouchDB，一個主打安裝好之後就可以直接有原生 Http API 進行 CRUD (新增、讀取、修改、刪除) 的 NoSQL 資料庫，對於較簡單的應用程式甚至就直接免去後端的開發成本，直接對接 CouchDB Http API 介面即可。 除此之外，CouchDB 同時也主打所謂的 muti-master cluster 架構，可以輕易地設定多個 CouchDB instances 來達到 HA 的目的，確保服務不會因為伺服器掛掉而無法存取。 而本篇就是在記錄如何透過 Docker Swarm 來佈署跨機器的三個 CouchDB 並且將之設定為 cluster mode.","text":"CouchDB，一個主打安裝好之後就可以直接有原生 Http API 進行 CRUD (新增、讀取、修改、刪除) 的 NoSQL 資料庫，對於較簡單的應用程式甚至就直接免去後端的開發成本，直接對接 CouchDB Http API 介面即可。 除此之外，CouchDB 同時也主打所謂的 muti-master cluster 架構，可以輕易地設定多個 CouchDB instances 來達到 HA 的目的，確保服務不會因為伺服器掛掉而無法存取。 而本篇就是在記錄如何透過 Docker Swarm 來佈署跨機器的三個 CouchDB 並且將之設定為 cluster mode. Prerequisites在開始之前，由於我是打算要用 docker swarm 做跨機佈署，所以首先要先準備好環境： 三台有不同 Public IP 的 Linux server 三台都裝好 docker 三台機器都設為 docker swarm mode 如何將 docker 設定成 swarm mode 可以參考文件，基本上就只要： 12345# init swarm$ docker swarm init# add into existing swarm$ docker swarm join --token &lt;token&gt; 設定好之後可以用 docker node ls 確認一下，結果會類似如下： 12345$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION2v2lb55cyes0rf3tbtqe2zp9x * docker-node-1 Ready Active Reachable 19.03.132gedpa6dac3c80ilr3f9ji3fw docker-node-2 Ready Active Leader 19.03.137zj2xk3up7ce34atj2nme9rf9 docker-node-3 Ready Active Reachable 19.03.13 Setup CouchDB as Single Node我們要使用的會是官方的 docker image — couchdb:3.1.1 要使用其實不難，這邊可以示範一下在本機佈署 single node 的方式，docker-compose.yml 123456789101112131415version: &quot;3.8&quot; services: couchdb: environment: COUCHDB_USER: &quot;admin&quot; COUCHDB_PASSWORD: &quot;admin123&quot; COUCHDB_SECRET: 46d689495ca02e8c35c3a3f683000ef1 NODENAME: &quot;couchdb01&quot; ERL_FLAGS: &quot;-setcookie a20b37d83ef18efce400b3ace400036e&quot; image: couchdb:3.1.1 ports: - &quot;5984:5984&quot; - &quot;9100:9100&quot; - &quot;4369:4369&quot; 可以透過 docker compose 來嘗試執行這個檔案： 12345$ docker-compose up -d$ docker-compose psName Command State Ports -----------------------------------------------------------------------------------------------------------------------------------couchdb_couchdb_1 tini -- /docker-entrypoint ... Up 0.0.0.0:4369-&gt;4369/tcp, 0.0.0.0:5984-&gt;5984/tcp, 0.0.0.0:9100-&gt;9100/tcp 然後就可以訪問 CouchDB 內建的管理介面: http://&lt;IP&gt;:5984/_utils/，接下來到 setup 頁面依照指示設定 single node，即可。 去 Verify 介面測試是否完成安裝。 至此就完成 Single Node CouchDB 的安裝，可喜可賀。 Deploy as CouchDB Cluster Mode剛剛嘗試了一鍵佈署 single node 的 CouchDB，那接下來就來嘗試主角吧 — Cluster Mode CouchDB 的 cluster mode 設定比起 single node 來的複雜非常多，而且存在許多坑 (都不會有 error log 的坑)，我這邊就是紀錄我摸索無數夜晚得出的結果😵 首先我先列出要設定 cluster mode 必須要滿足的條件： 每個 CouchDB 必須要有一樣的 admin &amp; password &amp; secret &amp; erl cookie，這對應到 docker image 的 COUCHDB_USER, COUCHDB_PASSWORD, COUCHDB_SECRET, ERL_FLAGS 每個 CouchDB 必須要可以透過 NODENAME 來互相溝通 每個 CouchDB 必須要有同樣的 uuid Prepare config.ini為了要保證大家的 Config 一致，這邊我要用事先準備好的 config.ini，而非透過 yml 的 environment 傳參數，這個方法也是官方建議的方法[1]: The best way to provide configuration to the %%REPO%% image is to provide a custom ini file to CouchDB, preferably stored in the /opt/couchdb/etc/local.d/ directory. There are many ways to provide this file to the container (via short Dockerfile with FROM + COPY, via Docker Configs, via runtime bind-mount, etc), the details of which are left as an exercise for the reader. 那接下來就來準備 config.ini： 12345678[admins]admin = -pbkdf2-07fe7c8d94281cafdfa065c0f9dd9b6fae56b649,8a3bfe04b1f4294d89d9e9d250fce77a,10[couch_httpd_auth]secret = 46d689495ca02e8c35c3a3f683000ef1[couchdb]uuid = 7ff6dd245116a7288b798b003f00099e 這邊就有一個坑，就是 admin 的 password 必須要是 hash 版本的，如果這邊是 plain text 的話，在啟動時 CouchDB 會自動做 hash，然後就會導致三台 CouchDB 的 password 不一致 (同樣的密碼 hash 的結果會不一樣，相關文章： Timing Attack in String Compare )；密碼不一致就會出現 unable to sync admin passwords 錯誤。 那關於要如何獲取 hash 過的密碼，官方是推薦透過建立一個 dummy 的 single node，然後去看他 hash 出來的密碼長怎樣，再 copy 過來 (好蠢…) 這邊提供另一個方法，如這篇文章[2]所說，可以透過 python script 產生 hashed password： 123$ PASS=&quot;admin123&quot; SALT=&quot;8a3bfe04b1f4294d89d9e9d250fce77a&quot; ITER=10 \\ python3 -c &quot;import os,hashlib; print(&#x27;-pbkdf2-%s,%s,%s&#x27; % (hashlib.pbkdf2_hmac(&#x27;sha1&#x27;,os.environ[&#x27;PASS&#x27;].encode(),os.environ[&#x27;SALT&#x27;].encode(),int(os.environ[&#x27;ITER&#x27;].encode())).hex(), os.environ[&#x27;SALT&#x27;], os.environ[&#x27;ITER&#x27;]))&quot;-pbkdf2-07fe7c8d94281cafdfa065c0f9dd9b6fae56b649,8a3bfe04b1f4294d89d9e9d250fce77a,10 Using same config across nodes官方說了三種方式提供 ini 檔： via Dockerfile COPY — 這太蠢了，每次改 config 都要重新 build via Docker Config — ok 👍 via runtime mount — 在 docker swarm 比較不適合，因為不是所有 node 都能夠 mount 所以其實只剩下 Docker Config 較適當。 docker config 設定方式如下， 12345678910services: couchdb: # ...skip configs: - source: couchdb_conf target: /opt/couchdb/etc/local.d/config.ini configs: couchdb_conf: file: ./config.ini 我們將事先準備好的 config.ini 透過 docker config 掛載至所有 CouchDB 的 local.d 資料夾。 這裡有另一個坑，有可能會出現 CrashLoopBackOff 的狀況，我嘗試發現根本沒辦法掛載到 /opt/couchdb/ 底下的任何目錄，大概是 bug 吧，這邊有相關 GitHub issue[3]。 為了繞過這個不能 mount 的問題，必須要覆寫 entrypoint，先把 config file copy 到適當的位置再執行原本的 entrypoint，所以會改成這樣： 12345678910services: couchdb: # ...skip entrypoint: /bin/bash -c &quot;cp -f /couchdb_conf /opt/couchdb/etc/local.d/couch.ini &amp;&amp; tini -- /docker-entrypoint.sh /opt/couchdb/bin/couchdb&quot; configs: - couchdb_conf configs: couchdb_conf: file: ./config.ini 這樣改意思是先把 config 掛載至別的地方，然後在執行 entrypoint 時先 copy 至正確位置之後再執行原本的指令。 Join All CouchDB Instances as a Cluster至此我們已經可以撰寫出完整的 docker-swarm.yml 1234567891011121314151617181920212223242526272829version: &quot;3.8&quot; services: couchdb: environment: NODENAME: &quot;&#123;&#123;.Service.Name&#125;&#125;.&#123;&#123;.Task.Slot&#125;&#125;.&#123;&#123;.Task.ID&#125;&#125;&quot; ERL_FLAGS: &quot;-setcookie a20b37d83ef18efce400b3ace400036e&quot; image: couchdb:3.1.1 deploy: mode: global networks: network: aliases: - couchdb ports: - &quot;5984:5984&quot; - &quot;9100:9100&quot; - &quot;4369:4369&quot; entrypoint: /bin/bash -c &quot;cp -f /couchdb_conf /opt/couchdb/etc/local.d/couch.ini &amp;&amp; tini -- /docker-entrypoint.sh /opt/couchdb/bin/couchdb&quot; configs: - source: couchdb_conf target: /opt/couchdb/etc/local.d/config.ini networks: network: configs: couchdb_conf: file: ./couchdb-conf.ini 這邊我用 global mode 是因為我希望每一台機器恰好只有一個 CouchDB。(上面的 yml 我也沒有掛 volume 所以資料會在 container 不見時一起消失)。 NODENAME: \"{{.Service.Name}}.{{.Task.Slot}}.{{.Task.ID}}\" 則是因為透過 docker swarm 佈署時，他的命名規則就是長這樣，在 docker 裡面是可以透過這個 nodename ping 到別台的。要查詢 docker container name 的話只要 docker ps 就可以看到。 接下來就直接佈署： 123456$ docker stack deploy -c docker-swarm.yaml test$ docker stack ps test ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ynacuaj8tx35 test_couchdb.7zj2xk3up7ce34atj2nme9rf9 couchdb:3.1.1 docker-node-3 Running Running 25 seconds ago5p5w38jtjh7z test_couchdb.2v2lb55cyes0rf3tbtqe2zp9x couchdb:3.1.1 docker-node-1 Running Running 25 seconds agobekjdetq739z test_couchdb.2gedpa6dac3c80ilr3f9ji3fw couchdb:3.1.1 docker-node-2 Running Running 26 seconds ago 每個 node 都起來之後就可以去做最後的設定，CouchDB 設定 Cluster 的方式是透過 admin http api 去把其他 CouchDB 加進某台。 12345678$ curl -X PUT &quot;http://admin:admin123@&lt;IP&gt;:5984/_node/_local/_nodes/couchdb@test_couchdb.2v2lb55cyes0rf3tbtqe2zp9x.strqjl8lsdm58tozn59mp8du7&quot; -d &#123;&#125;&#123;&quot;ok&quot;:true,&quot;id&quot;:&quot;couchdb@test_couchdb.2v2lb55cyes0rf3tbtqe2zp9x.strqjl8lsdm58tozn59mp8du7&quot;,&quot;rev&quot;:&quot;1-967a00dff5e02add41819138abb3284d&quot;&#125;$ curl -X PUT &quot;http://admin:admin123@&lt;IP&gt;:5984/_node/_local/_nodes/couchdb@test_couchdb.7zj2xk3up7ce34atj2nme9rf9.u5ce5bl7cmjlhkb2781cye7py&quot; -d &#123;&#125;&#123;&quot;ok&quot;:true,&quot;id&quot;:&quot;couchdb@test_couchdb.7zj2xk3up7ce34atj2nme9rf9.u5ce5bl7cmjlhkb2781cye7py&quot;,&quot;rev&quot;:&quot;1-967a00dff5e02add41819138abb3284d&quot;&#125;$ curl -X POST -H &quot;Content-Type: application/json&quot; &quot;http://admin:admin123@&lt;IP&gt;:5984/_cluster_setup&quot; -d &#x27;&#123;&quot;action&quot;: &quot;finish_cluster&quot;&#125;&#x27;&#123;&quot;ok&quot;:true&#125; 都加好之後，可以透過 /_membership 檢查是否正確： 12345678910111213$ curl &quot;http://admin:admin123@&lt;IP&gt;:5984/_membership&quot;&#123; &quot;all_nodes&quot;: [ &quot;couchdb@test_couchdb.2gedpa6dac3c80ilr3f9ji3fw.irqxr1k8e9v8xekae1xtuxxab&quot;, &quot;couchdb@test_couchdb.2v2lb55cyes0rf3tbtqe2zp9x.strqjl8lsdm58tozn59mp8du7&quot;, &quot;couchdb@test_couchdb.7zj2xk3up7ce34atj2nme9rf9.u5ce5bl7cmjlhkb2781cye7py&quot; ], &quot;cluster_nodes&quot;: [ &quot;couchdb@test_couchdb.2gedpa6dac3c80ilr3f9ji3fw.irqxr1k8e9v8xekae1xtuxxab&quot;, &quot;couchdb@test_couchdb.2v2lb55cyes0rf3tbtqe2zp9x.strqjl8lsdm58tozn59mp8du7&quot;, &quot;couchdb@test_couchdb.7zj2xk3up7ce34atj2nme9rf9.u5ce5bl7cmjlhkb2781cye7py&quot; ]&#125; 這樣就設定完成啦🎉 (記得再去管理介面 verifyinstall 檢查一次) Test High Availability設定好 cluster 之後就要來驗證 HA 是否正常，這邊測試的方法會是先在某台 CouchDB 新增資料，理論上其他台也會可以存取這筆資料： 先建立一個新 database 以及一個新 document： 12345$ curl -X PUT &quot;http://admin:admin123@&lt;server01&gt;:5984/mydatabase&quot; &#123;&quot;ok&quot;:true&#125;$ curl -X PUT &quot;http://admin:admin123@&lt;server01&gt;:5984/mydatabase/01&quot; -d &#x27;&#123;&quot;key&quot;: &quot;val&quot;&#125;&#x27; &#123;&quot;ok&quot;:true,&quot;id&quot;:&quot;01&quot;,&quot;rev&quot;:&quot;1-00e36163fac5c61bb681fef0c52528e2&quot;&#125; 接下來這個 document 會自動 replicated 到其他兩台 CouchDB，可以透過分別 curl 每一台來驗證是否有一樣的 document： 12345678$ curl &quot;http://admin:admin123@&lt;server01&gt;:5984/mydatabase/01&quot; &#123;&quot;_id&quot;:&quot;01&quot;,&quot;_rev&quot;:&quot;1-00e36163fac5c61bb681fef0c52528e2&quot;,&quot;key&quot;:&quot;val&quot;&#125;$ curl &quot;http://admin:admin123@&lt;server02&gt;:5984/mydatabase/01&quot;&#123;&quot;_id&quot;:&quot;01&quot;,&quot;_rev&quot;:&quot;1-00e36163fac5c61bb681fef0c52528e2&quot;,&quot;key&quot;:&quot;val&quot;&#125;$ curl &quot;http://admin:admin123@&lt;server03&gt;:5984/mydatabase/01&quot;&#123;&quot;_id&quot;:&quot;01&quot;,&quot;_rev&quot;:&quot;1-00e36163fac5c61bb681fef0c52528e2&quot;,&quot;key&quot;:&quot;val&quot;&#125; 這樣即使任意機器掛掉，整個系統都還是可以維持運作。 (實務上再去疊一層 Load Balancer 讓 Http endpoint 統一會更方便) Conclusion設定 single node 很簡單，但設定 cluster mode 頗複雜，我個人覺得 error log 沒有非常完整，很多各式各樣的坑都會直接死掉根本不會有任何 log，很崩潰…😱。 References Configuring CouchDB How to generate password hash for CouchDB administrator Configuration from docker config or secret? #73 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"docker","slug":"docker","permalink":"https://ssarcandy.tw/tags/docker/"}]},{"title":"用手機拍攝日食到底可不可行","slug":"Photographing-Solar-Eclipse-with-only-a-Smartphone","date":"2020-06-28T10:01:05.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2020/06/28/Photographing-Solar-Eclipse-with-only-a-Smartphone/","link":"","permalink":"https://ssarcandy.tw/2020/06/28/Photographing-Solar-Eclipse-with-only-a-Smartphone/","excerpt":"這次六月二十一日台灣正好有機會觀測到難得一見的日環食，我當然也是必不缺席，直接跑去嘉義的香湖公園餐與這次盛會。雖然我一直在腦中規劃這次該如何觀測，但實際要採購時還是太晚根本買不到巴德膜，最後只買到十個日食觀測眼鏡….慘… 而這麼小的日食觀測眼鏡也不太容易直接套在單眼相機鏡頭前，所以我最後就索性決定這次只使用我的手機拍攝，順便來實測看看用手機拍攝日食到底可不可行？ 日環食時間序列，下排左一算是有拍到貝里珠。器材：Google Pixel 4 + 太陽濾鏡","text":"這次六月二十一日台灣正好有機會觀測到難得一見的日環食，我當然也是必不缺席，直接跑去嘉義的香湖公園餐與這次盛會。雖然我一直在腦中規劃這次該如何觀測，但實際要採購時還是太晚根本買不到巴德膜，最後只買到十個日食觀測眼鏡….慘… 而這麼小的日食觀測眼鏡也不太容易直接套在單眼相機鏡頭前，所以我最後就索性決定這次只使用我的手機拍攝，順便來實測看看用手機拍攝日食到底可不可行？ 日環食時間序列，下排左一算是有拍到貝里珠。器材：Google Pixel 4 + 太陽濾鏡 一直以來天文攝影都是件昂貴的興趣，我個人其實除了以前在天文社能夠接觸到厲害的器材之外，畢業之後其實自己也沒有自行購買單眼或望遠鏡的裝備，簡單來說我器材都是蹭飯借來的…🤣 手機鏡頭跟單眼鏡頭其實最大的差別就是鏡頭大小，這直接影響了天體的進光亮，當然也直接影響訊躁比，這個差別在一般的天體攝影或星野攝影都是手機鏡頭巨大的弱勢，但日食則是例外，我們永遠不會擔心拍攝太陽的進光亮不足，只擔心進光亮太足燒了我的鏡頭🔥。 雖然手機鏡頭小對拍攝日食影響不大，但要用手機拍攝太陽還是必須解決許多問題，比如說： 減光。鏡頭雖小，太陽還是有可能燒壞手機鏡頭。 手動控制拍照參數。手機軟體不見得會提供對手動對焦功能。 長焦攝影。一般手機拍攝視角都偏廣，如何拍攝清楚的太陽是也是一個問題。 關於這幾個問題，我就分享我自己的做法： 減光太陽是個世界亮的東西，不減光肯定什麼鬼都拍不到。最簡單的減光方式就是把路上會發的日食觀測眼鏡剪下來貼在手機鏡頭前面，就可以了~🎉我是貼在手機殼上面，這樣也方便拆卸。 做工很醜的減光片。 減光前。 減光後，太陽超小。 手動控制拍照參數手機相機軟體通常都沒有提供手動控制快門時間、對焦、ISO 或其他參數，這部分也是我無法解決的部分，我的相機是不提供手動對焦的，所以我只能想辦法放大一點，然後跟智障一樣一直狂按太陽祈禱他會心血來潮對到焦…。我大概拍攝了 400 來張，真的夠清晰的不到 100 張，慘慘慘。 另外假設手機有支援儲存 RAW 檔的話一定要存 RAW 檔！RAW 檔可以保留照片更多資訊及細節，在後面要做影像的細部調整或拉曲線等等都較不易失真。 長焦攝影太陽其實只有半度的視角，不放大根本只占照片中小小一顆的，如上圖右一。要透過手機提高有效焦長，一是可以購買額外手機望遠配件 (感覺配件店不少類似的，但品質未知。)；二是目前不少手機有長焦鏡可以做到光學或數位變焦 (像是 iPhone 可以 2x 光學變焦) 也是可以湊合著用。而我是直接用 Pixel 4 數位變焦到八倍，順便測測他宣稱的 super res zoom 效果。 放大後效果還不錯，而且也比較好對焦。 其他突發狀況剛剛上面提到的幾點都是可以預想到並且先預練過，但真的上場時還是遇到其他突發狀況，像是我開拍不到半小時手機就直接熱到當機，然後直接關機掰掰….。只好趕緊在旁對手機搧風吹氣待它降溫再強行開機，旁邊的人一定覺得我是怪人😂 熱到當機這狀況直到食分夠大時，氣溫明顯下降後就改善許多哈哈。 小結就我這次的拍攝結果來說，我覺得其實目前的手機很多應該有辦法拍攝到跟本篇同等程度的照片，雖然離單眼或真正專業的器材所拍出來的還是有差距，但對於一日天文迷或沒錢的天文迷，其實用自己的手機也是有辦法記錄下這些特殊天象的。我個人覺得最難克服的會是手動對焦及熱到當機的問題，這兩者能解決就可以很輕鬆的拍攝了。 日食結束後上 PTT 發現有這篇文 [問卦] 手機拍太陽！？！？ ，本文應該算能夠解答這問卦~ 日環食跟偏食完全是不同的體驗，在食既到復圓之間，周遭氣溫、環境光變化十分明顯，感覺就像是突然變黃昏那樣暗但天空卻沒有黃昏的橘色，很難形容的光線；溫度也突然變涼爽。且湖面上閃耀著暗暗的光(?)，整個世界都感覺很不自然。🤯 環食那刻全場歡呼，我也親眼見證了那神奇的一刻，絕對比我之前看過的任何天象都神奇，不枉費我曬的 6 小時的太陽。","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"}]},{"title":"Build a High Performance Computing Cluster on GCP","slug":"Build-a-High-Performance-Computing-Cluster-on-GCP","date":"2020-04-21T19:26:24.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2020/04/21/Build-a-High-Performance-Computing-Cluster-on-GCP/","link":"","permalink":"https://ssarcandy.tw/2020/04/21/Build-a-High-Performance-Computing-Cluster-on-GCP/","excerpt":"敝司長久以來都是自建 Cluster 來做為日常運算資源使用的，長時間一直都被擴充性、I&#x2F;O 效能所卡住，若是卡在運算資源不夠光是採買新機器動輒要幾個月；被 share storage 的 I&#x2F;O 效能卡住更慘，整個系統會慢到爆 job 都卡住…所以我就在想有沒有辦法在雲上建立這種高效能運算叢集，想動態擴張幾台就幾台，再加上我相信無論是 AWS, GCP 等等大平台所提供的 NFS 服務應該都差不到哪裡去吧…?","text":"敝司長久以來都是自建 Cluster 來做為日常運算資源使用的，長時間一直都被擴充性、I&#x2F;O 效能所卡住，若是卡在運算資源不夠光是採買新機器動輒要幾個月；被 share storage 的 I&#x2F;O 效能卡住更慘，整個系統會慢到爆 job 都卡住…所以我就在想有沒有辦法在雲上建立這種高效能運算叢集，想動態擴張幾台就幾台，再加上我相信無論是 AWS, GCP 等等大平台所提供的 NFS 服務應該都差不到哪裡去吧…? 本文旨在在 Google Cloud Platform 上建立由 Slurm[1] 管理的運算叢集，並且也建立一個 NFS 自動掛載在所有 compute nodes 上面供大家讀取及寫入。 在開始之前要先釐清這個 cluster 需要的東西，基本上是下列： A client VM Slurm controller N nodes of Slurm computing node NFS that mount on all computing nodes 整體圖大概長這樣，基本上參考 google 的架構[2]，只是加上一個 NFS 架構圖。Client VM 也需要掛載 NFS Create a Share Storage首先先來建立一個儲存空間來當作 NFS ，用於掛載在所有 Nodes 上，這樣才可以在任何地方存取同樣的資料，我這邊選用 Google Filestore 因為他的 I&#x2F;O 會比一般 Google 的 pd-standard 或 pd-ssd 來的好[3] 建立這個就不太需要介紹了，就跑去 GCP console 案案案就好了，其中一個要注意的是 authorized VPC network 如果沒特別需要，可以選 default 會比較簡單。 案案案就會得到這個 Setup Slurm Cluster接下來要來佈屬 Slurm controller, compute nodes, client VM 到 GCP 上，這邊其實已經有整個模板了，github 連結。這個可以直接用 gcloud 佈署，但在此之前需要先改改 config ，主要要改的是 slurm-cluster.yaml cluster_name: 愛取啥取啥 zone: 既然在台灣就選 asia-east1-b vpc_net: 這要填 default，不填的話會自動建立 &#123;cluster_name&#125;-network 的 VPC vpc_subnet: 填 default 吧 controller_machine_type: 如果 compute node 打算超過 100 台的話可以選好一點 network_storage: 這邊要掛載剛剛建立的 nfs 其他就參考以下吧 12345678910111213141516171819202122232425262728imports:- path: slurm.jinja resources:- name: slurm-cluster type: slurm.jinja properties: cluster_name: cloud-slurm zone: asia-east1-b vpc_net: default vpc_subnet: default controller_machine_type: n1-standard-8 controller_disk_type: pd-standard controller_disk_size_gb: 20 login_machine_type: n1-standard-2 network_storage: - server_ip: &lt;filestore ip&gt; remote_mount: /slurm_nfs local_mount: /j fs_type: nfs compute_image_machine_type: n1-standard-2 ompi_version: v3.1.x partitions: - name: fast machine_type: n1-standard-2 max_node_count: 100 zone: asia-east1-b vpc_subnet: default 另外一個要改的是 scripts/startup.sh ，這個 script 是 VM 啟動會執行的，由於我們用到 NFS 所以要安裝 nfs package: 12345678# scripts/startup.sh PACKAGES=( &#x27;bind-utils&#x27; # ... skip &#x27;yum-utils&#x27; &#x27;nfs-utils&#x27; # we need this one for mount nfs ) 我個人認為最大的雷就是 filestore 跟 slurm cluster 必須在同一個 VPC 才能掛載，然後這個 slurm template 不指定 vpc 他會幫你建一個 (不是用 default)，所以我一開始搞一直不同 vpc 掛不起來… Deploy &amp; Test It設定通通解決後接下來就是佈署上 GCP 了，基本上透過 gcloud 就可以了:這邊可以 clone 我改過的設定檔 (跟上面說的設定一樣)，記得要填 nfs IP 12345678910$ git clone https://github.com/SSARCandy/slurm-gcp.git$ cd slurm-gcp$ gcloud deployment-manager deployments create gcp-slurm --config slurm-cluster.yamlWaiting for create [operation-5a3cd5941e0b1-f13e780b-ba00af15]...done.Create operation operation-5a3cd5941e0b1-f13e780b-ba00af15 completed successfully.NAME TYPE STATE ERRORS INTENTcloud-slurm-asia-east1-router compute.v1.router COMPLETED []cloud-slurm-compute-0-image compute.v1.instance COMPLETED []cloud-slurm-controller compute.v1.instance COMPLETED []cloud-slurm-login0 compute.v1.instance COMPLETED [] 接下來要等一下大概五分鐘，因為要建立一個 compute node 的 image，之後要自動擴展 compute node 時使用。 完成後可以在 console 上看到 待一切就序之後就可以登入試試，可以看到 slurm_nfs 也有掛載在上面： 1234567891011121314$ gcloud compute ssh cloud-slurm-login0 --zone=asia-east1-b$ df -hFilesystem Size Used Avail Use% Mounted ondevtmpfs 3.6G 0 3.6G 0% /devtmpfs 3.6G 0 3.6G 0% /dev/shmtmpfs 3.6G 8.5M 3.6G 1% /runtmpfs 3.6G 0 3.6G 0% /sys/fs/cgroup/dev/sda2 20G 2.8G 18G 15% //dev/sda1 200M 12M 189M 6% /boot/eficloud-slurm-controller:/home 20G 4.4G 16G 22% /homecloud-slurm-controller:/apps 20G 4.4G 16G 22% /appscloud-slurm-controller:/etc/munge 20G 4.4G 16G 22% /etc/munge10.173.83.218:/slurm_nfs 2.5T 9.9G 2.4T 1% /jtmpfs 732M 0 732M 0% /run/user/1993390025 利用 sinfo 可以查看 slurm cluster status，有一百台等著被使用。這一百台都是假的，要等到有人真的發 job 才會建立，然後閒置太久就會被關掉，是個很省錢的方式呢～ 123$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELISTfast* up infinite 100 idle~ cloud-slurm-compute-0-[0-99] 發送 jobs 時才會動態建立 computing nodes，閒置過久會刪掉 試試看發很多個會寫檔到 NFS 的 job，先建立個 slurm_filewriter.sh，這個檔案會執行寫入 1GB 的資料到 NFS 裡。 12345678910#!/bin/bash##SBATCH --job-name=io-performannce#SBATCH --output=out_%j#SBATCH --ntasks=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=1f=`hostname`rand=`head /dev/urandom | tr -dc A-Z | head -c 5 ; echo &#x27;&#x27;`sync &amp;&amp; dd if=/dev/zero of=/j/root/testfile_$&#123;f&#125;_$&#123;rand&#125; bs=10M count=100 oflag=direct 2&gt;&amp;1 | cat 然後透過 sbatch 發 jobs，關於 slurm 用法可參考這個小抄 123456789101112131415# send 5 parallel jobs that write file to NFS$ for i in `seq 5`; do sbatch slurm_filewriter.sh ; doneSubmitted batch job 203Submitted batch job 204Submitted batch job 205Submitted batch job 206Submitted batch job 207 # after squeue shows no job, can see that files has been written to NFS$ ls -1 /j/roottestfile_cloud-slurm-compute-0-0_CIDCRtestfile_cloud-slurm-compute-0-0_GKCQMtestfile_cloud-slurm-compute-0-0_XDIAQtestfile_cloud-slurm-compute-0-0_XJHTWtestfile_cloud-slurm-compute-0-2_VPSEW 至此整個 slurm cluster 就佈署到 GCP 上了，可以開始開心使用啦～這樣的優點包含可以無上限擴充運算資源，再也不用等待！NFS I&#x2F;O 根據我的測試也是完勝自架的分散式儲存系統！壞處則是可能也許會有些貴…(?) References https://slurm.schedmd.com/documentation.html https://codelabs.developers.google.com/codelabs/hpc-slurm-on-gcp https://cloud.google.com/compute/docs/disks h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; } 雜談 噫！好了！我畢業了！","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"slurm","slug":"slurm","permalink":"https://ssarcandy.tw/tags/slurm/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"},{"name":"cloud","slug":"cloud","permalink":"https://ssarcandy.tw/tags/cloud/"}]},{"title":"Develop a Plugin for Hexo","slug":"develop-hexo-plugin","date":"2020-02-09T05:42:04.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2020/02/09/develop-hexo-plugin/","link":"","permalink":"https://ssarcandy.tw/2020/02/09/develop-hexo-plugin/","excerpt":"這陣子心血來潮花了點時間整理敝部落格的原始碼，整理的過程發現其實可以把一些小工具獨立成模組，一方面可以讓 codebase 變精簡，另一方面則是抽出來的模組也可以給其他 Hexo 使用者使用。一開始我以為只是搬移一下程式、剪剪貼貼就可以完成，後來越做越不對勁…原來要做一個 Hexo Plugin 也是滿多地方需要注意的。最後的成品叫做 hexo-tag-photozoom，有興趣歡迎用用看～","text":"這陣子心血來潮花了點時間整理敝部落格的原始碼，整理的過程發現其實可以把一些小工具獨立成模組，一方面可以讓 codebase 變精簡，另一方面則是抽出來的模組也可以給其他 Hexo 使用者使用。一開始我以為只是搬移一下程式、剪剪貼貼就可以完成，後來越做越不對勁…原來要做一個 Hexo Plugin 也是滿多地方需要注意的。最後的成品叫做 hexo-tag-photozoom，有興趣歡迎用用看～ History of this Feature我這次要搬移的是我原本實作在 theme 裡面的功能，就是可以把內文圖片放大的功能，非常像 Medium 網站上按圖片會有的效果。效果如下： 按圖片會有跟 Medium 網站一樣的效果 (小螢幕裝置沒有) 原本我是直接在 theme 中使用 @nishanths 的 zoom.js，直接引用他的 sciprt，並自己註冊一個 Hexo tag： title: layout.yml123&lt;!-- in theme layout file include external resource --&gt;&lt;script src=&quot;./js/zoom.js&quot;&gt;&lt;/script&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;./css/zoom.css&quot;&gt; title: scripts/helper.js12345678910/** * &#123;% zoom /path/to/image [/path/to/thumbnail] [title] %&#125; */ hexo.extend.tag.register(&#x27;zoom&#x27;, (args) =&gt; &#123; const &#123; thumbnail, original, title &#125; = parse(args); return ` &lt;div&gt; &lt;img src=&quot;$&#123;(thumbnail || original)&#125;&quot; alt=&quot;$&#123;title&#125;&quot; data-action=&quot;zoom&quot; class=&quot;photozoom&quot;&gt; &lt;/div&gt;`;&#125;); 透過 hexo.extend.tag.register 可以註冊新的 tag 語法，可以直接在文章 markdown 中使用，這個 tag 本身把 {% zoom %} 轉換成完整的 html 格式，並且由於已經在前端引用 zoom.js library，所以就可以正常運作。 Move to Seperate Module原本的作法是直接在 layout 中引用 zoom.&#123;js,css&#125; library，這當然可行，但當要把這功能模組化時，是沒辦法直接接觸 layout 的 (除非你要在 readme 裡面叫使用者自己引用…)，所以必須要有個方式把這些必要的外部資源塞進去使用者的 html 裡面。關於這段「如何把外部資源塞到使用者的靜態檔中」，我參考了其他 plugin 的做法，發現大部份都是使用 hexo.extend.generator 來達成。不過我最後選擇其他做法來完成這件事。 Use Hexo GeneratorHexo 在編譯資源時，提供多種方式註冊自己的程式，來達到高度客製化。其中 Generator 是用來產生檔案對應的路由，所以 Generator 都是回傳 &#123; path: &#39;foo&#39;, data: &#39;foo&#39; &#125; 的格式，代表著 path 對應的 data 是什麼。透過 Generator 可以做到 copy file 的功能，官方網站也有提供範例，再搭配註冊 tag ，就可以達成動態插入必要的外部資源。 123456789101112131415161718192021// generator that create a virtual path to external filehexo.extend.generator.register(&#x27;asset&#x27;, function(locals)&#123; return [&#123; path: &#x27;zoom/zoom.js&#x27;, data: () =&gt; fs.createReadStream(&#x27;/path/to/zoom.js&#x27;), &#125;, &#123; path: &#x27;zoom/zoom.css&#x27;, data: () =&gt; fs.createReadStream(&#x27;/path/to/zoom.css&#x27;), &#125;];&#125;); // register tag that include generator&#x27;s pathhexo.extend.tag.register(&#x27;zoom&#x27;, (args) =&gt; &#123; const &#123; thumbnail, original, title &#125; = parse(args); return ` &lt;script src=&quot;/zoom/zoom.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;/zoom/zoom.css&quot;&gt; &lt;div&gt; &lt;img src=&quot;$&#123;(thumbnail || original)&#125;&quot; alt=&quot;$&#123;title&#125;&quot; data-action=&quot;zoom&quot; class=&quot;photozoom&quot;&gt; &lt;/div&gt;`;&#125;); 如此一來，每當使用者插入 {% zoom %} 時，就會被展開成包含 include 外部資源的 html code，來達成目的。 Use Hexo Filter Inject Code然而剛剛的方式有些小缺點，比方說當使用者插入很多 {% zoom %} 的 tag 時，就會出現很多重複引用的程式碼，感覺也是怪怪的。所以我最後利用另一種方式達到塞 code 的效果 － Filter。 Hexo Filter 提供很多 hook 的註冊點，比方說在渲染 html 之前執行註冊的 function …等等。我這邊用的是 after_generate，就是在全部檔案產生完成之後執行，詳細 hook 名稱跟意義可參考文件 透過 after_generate filter 我可以在最後決定是否要插入外部資源 zoom.&#123;js,css&#125;，實作如下： 12345678910111213141516171819202122232425hexo.extend.filter.register(&#x27;after_generate&#x27;, () =&gt; &#123; const route = hexo.route; const routes = route.list().filter(path =&gt; path.endsWith(&#x27;.html&#x27;)); const map = routes.map(path =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; const html = route.get(path); let htmlTxt = &#x27;&#x27;; html.on(&#x27;data&#x27;, chunk =&gt; (htmlTxt += chunk)); html.on(&#x27;end&#x27;, () =&gt; &#123; const $ = cheerio.load(htmlTxt, &#123; decodeEntities: true &#125;); if ($(&#x27;.photozoom&#x27;).length) &#123; $(&#x27;body&#x27;).append(`&lt;script type=&quot;text/javascript&quot;&gt;$&#123;fetch_asset(ZOOMJS_PATH)&#125;&lt;/script&gt;`); $(&#x27;body&#x27;).append(`&lt;style&gt;$&#123;fetch_asset(ZOOMCSS_PATH)&#125;&lt;/style&gt;`); hexo.log.info(`[hexo-tag-photozoom] Injected assets to $&#123;path&#125;`); &#125; resolve(&#123; path, html: $.html() &#125;); &#125;); &#125;); &#125;); // update route return Promise.all(map).then(res =&gt; res.map(obj =&gt; &#123; route.set(obj.path, obj.html); &#125;), ); &#125;); 我去掃所有 html 檔案，並搜尋有沒有 div class name 是 photozoom 的，如果有那就直接在 html body 插入所需的 javascript 跟 css 程式碼，非常暴力但還不錯～且這作法同時兼顧如果有使用者想在非文章內容的地方使用 zoom.js 的效果，只需要在 &lt;img&gt; 中加上 photozoom class name，在每次編譯時都會掃到並在需要的地方插入程式碼。 References Hexo Filter Develop a plugin for Hexo - Github Card h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://ssarcandy.tw/tags/nodejs/"},{"name":"hexo","slug":"hexo","permalink":"https://ssarcandy.tw/tags/hexo/"},{"name":"javascript","slug":"javascript","permalink":"https://ssarcandy.tw/tags/javascript/"}]},{"title":"Timing Attack in String Compare","slug":"timing-attack","date":"2020-01-29T11:28:29.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2020/01/29/timing-attack/","link":"","permalink":"https://ssarcandy.tw/2020/01/29/timing-attack/","excerpt":"程式語言通常在執行的時候，為了要最佳化執行的速度，常常會利用所謂的 Early Return。[1]比如說條件式裡面 if (a &amp;&amp; b) 這種判斷式，假設已經知道 A == false，那其實就可以不需要知道 b 的值，如此就可以直接忽略 b 而達到更快的知道這個判斷式是否為真[2]。同樣的這種作法其實會發生在很多地方，比方說像是比對兩個字串是否一樣：在很多程式語言中的實作其實就是遍歷兩個字串比對每一個字元，那只要過程中有一個字元不一樣那這兩個字串肯定就是不一樣，即可提早返回結果。","text":"程式語言通常在執行的時候，為了要最佳化執行的速度，常常會利用所謂的 Early Return。[1]比如說條件式裡面 if (a &amp;&amp; b) 這種判斷式，假設已經知道 A == false，那其實就可以不需要知道 b 的值，如此就可以直接忽略 b 而達到更快的知道這個判斷式是否為真[2]。同樣的這種作法其實會發生在很多地方，比方說像是比對兩個字串是否一樣：在很多程式語言中的實作其實就是遍歷兩個字串比對每一個字元，那只要過程中有一個字元不一樣那這兩個字串肯定就是不一樣，即可提早返回結果。 底下是 C 的 strcmp 程式碼片段實作比較兩個字串是否一樣： 123456789101112131415int strcmp(const char *p1, const char *p2){ const unsigned char *s1 = (const unsigned char *) p1; const unsigned char *s2 = (const unsigned char *) p2; unsigned char c1, c2; do { c1 = (unsigned char) *s1++; c2 = (unsigned char) *s2++; if (c1 == '\\0') return c1 - c2; } while (c1 == c2); return c1 - c2;} 從上面的邏輯可以看出來，如果第二個字元就不一樣的話，那我們馬上就可以結束整個邏輯然後返回兩個字串不一樣的結果，如此就能提升程式執行的速度。而本文就是要來探討這種字串比對的方式所衍生的其他的安全性問題，也就是所謂的 Timing Attack Timing Attack Timing Attack 其實就是所謂的時間差攻擊。 12345678do{ c1 = (unsigned char) *s1++; c2 = (unsigned char) *s2++; if (c1 == '\\0') return c1 - c2;}while (c1 == c2); 再來看一下剛剛字串比對的實作中的迴圈，由於這個迴圈實作的關係我們可以知道不同字串比對其實會花不一樣的時間，這很合理因為有時候比較字串到一半的時候我們就已經知道這兩個字串不一樣，所以提早返回結果。那這樣到底有什麼安全性的問題呢？ 試想，今天在輸入密碼的時候輸入錯了，結果電腦告訴你：「喔你第三個字元錯了。」這樣其實蠻奇怪的吧？這表示假設駭客想要猜你的密碼，基本上他就可以先亂猜第一個字元，猜對之後再繼續猜一個字元…以此類推，那勢必可以破解密碼。 這個例子跟我們剛剛的字串比對其實基本上是同一件事情，因為如果你輸入密碼是錯的其實會比輸入正確密碼來的花更少的時間，因為錯誤密碼可能前幾個字元就錯所以提早返還結果。雖然這時間上的差異幾乎微乎其微，但是只要多做幾次然後再平均一下，還是可以得出有意義的結果。底下是一段程式碼來證明，只要多跑幾次就可以發現字串中不一樣的那個字元如果越後面字串比對的時間就會明顯有差異，利用這樣的資訊就可以慢慢推出答案的字串。 12345678910111213141516std::vector&lt;std::string&gt; str{ \"x2345\", \"1x345\", \"12x45\", \"123x5\", \"1234x\",}; for (auto ss: str) { auto start = std::chrono::system_clock::now(); for (int i = 0; i &lt; 1e8; i++) { strcmp(\"12345\", ss.c_str()); } std::chrono::duration&lt;double&gt; diff{std::chrono::system_clock::now() - start}; std::cout &lt;&lt; ss &lt;&lt; \": \" &lt;&lt; diff.count() &lt;&lt; \" s\\n\";} 這段程式會吐出以下的結果： 12345x2345: 4.07176 s1x345: 4.89044 s12x45: 5.79689 s123x5: 6.84836 s1234x: 7.37571 s 可以發現，比較的字串越後面才不一樣，花費越長的時間。這就是 Timing Attack 的主要概念。 In Real World那在實務上這個漏洞會出現在哪裡呢？其實要先知道這個漏洞的意義：必須要是那個答案字串是敏感資料，像是密碼、或者某種 Token。 如果不是敏感資料那就算可以間接猜出來也是沒有什麼意義。 Account/Password Login那就先來說說看最常見的密碼比對好了，現在隨處可見什麼帳號密碼登入，這種東西會不會踩到這個漏洞呢？就我的知識來講：基本上是不會。因為假設是一個正常的後端工程師，他們不會去做所謂的密碼明文儲存。密碼這種東西即使在資料庫裡面也不會是明文儲存的，少說也是會經過一次雜湊而且還要加鹽。[3] 在資料庫存的會是 而非 。 使用者輸入帳號密碼的時候，伺服器端會透過同樣的雜湊邏輯，就可以得出跟資料庫儲存的一樣的雜湊，這樣就完成一個正常的密碼驗證。也由於 hashing 會讓輸入的字串跟得到的雜湊有很不一樣的結果，即使只改輸入的密碼一個字元，得到的 hash 也會完全不一樣。這樣的機制導致 Timing Attack 在這個例子上就完全沒有用了，因為攻擊者根本不能預期真正在做字串比對的那個雜湊是不是如攻擊者預期的一個字元一個字元改變，那如此即使有時間上的差異，也跟第幾個字元比對失敗沒有直接的關係。 Request Signature那到底有沒有其他例子是真的會需要注意這個漏洞的呢？我能想到的大概就是像是某些加密貨幣交易所，他們的 API 幾乎都需要做所謂的簽章。概念如下：交易所需要透過 API key/secret 確保這個請求是來自合法的使用者，所以每個請求都必須附帶上簽章，公式大概是這樣： 這個的用意是使用者利用自己的私鑰去加密請求的參數，來證明自己是真的自己。伺服器端則會用使用者本次請求的參數加上使用者的私鑰來去重組 Signature，假設 Signature 跟請求端附帶的一樣，那就是合法的請求。在駭客的角度，由於沒有使用者的私鑰所以沒有辦法用正規途徑得到 Signature，但是利用 Timing Attack 這招就可以猜出本次請求所對應的 Signature 從而達到偽造身份的效果。但這有幾個不實際的地方： 利用 Timing Attack 需要大量的嘗試，但通常伺服器端會有 rate limit，根本沒辦法在合理的時間猜到答案。 再者，這種 Signature 加密機制都會再帶一個所謂的 nonce[4]，所以其實實務上也很難有辦法利用 Timing Attack。 講白了這個攻擊手段我個人覺得看起來很厲害但其實沒這麼可怕。除非是菜鳥工程師，不然實務上不太可能做出會被這個攻擊手段影響的系統… References Return early and clearly https://arne-mertz.de/2016/12/early-return/ 這個例子其實是所謂的 Order of evaluation, 跟 Early return 有一點不同。https://en.cppreference.com/w/cpp/language/eval_order Adding Salt to Hashing: A Better Way to Store Passwords nonce 可以有效避免重送攻擊。 (重送攻擊我常常用，可參考我的另一篇文章 從奧客玩家視角看遊戲防禦性設計) 另外補充一篇也是在介紹 Timing Attack 的文 Using Node.js Event Loop for Timing Attacks h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"},{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"}]},{"title":"從奧客玩家視角看遊戲防禦性設計","slug":"game-design-from-perspective-of-hacker","date":"2019-10-02T15:31:16.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2019/10/02/game-design-from-perspective-of-hacker/","link":"","permalink":"https://ssarcandy.tw/2019/10/02/game-design-from-perspective-of-hacker/","excerpt":"常常聽到某某遊戲外掛猖獗，作弊仔沒品、破壞遊戲樂趣等等行徑。身為一個好奇寶寶，總是很想知道其中奧秘，雖然我沒有厲害到開發出什麼外掛程式，但這幾年也陸續發現多款遊戲設計上的缺陷，成功用非正當的方式遊玩遊戲 (好含蓄…XD)。看著看著也發現其實這些遊戲的開發者理論上有辦法防止這類型的攻擊，所以只好用我不專業的見解寫下這篇文章。 對沒錯我就是奧客玩家！ 本文提及的遊戲只代表曾經可以作弊，不代表現在或未來也可以。","text":"常常聽到某某遊戲外掛猖獗，作弊仔沒品、破壞遊戲樂趣等等行徑。身為一個好奇寶寶，總是很想知道其中奧秘，雖然我沒有厲害到開發出什麼外掛程式，但這幾年也陸續發現多款遊戲設計上的缺陷，成功用非正當的方式遊玩遊戲 (好含蓄…XD)。看著看著也發現其實這些遊戲的開發者理論上有辦法防止這類型的攻擊，所以只好用我不專業的見解寫下這篇文章。 對沒錯我就是奧客玩家！ 本文提及的遊戲只代表曾經可以作弊，不代表現在或未來也可以。 說到這幾年陸續看到有隙可趁的遊戲，方法不外乎是兩個，一是藉由封包攔截查看內容再藉由修改封包資訊重送來攻擊，二是直接更改記憶體位置的值，這兩個方法都是非常直接暴力，案例很多，容我一一介紹。 Metal Slug Attack 這遊戲是我的啟蒙導師，他教會我好多網路傳輸知識。金幣 99999999+ 是基本 其實這遊戲我已經寫過一篇文章了，但容我再次鞭屍 🤣 這遊戲是十分經典只要會察看網路封包，一定有辦法使用重送攻擊。到底什麼是查看封包再重送呢？來看看他的遊戲模式：基本上是關卡制塔防遊戲，勝利可以拿到獎勵。所以當我攔截到開始跟結束的封包訊息之後，我就可以直接自動刷關了。 12345678910echo \"Start event level=HELL......$i\"send_request \\ \"cover=1&amp;deck_no=12&amp;stage_id=$stage_id&amp;unit_ids[]=604&amp;unit_ids[]=485&amp;unit_level[]=50&amp;unit_level[]=50\" \\ \"https://msattack.snkplaymore.info/event/marathon2nd/battle_start/?kpi1=$stage_id\" &gt; /dev/nullsend_request \\ \"stage_id=$stage_id&amp;battle_time=25&amp;drop_num=$drop_num\" \\ \"https://msattack.snkplaymore.info/event/marathon2nd/battle_win/\" &gt; /dev/nullecho \"Win event level=HELL........$i\" 可以看到所有的參數都是透過明文傳輸，唯一的保護是有使用 https，但這樣並無法防止封包被查看，因為只要建立 Proxy server 並自己簽署一個 SSL 憑證，還是可以偷看 https 的封包。那如果我是開發者我會怎麼改善呢？比較基本的是加上 nonce 在 request 裡面並加密參數。nonce 本身是一個遞增數字，伺服器端須檢查 nonce 值要是遞增的不然就非合法。這種作法常見於加密貨幣交易所的 API 設計，這也是防止重送攻擊常見的手段。[1] 遊戲王 Duel Links曾經風靡一陣子的遊戲王卡牌遊戲，同時也是伺服器設計有嚴重缺陷，嚴重到我覺得他們工程師可以東西收一收的程度。這遊戲我開服三天就把全腳色練到滿等，同時集齊當時所有 UR SR 卡片，兩個禮拜後覺得太無聊直接棄坑，堪稱我玩過時間最短的遊戲。 Blue Eyes White Dragon!! 我有一打!! 其實他們犯了常見的錯誤，就是所謂 Atomic 操作原則。有些事情是必須一起完成的，否則就應當全部都不算數。比如說：「我用十元買了一枝筆」，那我的錢包應該要減少十元，並且增加一枝筆。這其中每個動作缺一不可：如果錢包沒扣錢，則整段交易應該都要失敗並回復到原本的狀態。這很基本很合理。此遊戲一樣是遊玩扣體力的機制，藉由有限體力來使一般人無法狂練等。但查看封包後發現，他開始關卡跟扣體力是兩個不同的 API，是透過遊戲先發送開始再發送扣體力的訊息，這樣一來我只要一直狂重送開始但不送扣體力，我就可以無限遊玩刷等。真的太扯了….. 另外在設計 server side service 時，有一個大原則就是不可以相信客戶端送來的資料，永遠必須做驗證。 而且客戶端不該發送結果，應該只發送欲執行的操作。用剛剛的例子來說：「我用十元買了一枝筆，我的錢包減少十元，並且增加一枝筆。」→ 由客戶端計算結果是不可以的。「我要用十元買一枝筆。」→ 只發送欲執行的操作，實際結果應由伺服器端執行，才能確保合法性。 唉，本遊戲真的是奇觀。 Messanger 籃球、足球Messanger 兩年前推出可以在聊天室內玩籃球的彩蛋，浪費了我無數個小時在那邊投籃，其實也是有辦法作弊的。前面提到我目前知道的方法不外乎是藉由封包攔截重送來攻擊或是直接更改記憶體位置的值，facebook 在中間人攻擊下了不少工夫，至少就我的觀察只要使用任何 Proxy server 似乎都會無法正常使用 Messanger，所以封包攔截重送這條路就很難走了。 剩下試試直接更改記憶體位置吧，基本上原理就是掃過全部此應用程式用到的記憶體位置並尋找指定的數值。比如說投籃好了，投進 1 分時先尋找記憶體中數值等於 1 的，再投進一次就進一步搜尋數值等於 2 的記憶體位置，以此類推直到精確地找到代表分數的記憶體位置，再修改其值。 找到對的記憶體位置，成為灌籃高手不成問題！ 很遺憾的是如果遊戲是單機遊戲，基本上是不可能防止記憶體竄改的[2]，只能讓他變更難修改沒辦法完全防止。比較常見的做法是增加一個 dirty check 的檢查值，比如說分數的平方。在這樣的設計下，當你的分數被竄改成 時，你的檢查值如果不是 ，那就代表本次分數不合法。但說實在的這種方式防君子不防小人，真的有耐心的還是找得出檢查值並加以修改…。 不過其實這類的防禦性設計有時候根本不見得需要，以我上述提到的例子而言，除了遊戲王缺陷太嚴重以外，其實其他的都不見得需要做改善。像是 Metal Slug Attack 無法做到數據的篡改，能做到的只有自動花體力刷關；而 Messanger 的遊戲根本是類單機遊戲，就算拿高分也只是自爽。 所以到頭來，最有趣的還是尋找系統缺失的過程吧…😅 References Cryptographic nonce How do I prevent memory-modification cheats? h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"}]},{"title":"Ways to Access Internal Network","slug":"Ways-to-Access-Internal-Network","date":"2019-08-11T19:02:06.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2019/08/11/Ways-to-Access-Internal-Network/","link":"","permalink":"https://ssarcandy.tw/2019/08/11/Ways-to-Access-Internal-Network/","excerpt":"一般的公司或者實驗室都會隔離內部資源，只留一個統一的對外出口來達到比較高的安全性以及讓系統管理者較好控管。但是較高的安全性總是會帶來不便，這種隔離內部資源的架構導致我們無法直接存取內部資源，要透過一些拐彎抹角的方式來存取。","text":"一般的公司或者實驗室都會隔離內部資源，只留一個統一的對外出口來達到比較高的安全性以及讓系統管理者較好控管。但是較高的安全性總是會帶來不便，這種隔離內部資源的架構導致我們無法直接存取內部資源，要透過一些拐彎抹角的方式來存取。 從以前當網管到現在工作一陣子之後，因為常常在家工作(加班?)，也累積了不少存取內部資源的方式，本篇就是紀錄一下這些方式，以免我這個金魚腦以後又忘記…. VPN使用 VPN 來存取 LAN 資源我想是最簡單直覺的了，前提是公司或實驗室有提供 VPN server 嘿。阿假設你是系統管理者且你們想提供 VPN 服務，我會推薦使用 OpenVPN server，簡單易用。然後我之前有看到一個大神寫了個一鍵架設 VPN server 的 script這個 script 從安裝，新增&#x2F;刪除使用者， 應有盡有，堪稱無敵(?) OpenVPN client 端設定很簡單，只要匯入預先產生的金鑰 (.ovpn) 至 client 端應用程式即可。OpenVPN client 端應用程式也是十分完備，無論 Windows&#x2F;Mac&#x2F;Android&#x2F;iOS 全都有！真的可以做到隨時隨地，手機拿起來就可以工作…? SSH如果有時候你只是需要存取某台內部 server，只需要 terminal 環境，那其實直接使用 ssh 登入即可。如同引言所說，「通常一般的公司或者實驗室都會隔離內部資源，只留一個統一的對外出口」，那其實可以透過那台對望出口當作跳板，使用兩次 ssh 來做到登入你想要用的機器。 12ssh -i ssh_key_path -p port username@office.domain.comssh username@my_computer 另外可以設定 ssh_config 來省去每次都要打一長串的指令，設定像這樣： 123456789101112Host office User username HostName office.domain.com Port 56789 IdentityFile ssh_key_path Host my_computer_in_lan User username HostName 192.168.0.xx ProxyJump office # If is Windows: use ProxyCommand: # ssh.exe office -W %h:%p 這樣就只需要輸入 ssh my_computer_in_lan 即可。 然後給個小建議，系統管理者在設定對外 ssh 服務時，盡量設定成只允許 ssh-key 登入，並且把 ssh port 改成別的 (不要用預設 22)。網路世界很可怕 der~~ 用預設設定就不要抱怨天天被掃 port 或被暴力破解密碼 (想當初實驗室某伺服器 root 帳號被暴力破解，最後只好重灌QQ)。 SSH tunnel + browser proxy那假設你們的網管不願意提供 VPN 服務，你又想存取內部網頁之類的服務，匹如說內部自架的 GitLab，怎辦？沒關係還是有招，這招叫做使用 ssh tunnel + browser proxy。聽起來很複雜？其實還好啦，這方法分為兩部分： 打通 ssh tunnelssh 其實是一個很強大的工具，藉由他其實可以做到打開一個通道，從你的電腦連通道組織的單一的對外出口，變成說只要透過這個 tunnel ，就等同連結到公司的那台對外的電腦上。指令如下： 12345ssh -i ssh_key_path \\ # As usual, use ssh key to access is better -p 56789 \\ # Your ssh server port -vvv \\ # robust logging -NfD 12345 \\ # N: do nothing, f: in background, D: create socket5 proxy username@office.domain.com 這神奇指令幫你打通一個 socket5 的通道，然後再設定瀏覽器去使用這個 proxy，就可以達成跟 vpn 一樣的效果！ 設定瀏覽器我這邊列出在 Mac 跟 Windows 上設定 proxy 的方式，但我想其他瀏覽器一定也有對應的方式設定。 Mac: open -a &quot;Google Chrome&quot; --args --proxy-server=&quot;socks5://localhost:12345&quot; Windows: 在 chrome 上案右鍵 &gt; 內容 &gt; 目標: &quot;C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe&quot; --proxy-server=&quot;socks5://localhost:12345&quot; Use VSCode to access remote project有時候其實只是想用遠端電腦寫程式，用 ssh 當然也可，只是就是限制只能用 vim 之類的編輯器。VSCode 最近推出了實用的 remote-ssh 功能，讓在家也可以使用遠端電腦開發。 可參考 ssh 那段來設定好 ssh_config，其他就如同平常使用 vscode 一樣，十分方便。 雜談: 最近很多想寫想留個紀錄的東西，但總是有點沒時間寫 (或懶?) 本站最近突破四萬瀏覽囉～！恭喜四萬人浪費了五分鐘 (平均網頁停留時間)","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"}]},{"title":"Monitoring: from Handcraft to Prometheus","slug":"Monitoring-from-Handcraft-to-Prometheus","date":"2019-06-10T16:41:55.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2019/06/10/Monitoring-from-Handcraft-to-Prometheus/","link":"","permalink":"https://ssarcandy.tw/2019/06/10/Monitoring-from-Handcraft-to-Prometheus/","excerpt":"身為一個系統管理者，時常要花時間確保系統正常運行，這時一個好的監控系統就很重要，將所有必要的資訊統整在一個畫面上一目瞭然，既能節省時間，當有問題時又能快速找出問題所在，好處多多。","text":"身為一個系統管理者，時常要花時間確保系統正常運行，這時一個好的監控系統就很重要，將所有必要的資訊統整在一個畫面上一目瞭然，既能節省時間，當有問題時又能快速找出問題所在，好處多多。 從前從前，在我還小的時候（大概三四年前），我租了一台小的 Ubuntu cloud server 來跑一些小專案，從那時候開始我就一直想要一個可以監控程式或者伺服器的狀態的東西，那時候查到了 pm2 monitor 跟 netdata ，其中 netdata 那時候搞半天弄不好（現在有了 docker 大概一秒就搞定吧），再加上我比較在意我的 node.js 的程式狀態，最後選擇使用 pm2 monitor。 pm2 monitor screenshot 過一陣子後，正好有機會成為 CMLab 的網管，一下子要管的機器從原本自己的一台變成超過二十台，又重新讓我思考該怎麼才能好好監控這些機器的狀態。那時候前人（學長姊吧?）留下來的一個網頁，採用 snmp 協定將各台資料彙整至 web server，實作上使用 perl 彙整並產生 HTML 檔，呈現如下： 老實說雖然簡陋，但完全可以一目瞭然各台機器狀況，可以稱做一個不錯的監控系統了，那時候我也基於這個網頁再實作額外的自動警報系統，當記憶體或 CPU 用量過高時，發警訊到我的 Facebook Messanger 群組。順便呢也寫了一篇文章記錄： 用 Facebook 聊天機器人當通知系統 從那時候之後我就秉持著要啥自己幹的原則，又利用類似 Pull Based 的方式由一台機器整合的作法，實作出另一套專門監控多台 GPU 伺服器的 Multi-server GPU status monitor，實驗室大家看起來也是挺喜歡這個的 (還曾被教授關注表示讚 🥰)，至今仍在運作也令我相當開心～ Multi-server GPU status monitor 暨用量檢舉系統一隅 時間快轉到近期，又遇到一樣的狀況：同樣有一堆機器要監控。於是我就又再自幹了一套，只是這次實作方式並非是各自產生資料再由一台彙整的作法，而是一台機器主動去各台電腦撈取狀態。當然，這次重作絕對是做得比以前實驗室那用 perl 寫的來的好維護許多，外觀上也比較漂亮～ 示意圖。任何狀態: 機器死掉、閒置、目前使用狀況都顯示在同一個畫面上。 依舊是要啥自己做、高度客製化，整合所有我想知道的事情，稍微不一樣的是這次我將前後分離，資料搜集器負責蒐集我在意的資料 (metrics)，存成 JSON file 直接由前端抓取，資料形式大致如下： 1234567891011121314151617181920[ &#123; &quot;Server&quot;: &quot;research01&quot;, &quot;Uptime&quot;: &quot;178 days&quot;, &quot;Users&quot;: &quot;2 users&quot;, &quot;1m Load&quot;: &quot;0.29&quot;, &quot;5m Load&quot;: &quot;0.34&quot;, &quot;15m Load&quot;: &quot;0.56&quot;, &quot;CPU cores&quot;: &quot;8&quot;, &quot;MemTotal&quot;: &quot;20440876&quot;, &quot;MemAvailable&quot;: &quot;12715432&quot;, &quot;SwapTotal&quot;: &quot;9389048&quot;, &quot;SwapFree&quot;: &quot;8332180&quot;, &quot;DiskTotal&quot;: &quot;531G&quot;, &quot;DiskUsed&quot;: &quot;264G&quot;, &quot;Disk%&quot;: &quot;52&quot;, &quot;Update Time&quot;: &quot;16:15&quot; &#125;, ...] 這樣做的好處是這些 metrics 可以直接被其他人存取，像是我另外用 React Native 來做手機版的 Dashboard；以及一些 Alerter 就是直接讀取這個 JSON，有別於之前寫的 Facebook Messanger Alerter 去爬網頁才得到資料，少繞一圈。 然而就在最近，有同事就說：阿幹嘛不用 Prometheus ? 研究一下才發現，嗯…這東西真的很厲害 XD有 Prometheus 當 metric collector，加上 Grafana 高度客製化的前端 Dashboard，要監控甚麼幾乎只剩要實作 exporter 而已（而且大部分狀況都有現成的）。 整個心路歷程走過來，從一開始用簡單的現成工具 &#x3D;&gt; 自幹 &#x3D;&gt; 自幹（前後端分離）&#x3D;&gt; 到最後又回到使用現成但更成熟的工具… 有種繞了一圈的感覺哈哈。但我還是認真覺得我自己做的 Dashboard 比 Grafana 好看…","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"}]},{"title":"Setup Slurm Cluster","slug":"Setup-Slurm-Cluster","date":"2019-03-16T15:06:53.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2019/03/16/Setup-Slurm-Cluster/","link":"","permalink":"https://ssarcandy.tw/2019/03/16/Setup-Slurm-Cluster/","excerpt":"Slurm 是一個專門拿來做分散式平行運算的平台，已被各式超級運算電腦群集採用[1]。跟 Hadoop 其實有點像，但是我個人感覺是 Slurm 好用太多，更穩定更快速，而且不用會 Java…XD最近剛好有機會需要建立以及管理一個 Slurm Cluster，就想說來記錄一下過程以及一些雷。","text":"Slurm 是一個專門拿來做分散式平行運算的平台，已被各式超級運算電腦群集採用[1]。跟 Hadoop 其實有點像，但是我個人感覺是 Slurm 好用太多，更穩定更快速，而且不用會 Java…XD最近剛好有機會需要建立以及管理一個 Slurm Cluster，就想說來記錄一下過程以及一些雷。 先來看一張 Slurm 架構的圖，基本上最重要的兩個東西就是 (1) Slurm Controller (slurmctld) 跟 (2) Slurm Compute Node (slurmd)，Controller 是拿來分配任務用的，他管理所有 Compute Node，負責決定哪個任務該去哪個 Node 執行，而 Compute Node 就是真的會執行任務的機器。 Slurm 架構圖。 [2] 所以要建置一個 Slurm Cluster，最少要弄一個 Controller 跟多個 Compute Nodes，至於其他像是 slurmdbd 等等，就並不是必需的東西。 Preparation在開始安裝 Controller 跟 Compute Node 之前，要先準備一些事情， 需要安裝 munge，透過 apt-get install libmunge-dev libmunge2 munge 即可。 需要創一個 slurm 帳號跟一個 munge 帳號，並且要在所有機器上都有這些帳號 (uid 也必須一致)。 munge 是 slurm 拿來做 Authentication 的組件。 12345# Create a slurm user, and change it to some id, the is must same across nodes.$ useradd slurm; usermod slurm -u 151; groupmod -g 151 slurm; # Change the munge user id, the is must same across nodes.$ killall munged; usermod munge -u 150; groupmod -g 150 munge; chown munge:munge -R /var/log/munge/ /run/munge /var/lib/munge /etc/munge Setup Slurm Controller下載原始碼來編譯然後安裝 123cd /tmp; wget https://download.schedmd.com/slurm/slurm-18.08.2.tar.bz2; tar xvjf slurm-18.08.2.tar.bz2; cd slurm-18.08.2/(./configure --prefix=/usr &amp;&amp; make &amp;&amp; make install) | tee /tmp/slurm.setup.log 2&gt;&amp;1 雖然 apt-get install slurm 有東西，但那個不是對的… 安裝好以後，可以透過一個網頁來設定基本的 config 檔，預設位置在 /usr/share/doc/slurmctld/slurm-wlm-configurator.html，設定好以後存檔並放至 /etc/slurm-llnl/slurm.conf。記得更改權限。然後就可以啟用。 12345678# set slurmctld &amp; slurmdbd auto start via systemd (only for the controller)cp /tmp/slurm-18.08.2/etc/slurmctld.service /lib/systemd/system/slurmctld.servicecp /tmp/slurm-18.08.2/etc/slurmdbd.service /lib/systemd/system/slurmdbd.servicesystemctl daemon-reload # force systemd reload unitsystemctl enable slurmdbd slurmctld # force slurmdbd &amp; slurmctld start after the machine is readysystemctl start slurmdbd slurmctld # start slurmdbd &amp; slurmctldsystemctl status slurmdbd slurmctld # check slurmdbd &amp; slurmctld statussystemctl is-enabled slurmdbd slurmctld # check slurmdbd &amp; slurmctld enabled Setup Slurm Compute NodesSlurm Compute Node 也可以透過 apt 安裝，但是由於我需要使用 slurm 的一些 api，所以這部分會使用從 source code 建置。 先安裝 munge，改 user id 以及複製 munge key: 123456789# Copy munge key from slurm controller$ scp controller:/etc/munge/munge.key /etc/munge/ # Change the permissions$ chown munge:munge /etc/munge/munge.key$ chmod 400 /etc/munge/munge.key # Start munge service$ service munge start 接下來先下載 slurm source code，並且 build 12345678910111213141516# Install slurm$ cd /tmp; wget https://download.schedmd.com/slurm/slurm-18.08.2.tar.bz2; tar xvjf slurm-18.08.2.tar.bz2; cd slurm-18.08.2/$ (./configure &amp;&amp; make &amp;&amp; make install) | tee /tmp/slurm.setup.log 2&gt;&amp;1 # Copy slurm.conf, and add slurm user$ mkdir /usr/etc /var/spool/slurmctld /var/spool/slurmd$ scp controller:/usr/etc/slurm.conf /usr/etc/slurm.conf$ useradd slurm; usermod slurm -u 151; groupmod -g 151 slurm; chown slurm:slurm -R /var/log/slurm* /run/slurm* /var/lib/slurm* /etc/slurm* /var/spool/slurm*# set slurmd auto start via system (for the controller and workers)cp /tmp/slurm-18.08.2/etc/slurmd.service /lib/systemd/system/slurmd.servicesystemctl daemon-reload # force systemd reload unitsystemctl enable slurmd # force slurmd start when the machine is ready.systemctl start slurmd # start slurmdsystemctl status slurmd # check slurmd statussystemctl is-enabled slurmd # check slurmd enabled 到此基本完成 Slurm Cluster 的設定，可以透過一些指令來檢查 slurm 的狀態。 sinfo: 會顯示目前 cluster nodes 的狀態 123PARTITION AVAIL TIMELIMIT NODES STATE NODELISTresearch* up infinite 1 idle research[01-10]research* up infinite 3 alloc research[11-15] squeue: 顯示目前正在執行&#x2F;等待執行的任務 123456 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)578116 research test user123 R 0:06 1 research12578116 research test user123 R 0:06 1 research12578116 research test user123 R 0:06 1 research12578116 research test user123 R 0:13 1 research15578116 research test user123 R 0:16 1 research13 scancel: 取消任務 還有很多其他指令，可以看這張 Cheat sheet。 Some Common Issues使用的過程中總是會遇到一些奇奇怪怪的問題，這邊就列舉一些我常見的: Zero Bytes were transmitted or received在使用 slurm 相關的指令時噴出的錯誤。這個基本上是因為 Authentication 出錯，把所有 nodes 的 munge 重啟就會解決。 Slurm job stock in CG state有時候會發現有一些 Job 就是一直卡在 Completing (CG state)，這時候把那個 node 設為 down 再設為 resume 就會消失了。 12$ scontrol update nodename=research04 state=down reason=job_stuck;$ scontrol update nodename=research04 state=resume Invalid job credential這表示有些 node 沒有 slurm, munge user，或者他們的 uid 不一致，解決方法就是把他們設為一致。 References https://en.wikipedia.org/wiki/Slurm_Workload_Manager https://slurm.schedmd.com/overview.html h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"slurm","slug":"slurm","permalink":"https://ssarcandy.tw/tags/slurm/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"}]},{"title":"Mac 維修小記","slug":"mbpr2015-repair","date":"2018-08-17T20:13:08.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2018/08/17/mbpr2015-repair/","link":"","permalink":"https://ssarcandy.tw/2018/08/17/mbpr2015-repair/","excerpt":"前陣子我的 Macbook pro 13” (2015) 出現奇怪的問題，突然一直跳出訊息叫我連接藍芽滑鼠，內建觸控板跟鍵盤完全失靈，看著電腦登入畫面我卻什麼都做不了，超級崩潰… 後來稍微多重開機試幾次發現這種狀況有時候會消失，查一下網路發現大概有幾種可能，一是主機板壞了，二是鍵盤跟滑鼠的連接到主機板的排線壞了。但是這兩種壞法修的價格可差距不少，所以我就堅信只是排線壞了（Ｘ","text":"前陣子我的 Macbook pro 13” (2015) 出現奇怪的問題，突然一直跳出訊息叫我連接藍芽滑鼠，內建觸控板跟鍵盤完全失靈，看著電腦登入畫面我卻什麼都做不了，超級崩潰… 後來稍微多重開機試幾次發現這種狀況有時候會消失，查一下網路發現大概有幾種可能，一是主機板壞了，二是鍵盤跟滑鼠的連接到主機板的排線壞了。但是這兩種壞法修的價格可差距不少，所以我就堅信只是排線壞了（Ｘ 既然都免不了要維修一趟了，又由於官方的螢幕脫膜更換計畫規定是購買日四年內可免費更換，我這台剛好接近過期，就想說順便將我螢幕也換掉。所以我打算維修的問題就有兩件： 維修鍵盤滑鼠失靈 更換螢幕 但由於我螢幕脫膜的情況非常輕微，大概只有一兩根細線般的脫落，所以我先跑去學校內的授權店詢問這種狀況是不是可以更換，結果馬上被打槍說這麼輕微不可能可以換，不信自己去直營店問。 既然都被這麼打槍了，我就只好轉戰 101 直營店拉… 久聞直營店非常難排到維修，我每天開 Genius Bar 預約維修，然後無論平日假日時段永遠是滿的，非常驚人…後來發現據說 iPhone 有 App 可以比網頁版多提前預約一天，根本歧視非 iPhone 用戶…難怪網頁版永遠預約不到。嘗試預約兩三天後我就放棄了，打算直接耗一整天去現場排候補維修名額。就這樣，我大約中午時刻到達 101 直營店，進門右手邊就是 Genius Bar 排隊的地方。排到以後會寄一封簡訊來，並在快輪到時會再寄一封來提醒，我自己大概是等了五個小時才輪到我… 輪到我之後就要我描述狀況，可以的話儘量直接重現問題給他們看，就會減少很多檢測，剛好我那天鍵盤滑鼠依舊是不能使用的狀況，所以他們只跑簡單的檢測就結束了。（為了跑檢測他們還跑去拿外接鍵盤跟滑鼠 XD） 另外關於螢幕脫膜的部分二話不說直接跟我說可以換～ 後來在報價時跟我說可能要換主機板，我就說我只想換排線跟螢幕，如果要修主機板我就不修了。店員也就如實把我的需求寫進備註。 如果要換主機板我就不想修了。 送修之後雖然說 3-5 天會打給我告知維修情況，但根本沒打來…過了大概 20 天之後才寄信來說可以取貨，也附上維修明細。 維修明細，只需支付排線的錢。 總結來說 Apple 直營店的服務還算不錯，更換鍍膜完全不會阻撓，檢測維修時也十分迅速。缺點就是預約維修太難，現場排也要超久真的很花時間… 所以若需要去直營店維修的話有幾個建議： 有 iPhone 的話可以用他們的 App 來預約，可以比其他人更容易約到。 沒有 iPhone 的話…乖乖早點去現場排吧… 雜談 有點久沒新文章了…人就是這樣一怠墮就永無止盡QQ 理論上最近應該是滿閒的應該要提高產出… 時間都被 Slay the Spire 吃光光…","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"}]},{"title":"Use PyTorch to solve FizzBuzz","slug":"use-pytorch-to-solve-fizzbuzz","date":"2018-01-14T17:13:49.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2018/01/14/use-pytorch-to-solve-fizzbuzz/","link":"","permalink":"https://ssarcandy.tw/2018/01/14/use-pytorch-to-solve-fizzbuzz/","excerpt":"FizzBuzz 是一個常見的程式考題，題目很簡單，就是給一個整數，如果可以被 15 整除就回傳 FizzBuzz；可以被 3 整除就回傳 Fizz；被 5 整除就回傳 Buzz；都不能整除就回傳原本的數字。 用 Python 可以簡單幾行就寫出來： 123456789def fizz_buzz(num): if num % 15 == 0: return &#x27;FizzBuzz&#x27; elif num % 3 == 0: return &#x27;Fizz&#x27; elif num % 5 == 0: return &#x27;Buzz&#x27; else: return str(num) 不過有狂人就把這當作分類問題，用 tensorflow 來解這個問題，原文在此，是篇很有趣的文章 XD","text":"FizzBuzz 是一個常見的程式考題，題目很簡單，就是給一個整數，如果可以被 15 整除就回傳 FizzBuzz；可以被 3 整除就回傳 Fizz；被 5 整除就回傳 Buzz；都不能整除就回傳原本的數字。 用 Python 可以簡單幾行就寫出來： 123456789def fizz_buzz(num): if num % 15 == 0: return &#x27;FizzBuzz&#x27; elif num % 3 == 0: return &#x27;Fizz&#x27; elif num % 5 == 0: return &#x27;Buzz&#x27; else: return str(num) 不過有狂人就把這當作分類問題，用 tensorflow 來解這個問題，原文在此，是篇很有趣的文章 XD 由於原文是用 tensorflow 實作，我想我就來寫個 PyTorch 版練習一下吧！基本上就是把 FizzBuzz 當作分類問題 (Classification) 來訓練，要做的事大概有這些： 準備 training, testing data 定義 model Training 那就來一步一步看看 準備資料雖然 FizzBuzz 輸入是一個整數，但是把他轉成二進位會比較好訓練，所以先來寫個轉二進位的函式： 12def encode(num): return list(map(lambda x: int(x), (&#x27;&#123;:010b&#125;&#x27;).format(num))) 因為我不想 import numpy，所以這邊轉二進位的方式是用 Python 的 format 來做。 另外還要把 FizzBuzz 改寫成回傳分類號碼： 123456789def fizz_buzz(num): if num % 15 == 0: return 0 # &#x27;FizzBuzz&#x27; elif num % 3 == 0: return 1 # &#x27;Fizz&#x27; elif num % 5 == 0: return 2 # &#x27;Buzz&#x27; else: return 3 # num 接下來要來產生資料拉 1234567def make_data(num_of_data, batch_size): xs, ys = [], [] for _ in range(num_of_data): x = random.randint(0, 2**DIGITS-1) xs += [encode(x)] ys += [fizz_buzz(x)] return xs, ys 由於 training 的時候通常會是一批一批 (batch) 下去訓練的，所以在準備資料時就先一批一批放在一起會比較方便。 所以改一下， 1234567891011def make_data(num_of_data, batch_size): xs, ys, data = [], [], [] for _ in range(num_of_data): x = random.randint(0, 2**DIGITS-1) xs += [encode(x)] ys += [fizz_buzz(x)] for b in range(num_of_data//batch_size): xxs = xs[b*batch_size:(b+1)*batch_size] yys = ys[b*batch_size:(b+1)*batch_size] data += [(xxs, yys)] return data 前置步驟都弄好之後，終於可以來產生訓練跟測試資料拉，Batch size 就訂個 32 好了： 12training_data = make_data(1000, 32)testing_data = make_data(100, 32) 定義 Model其實我對於如何設計 model 還是沒有很了解，不過這問題應該是挺簡單的，弄個幾層 fully-connected layer 應該就夠了吧? 1234567891011121314151617class FizzBuzz(nn.Module): def __init__(self, in_channel, out_channel): super(FizzBuzz, self).__init__() self.layers = nn.Sequential( nn.Linear(in_channel, 1024), nn.ReLU(), # Activation function nn.Linear(1024, 1024), nn.ReLU(), # Activation function nn.Linear(1024, out_channel) ) def forward(self, x): x = self.layers(x) return x # Input 10 digits vector (binary format), output 4 classes vectormodel = FizzBuzz(10, 4) 我用了一層隱藏層，1024 個神經元，activation function 則都是最基本的 ReLU 。in_channel, out_channel 分別是輸入數字是長度多少的二進位 (10)，以及輸出幾種分類 (4)。PyTorch 的 model 是繼承 torch.nn.Module 來寫個 class，通常只要定義 __init()__ 跟 forward()就好，如果要自己做特殊的 backward 的話，也可以實作 backward()。 Training整個訓練的過程基本就是按照一般的分類問題流程做，把資料丟進 model 的到預測，把預測跟正確答案做 cross entropy 當作 loss ，然後去最小化這個 loss 用 PyTorch 寫大概是這樣： 12345678910def training(model, optimizer, training_data): model.train() for data, label in training_data: data = Variable(torch.FloatTensor(data)) label = Variable(torch.LongTensor(label)) optimizer.zero_grad() # Clear gradient out = model(data) # predict by model classification_loss = F.cross_entropy(out, label) # Cross entropy loss classification_loss.backward() # Calculate gradient optimizer.step() # Update model parameters 由於 要是 Variable 才能自動算 back propagation ，所以 data 跟 label 都要變成 Variable。這邊我用的 optimize 方法是 Stochastic Gradient Descent (SGD)，記得每次都要先清空 gradient 再做 backward。 Result萬事皆備，可以開始來看看結果如何了，來 train 300 個 Epoch 好了， 1234567==== Start Training ====Epoch 50/300, Loss: 0.78973, Accuracy: 64.58%Epoch 100/300, Loss: 0.29299, Accuracy: 91.67%Epoch 150/300, Loss: 0.14616, Accuracy: 93.75%Epoch 200/300, Loss: 0.10606, Accuracy: 96.88%Epoch 250/300, Loss: 0.09937, Accuracy: 96.88%Epoch 300/300, Loss: 0.06472, Accuracy: 98.96% 哇！才 98% 準確率呢… 拿去 online judge 解題大概不會過呢 XD 如果想要玩玩看我的 code，這邊可以看：https://github.com/SSARCandy/pytorch_fizzbuzz","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"},{"name":"pytorch","slug":"pytorch","permalink":"https://ssarcandy.tw/tags/pytorch/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://ssarcandy.tw/tags/machine-learning/"}]},{"title":"搞懂 JavaScript 原型鍊","slug":"javascript-prototype-chain","date":"2017-12-06T00:40:01.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/12/06/javascript-prototype-chain/","link":"","permalink":"https://ssarcandy.tw/2017/12/06/javascript-prototype-chain/","excerpt":"最近突然看到許多關於 js 原型鍊的介紹，這東西我從來沒搞懂過也沒認真想要搞懂過 XD說真的好像不懂也不會影響甚麼，畢竟實戰上基本用不到這些比較底層的東西…但是當作是邊緣冷知識來看看也是挺不錯的~","text":"最近突然看到許多關於 js 原型鍊的介紹，這東西我從來沒搞懂過也沒認真想要搞懂過 XD說真的好像不懂也不會影響甚麼，畢竟實戰上基本用不到這些比較底層的東西…但是當作是邊緣冷知識來看看也是挺不錯的~ 歷史JavaScript，一個為了網頁互動而誕生的腳本語言，最早是因為 Netscape 開發了一個比較成熟的瀏覽器 Navigator，但由於沒有可以讓網頁與使用者互動的方式，所以他們就開發了 JavaScript 來當作網頁的腳本語言，其中主要開發者是 Brendan Eich。由於當時物件導向正夯，Brendan Eich 也決定讓 JavaScript 所有東西都是 Object。如此用途明確的語言，似乎不太需要非常完整的底層架構吧？用不著像 C++, Java 這種泛用式程式語言一樣完整，所以 Brendan Eich 並不打算引入 Class 的概念。但又由於JavaScript 所有東西都是 Object，勢必要有種方法做到類似繼承這件事。所以原型鍊就出現了！ JavaScript 語法背後的運作JavaScript 要建構一個 instance 會用 new 關鍵字，但實際上這 new 跟 C++, Java 的不一樣。JavaScript 的 new 其實後面接的是一個 function，類似於 C++ constroctor 1234567891011function Person(name) &#123; this.name = name; this.sayHi = function () &#123; console.log(&#x27;Hi&#x27;); &#125;;&#125; var man1 = new Person(&#x27;Jack&#x27;);var man2 = new Person(&#x27;Andy&#x27;); console.log(man1.sayHi === man2.sayHi); // false 上面這段例子是 JavaScript 創造實例的方式。可以看到 Person 中有兩個東西，一個是 name, 另一個是 Person 的 method sayHi，雖然這樣很好了，但是這樣 man1, man2 中其實包含了一樣的 sayHi function，浪費記憶體空間。 所以如果要做一個類別共用的方法可以這樣: 123456789101112function Person(name) &#123; this.name = name;&#125; Person.prototype.sayHi = function () &#123; console.log(&#x27;Hi&#x27;);&#125;; var man1 = new Person(&#x27;Jack&#x27;);var man2 = new Person(&#x27;Andy&#x27;); console.log(man1.sayHi === man2.sayHi); // true 把類別共用的 function 寫在 prototype 中就可以達成共用的效果，而其實這個 prototype 就是原型鍊。 用原型鍊模擬繼承我們用瀏覽器偵錯模式印變數的時候，相信經常看到 __proto__ 藏在變數裡，那個東東就是原型鍊。JavaScript 中有幾個預設的類別，像是 Object, Array 等等，我們在宣告變數的時候其實裡面都會藉由原型鍊 串 到預設的類別。 a 的 __proto__ 指向 Object 所以其實在創造實例時，JavaScript 會把 __proto__ 指向他的原型，以空物件 &#123;&#125; 而言，就是預設類別 Object。回到上面 Person 的例子，他的原型鍊就會是長這樣: man 的 sayHi 是定義在他的 __proto__ 中 可以發現寫在 Person.prototype 的 sayHi ，實際上是定義在 man.__proto__.sayHi ，也就是 Person 的原型，而在呼叫 man.sayHi() 時，由於找不到，所以 JavaScript 會藉由__proto__嘗試往上找，就會在 man.__proto__ 中找到。 而這個一直往上一層原型找的過程，其實就模擬了繼承的效果。 ES6 語法糖與原型鍊雖說 JavaScript 當初沒有 Class 的概念，但在 ES6 中其實出現 class 關鍵字了，但其實這只是一個語法糖而已，可以藉由幾個例子發現 ES6 背後還是透過原型鍊來運作。 123456789101112131415161718class level1 &#123; constructor() &#123; this.x = 1; &#125; getX() &#123; return this.x; &#125;&#125; class level2 extends level1 &#123; constructor() &#123; super(); this.y = 10; &#125; getY() &#123; return this.y; &#125;&#125; let l2 = new level2(); 上面這個例子是用 ES6 寫的繼承小程式， level2 繼承 level1 。直接來看看創造出的實例 l2 裡面是什麼: ES6 的繼承其實也是用原型鍊串起來的。 又看到原型鍊了！l2 的原型鍊串成這樣: l2 → level1 → Object。看看 getX, getY 就會發現他們定義在不同層級，因為 getX 是父類別的方法，所以在原型鍊中的更上一層。由此就可以看出 ES6 雖然有 class 關鍵字，但其實原理還是原型鍊。 References Javascript继承机制的设计思想 該來理解 JavaScript 的原型鍊了 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; } 雜談除了學校教的 C&#x2F;C++ 以外，我似乎沒去搞懂過其他語言背後的邏輯，秉持者會用就好的心態活到現在(X這次稍微理解原型鍊以後，好像又更了解一點 JavaScript 了呢～！","categories":[],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://ssarcandy.tw/tags/javascript/"}]},{"title":"Deep CORAL: Correlation Alignment for Deep Domain Adaptation","slug":"deep-coral","date":"2017-10-31T13:18:50.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/10/31/deep-coral/","link":"","permalink":"https://ssarcandy.tw/2017/10/31/deep-coral/","excerpt":"雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 domain 的資料上。 這篇論文[1] (Deep CORAL: Correlation Alignment for Deep Domain Adaptation, B Sun, K Saenko, ECCV 2016) 提出一個 CORAL loss，通過對 source domain 和 target domain 進行線性變換來將他們各自的的二階統計量對齊 (minimizing the difference between source/target correlations).","text":"雖然現在可以透過 Deep neural networks 來訓練出非常強大的能力，但卻難以學到比較通用的知識，通常最後訓練出的 model 會只適合處理類似於 training data 的資料。Domain adaption 是目前在 machine learning 比較新的問題，是希望可以讓 network 學到比較跨領域的 features ，進而使訓練出來的模型可以應用在不同 domain 的資料上。 這篇論文[1] (Deep CORAL: Correlation Alignment for Deep Domain Adaptation, B Sun, K Saenko, ECCV 2016) 提出一個 CORAL loss，通過對 source domain 和 target domain 進行線性變換來將他們各自的的二階統計量對齊 (minimizing the difference between source/target correlations). Introduction 作者引入了 CORAL[2] 這一方法，並且將其擴展成一 differentiable loss function。 作者藉由 CORAL loss 來做 unsupervised learning，並達到了 state-of-the-art 的 performance。 CORAL loss 十分簡單並且可以輕易地整合至一般的神經網路中。 Architecture 作者將 CORAL loss 加進一分類問題的網路架構中。 [1] 作者將 Deep CORAL 應用在一般的分類問題上，整個神經網路架構如圖。從中間 cov1 ~ fc8 其實就是一般的 AlexNet，只是稍作修改改成有兩個 input (source data &amp; target data) 以及兩個 output。 在訓練的過程中，每個 batch 都包含了 source data &amp; target data，其中 source data 是包含 label 資料的；而 target data 則完全沒有 label 資料。source data &amp; target data 各自經過一 shared weight 的 networks 之後會有兩個 output，其中: source task 會算一個 classification loss source 的 fc8 及 target 的 fc8 會再拿來算 CORAL loss 而總和 loss 為兩者相加: CORAL Loss作者提出的 CORAL loss 是在計算 source &amp; target covariance matrix 之間的 distance。 We define the CORAL loss as the distance between the second-order statistics(covariances) of the source and target features. Forward而這個 loss function 定義如下: 其中，, 為 source, target 的 covariance matrix，其定義如下: 詳細符號定義可以參考 paper[1] section 3.1 Backward (gradient)至於 gradient 可以由 chain rule 算出來，如下: 注意 target 那邊是有個負號的，當初在實作時忘記這個負號而搞半天… Experiment作者做的實驗也是在分類問題上，架構如同上面提及的神經網路架構圖。實驗採用 Office31 dataset[3]，這是一個專門拿來做 domain adaption 的資料集，裡面有三種不同 domain 的影像: Amazon, DSLR, and Webcam裡面都有相同的 31 種類別，也就是說這三大類唯一不同的點就是圖片的樣貌: Amazon 就是去背的圖片(背景都是白色的) DSLR 就是用單眼拍的圖片(背景就是真實場景的背景) Webcam 跟 DSLR 很相近，差別比較大的部分是 webcam 的畫質比較差，有的還有色偏 在實驗進行過程中，source data 會有 label；而 target data 則沒有。且在開始之前會先預載 ImageNet pre-trained model。 由於 Office31 有三種 domain data，所以作者就做了所有 domain adaption 的組合，以下是結果圖: 各種方法比較圖。螢光的是作者的方法。[1] 可以看到 D-CORAL 在大部分的 domain adaption tasks 中都取得了最好的成績。 再來看看其中一個實驗 Amazon → Webcam 的詳細結果: Amazon → Webcam 的詳細結果圖。[1] 圖 (a) 比較了有 CORAL loss 與沒有 CORAL loss 的差別，可以看到當加入CORAL loss 之後，target (test) task 有顯著的提升，而且並未使得 source (training) task 的準確率下降太多。 圖 (b) 則可以看出，classification loss 跟 CORAL loss 其實是扮演互相抗衡的腳色，隨著訓練的進行會讓兩者到達一穩定的狀態。 Implementation我也試著用 PyTorch 實做了此篇論文的方法，最重要的其實就是新增一 loss function 到整個網路架構中，其中 forward and backward 的算法剛好也有詳細說明。 Forward 的部分大概如下: 123456789def forward(self, source, target): d = source.shape[1] ns, nt = source.shape[0], target.shape[0] cs = feature_covariance_mat(ns, source) ct = feature_covariance_mat(nt, target) self.saved = (source, target, cs, ct, ns, nt, d) res = forbenius_norm(cs - ct)**2/(4*d*d) res = torch.FloatTensor([res]) return res Backward 則如下: 12345678def backward(self, grad_output): source, target, cs, ct, ns, nt, d = self.saved ones_s_t = torch.ones(ns).view(1, -1) ones_t_t = torch.ones(nt).view(1, -1) s_gradient = (source.t() - (ones_s_t.matmul(source).t().matmul(ones_s_t)/ns)).t().matmul(cs - ct) / (d*d*(ns - 1)) t_gradient = (target.t() - (ones_t_t.matmul(target).t().matmul(ones_t_t)/nt)).t().matmul(cs - ct) / (d*d*(nt - 1)) t_gradient = -t_gradient return s_gradient*grad_output, t_gradient*grad_output 寫起來公式的部分又臭又長 XD 我也實際跑了 Amazon → Webcam 的例子，做了個圖: 我做出來的 Amazon → Webcam 的詳細結果圖。 可以看出有 CORAL loss 的確使得 target task 的準確率提升一些。不過我做出來的整體準確率並沒有與論文上的一樣有 60% 左右，而是大概在 50% 左右，不知道為甚麼… QQ Update經過 redhat12345 的建議後，修正了一下 CORAL Loss 的算法，終於使 Target accuracy 提升到原論文的程度。 123456789101112def CORAL(source, target): d = source.data.shape[1] # source covariance xm = torch.mean(source, 1, keepdim=True) - source xc = torch.matmul(torch.transpose(xm, 0, 1), xm) # target covariance xmt = torch.mean(target, 1, keepdim=True) - target xct = torch.matmul(torch.transpose(xmt, 0, 1), xmt) # frobenius norm between source and target loss = torch.mean(torch.mul((xc - xct), (xc - xct))) loss = loss/(4*d*d) return loss 修正過後的結果。 References Sun, B., Saenko, K.: Deep CORAL: Correlation Alignment for Deep Domain Adaptation. In: ECCV (2016) Sun, B., Feng, J., Saenko, K.: Return of frustratingly easy domain adaptation. In: AAAI (2016) Domain Adaptation Project h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"paper","slug":"paper","permalink":"https://ssarcandy.tw/tags/paper/"},{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"},{"name":"pytorch","slug":"pytorch","permalink":"https://ssarcandy.tw/tags/pytorch/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://ssarcandy.tw/tags/machine-learning/"}]},{"title":"Using PyTorch in Windows 10","slug":"using-pytorch-in-windows","date":"2017-09-27T14:17:32.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2017/09/27/using-pytorch-in-windows/","link":"","permalink":"https://ssarcandy.tw/2017/09/27/using-pytorch-in-windows/","excerpt":"最近開始在使用 PyTorch 寫些東東，他支援 MacOS&#x2F;Linux 使用，唯獨 Windows 不支援…所以我一直都是使用 Mac 寫相關的東西。 但是我的桌機都是 Windows，在可以用桌機的環境下卻必須使用小小的 Mac 打字真的不是很高興…正好最近從學長那邊得知有個方法可以讓 Windows 使用 PyTorch ，就趕緊來試試！","text":"最近開始在使用 PyTorch 寫些東東，他支援 MacOS&#x2F;Linux 使用，唯獨 Windows 不支援…所以我一直都是使用 Mac 寫相關的東西。 但是我的桌機都是 Windows，在可以用桌機的環境下卻必須使用小小的 Mac 打字真的不是很高興…正好最近從學長那邊得知有個方法可以讓 Windows 使用 PyTorch ，就趕緊來試試！ Window 10 現在有個東西叫 Windows Subsystem for Linux (WSL) ，是一個在 Windows 下的 Ubuntu 子系統，這個子系統可以做到任何正常 Ubuntu 做得到的事。那我就可以在 WSL 中按照 Linux 的流程設定好 PyTorch 的相關環境，然後在 Windows 中使用 WSL 的 Python 環境，就可以達到目的 (讓 Windows 使用 PyTorch)。 所以基本上環境設置步驟： 啟用 Windows Subsystem for Linux 。 弄好 WSL 中的環境，包含 Python 以及 ssh server 的設定。 讓 Windows 使用遠端 (WSL) 的 Python 環境。 Install Windows Subsystem for Linux 開啟開發人員模式 用系統管理員開啟 cmd，輸入 OptionalFeatures 指令，會跳出一個視窗 勾選「適用於 Linux 的 Windows 子系統」 完成以後可能需要重開機。 Install PyTorch in WSL接下來是要在 WSL 中設置 Python 以及 PyTorch 的相關環境。如果沒有 Python 記得先安裝。 然後安裝 PyTorch，基本上按照 PyTorch 官方網站 教學操作： 12$ pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post3-cp27-cp27mu-manylinux1_x86_64.whl$ pip install torchvision Install Python IDE (PyCharm)PyCharm 是一個可以寫 Python 的 IDE，雖然專業版要錢，不過學生免費～YA！ 安裝就不贅述了，反正就是一直下一步… 設定使用遠端 Python由於要用 WSL 裡面的 python，所以必須設定 Remote Python Interpreter Project setting &gt; Project interpreter Add Remote 填入 ssh 資訊 填一填 ssh 相關資訊。 由於是要透過 ssh 去存取 WSL 中的 Python ，所以 WSL 那邊要開啟 ssh service 好讓 PyCharm 連線。 在 WSL 中: 12$ sudo service ssh start * Starting OpenBSD Secure Shell server sshd [ OK ] 設定 Path mappingsWSL 其實是可以存取本機 (Windows) 的資料的，預設 C 槽是掛載在 /mnt/c這也要設定一下才能讓 PyCharm 運作正常： Project setting &gt; Project interpreter 新增 mapping C: → /mnt/c 設定 Path mapping 的地方也是在 Project setting > Project interpreter Misc開啟 ssh service 時噴錯123$ sudo service ssh startinitctl: Unable to connect to UpstartBind to port 22 on 0.0.0.0 failed: Address already in use. 去更改 /etc/ssh/sshd_config : 123PasswordAuthentication yesUsePrivilegeSeparation noPort &lt;random number&gt; 基本上最重要的就是換個 Port 了，會沒辦法啟動大概是本機 (Windows) 有程式已經占用 Port 22 了。 不能安裝 PyTorch12$ pip install torch-xxx.whltorch-xxx.whl is not a supported wheel on this platform. 請檢察 pip -V 版本，起碼要是 9.0 以上，可以用以下方法更新 pip: 1$ pip install --upgrade pip References Windows Subsystem for Linux (WSL) 安裝教學 &amp; 初體驗 Configuring Remote Interpreters via WSL h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"},{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"},{"name":"pytorch","slug":"pytorch","permalink":"https://ssarcandy.tw/tags/pytorch/"}]},{"title":"Multi-server GPU status monitor","slug":"multi-server-gpu-status-monitor","date":"2017-08-25T15:29:39.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/08/25/multi-server-gpu-status-monitor/","link":"","permalink":"https://ssarcandy.tw/2017/08/25/multi-server-gpu-status-monitor/","excerpt":"隨著實驗室 GPU 資源日益增加，有越來越多人在抱怨搶不到 GPU、不知道哪台有空的 GPU 、有人佔用太多 GPU 資源等等問題，這些問題似乎跟沒有一個方法可以「一目瞭然的知道所有 GPU 的狀況」有關。 對，就像是 cml-status 一樣，假設有個 GPU 版的 cml-status，應該就可以讓大家更輕易地找到閒置的 GPU，如果有人佔用過多運算資源也容易發現。 於是 CMLab GPU Status 就誕生拉~","text":"隨著實驗室 GPU 資源日益增加，有越來越多人在抱怨搶不到 GPU、不知道哪台有空的 GPU 、有人佔用太多 GPU 資源等等問題，這些問題似乎跟沒有一個方法可以「一目瞭然的知道所有 GPU 的狀況」有關。 對，就像是 cml-status 一樣，假設有個 GPU 版的 cml-status，應該就可以讓大家更輕易地找到閒置的 GPU，如果有人佔用過多運算資源也容易發現。 於是 CMLab GPU Status 就誕生拉~ 現況由於我們有數台伺服器是有 GPU 資源的，所以要做出一個網頁版的監控系統大概要有兩個步驟: 彙整各台資訊到某一台 server 將這些資訊轉成網頁形式呈現 對於第一點大概有兩種做法，一是主動去取得資訊，也就是透過 ssh 登入到各個有 GPU 的伺服器詢問資訊；二是各個有 GPU 的伺服器各自回報資訊給某一台來彙整。 而主動去取得資訊的方法有幾個缺點， 透過 ssh 登入需要密碼，當然可以透過建立 ssh 認證來省去這一步，但好麻煩 _:(´□&#96;」 ∠):_ 這種方法會使得事情都是一台伺服器在做(又要登入各台又要彙整資訊)，感覺不是很人道… 另一個方法則是「各自回報，統一呈現」，就是大家各自回報 GPU 狀況，然後由 web server 統一彙整資訊，這種感覺就比較人道一點，大家一起分擔工作~ 各自回報，統一呈現決定了大方向的做法以後，可以繼續切分整件事情的流程: 各台機器如何回報?回報去哪? 如何彙整? 如何以網頁呈現? 各台機器如何回報、回報去哪？ 實驗室每台伺服器都有共用一個 NFS 幸好我們的 server 都有用 NFS ，所以各自回報到 NFS 上就可以讓其他台存取到資訊了。那獲得 GPU 資訊的方法不外乎就是下 nvidia-smi 來取得囉，但說真的這指令太豐富了，所以我改用別的神人做的指令 gpustat，輸出就乾淨多了~ gpustat sample output 所以每一台 GPU server 要做的是「每分鐘回報一次 GPU status 並存至 NFS」，可以透過 crontab 註冊: 12# crontab on each GPU server* * * * * /usr/local/bin/gpustat -pcu &gt; /NFS/status-gpu/$(hostname) 如何彙整？剛好我們 server 名子都是很沒創意的 cml*，所以彙整相當簡單。由於各自回報的關係，在 NFS 上會有如下的檔案: 12345678910-rw-r--r-- 1 root root 875 Aug 25 23:04 cml10-rw-r--r-- 1 root root 940 Aug 25 23:04 cml11-rw-r--r-- 1 root root 169 Aug 25 23:04 cml14-rw-r--r-- 1 root root 682 Aug 25 23:04 cml16-rw-r--r-- 1 root root 746 Aug 25 23:04 cml19-rw-r--r-- 1 root root 364 Aug 25 23:04 cml21-rw-r--r-- 1 root root 169 Aug 25 23:04 cml22-rw-r--r-- 1 root root 620 Aug 25 23:04 cml23-rw-r--r-- 1 root root 748 Aug 25 23:04 cml24-rw-r--r-- 1 root root 747 Aug 25 23:04 cml25 那要彙整就下個 cat cml* 就解決了。 如何以網頁呈現？最後有了彙整後的資訊後該如何呈現置網頁上呢?由於我們的 web server 有 apache，所以基本上只要多搞個資料夾底下有 index.html就可以了。所以只要想辦法將彙整的資訊轉成 html 即可。 網路上大神很多，我又發現了 ansi2html.sh ，這工具可以把 terminal output 轉成 html ，並且連顏色都幫你轉成 css ，太神拉~ 所以要變成網頁呈現就可以註冊個 crontab: 12# crontab on web server* * * * * cat /status-gpu/cml* | /bin/sh ansi2html.sh &gt; index.html 每分鐘重新刷新 index.html BUG做好以後還是逃不掉 BUG 的摧殘QQ有時候會發現彙整的資訊會缺少某幾台 GPU server 的資訊 查來查去發現原來是因為 crontab 註冊的時間一樣(都是每分鐘)，再加上 NFS 是透過網路傳輸所以會比較慢，導致各自機器每分鐘回報狀況時檔案還沒寫入，web server 就執行彙整動作，就會出現缺檔的情形。解決方式很簡單，就是彙整時間稍微延遲一點，讓各自回報有時間完成。 12# crontab on web server* * * * * sleep 30 &amp;&amp; cat /status-gpu/cml* | /bin/sh ansi2html.sh &gt; index.html 用 sleep 即可延遲指令。 雜談 差點這個月就要開天窗了… 大力募集網管中!","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"}]},{"title":"接案三兩事","slug":"about-taking-case","date":"2017-07-21T11:06:10.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2017/07/21/about-taking-case/","link":"","permalink":"https://ssarcandy.tw/2017/07/21/about-taking-case/","excerpt":"最近終於結束了一個搞了很久的 case 。我其實沒什麼接案經驗，這次也是剛好算是學校單位找我做個活動的宣傳網站，我心想既然是學校的應該也不太容易被騙，就來試試看所謂的接案。在這次經驗中其實也是學到不少東西、踩了很多雷，大部份都是因為沒什麼經驗導致踩到的很多很多算專案管理相關的雷。如果還有下次的話，一定要注意一些事情才可以讓整個案子的進行更加順利。","text":"最近終於結束了一個搞了很久的 case 。我其實沒什麼接案經驗，這次也是剛好算是學校單位找我做個活動的宣傳網站，我心想既然是學校的應該也不太容易被騙，就來試試看所謂的接案。在這次經驗中其實也是學到不少東西、踩了很多雷，大部份都是因為沒什麼經驗導致踩到的很多很多算專案管理相關的雷。如果還有下次的話，一定要注意一些事情才可以讓整個案子的進行更加順利。 一定要寫好合約這看起來是基本中的基本，但我當初就是沒做(掩面當初只有口頭說好而已。這件事情之所以重要，是因為這是個最基本的保障，不論是對我們或是對客戶方，和約可以說清楚講明白我們到底要做什麼?哪些是我們的負責範圍、哪些不是?然後當然還有其他的細節，包含價格，時程等等都應該要寫在合約。 一定要訂好時程在這次的案子中我遇到最大的問題就是：我們開發好了 ，但是客戶卻一直沒時間驗收，導致每次來回 feedback 就可能拖了好幾個月。 這樣造成很多問題一個是實在是拖太久了，隔好幾個月才又重起專案做起來真的很煩；另一方面客戶方也會覺得我們做得很慢。因為我們做出來的東西不一定是客戶想要的，一來一回修正就花掉太多時間，整體而言就會有我們也開發得很慢的感覺。 如果當初可以在合約中訂好幾次 check point 的時間點，至少就可以確保不會無限期拖延下去。 一定要找好對口我這邊指得對口是指:「 可以快速獲得 feedback 的人」。因為我們這一次的開發包含了所有的設計，都是我們負責。但是呢，扯到設計這種比較主觀的東西，我們做出來的並不一定客戶接受。所以與其等到 check point 再來被客戶打槍要求重做，不如當初就先確保一個客戶方的對口，才可以在我們做 prototype 時就迅速的確認到底是不是符合他們想要的。 簡單來說，確認對口就是可以使我們少做很多白工。 不要使用別人不熟悉的工具這個點其實跟上一點有一點關係。案子開發到後期勢必是有一些 BUG 或其他要調整的部分。所以也是必須要追蹤這些 issue。那當然大家都認為 github issue tracker 很好用很棒，但事實上客戶這邊根本不會使用這種工具，最後就會變成他們根本沒在看。所以與其用我們工程師覺得好用的工具，不如使用大家都會用的工具。像是在這一次的案子裡我的 issue tracker 是簡單的用 Google sheet 拉一拉表格，這樣反而更能讓客戶清楚知道我們的進度。 即時通訊也是，工程師可能覺得 Slack 很棒很適合案子的溝通，但事實上就是客戶那邊永遠都不會上線 XD所以反而使用私人的 Facebook Messanger 還比較有效率。 錢的事情要講清楚有一件事情我體會深刻，就是「錢的事情一定要講清楚」。也不要覺得自己的開價會不會太貴，因為會不會太貴是客戶那邊該煩惱的 XD雖然每一個案子可能順利的程度不同，但是我個人認為接案一定會比想像中的麻煩。所以在錢的方面一定要開一個確保自己不會做到覺得「很不划算」。具體而言大概就是你覺得可以的價格再乘以 1.5 倍，到時候你會覺得這才只是「剛剛好的價格」。 另外，錢也是應該要有先付、後付的部分，才比較有點保障，至少不會一毛錢都拿不到的風險。像我這一次的案子就因為也沒有寫合約所以最後變成是結案了才要付錢，那這樣子其實對於接案方非常虧，因為他們隨時可以突然說案子不要做了，然後我就會變成做一堆但啥都拿不到。 總而言之，錢的事情就幾個原則： 開一個比自己覺得 ok 的價格高一點的價格，因為接案的過程肯定會比想像中的麻煩。 先付後付、怎麼分配都應該於一開始的合約中訂好。 雜談 自從決定要弄個部落格以後，就一直督促自己一個月起碼要寫個一篇文章，自今也滿一年了，可喜可賀～ 最近剛修正了身體的一個 BUG，看來要痛苦一陣子了。 正事依舊沒啥進展，果然我吃草吃太久了QQ","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"}]},{"title":"Coherent Line Drawing","slug":"Coherent-Line-Drawing","date":"2017-06-25T16:14:38.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2017/06/25/Coherent-Line-Drawing/","link":"","permalink":"https://ssarcandy.tw/2017/06/25/Coherent-Line-Drawing/","excerpt":"線條藝術畫(line drawing) 是最簡單的一種視覺呈現圖畫的方式，僅僅是幾條線條即能清楚的表示出圖片中的物件。這篇論文(Coherent Line Drawing’ by Kang et al, Proc. NPAR 2007)提出一個全自動的方法，可以將相片轉換成簡單、高品質的線條畫風格圖片。 輸入一張影像，即可產生出一張線條藝術風格畫。","text":"線條藝術畫(line drawing) 是最簡單的一種視覺呈現圖畫的方式，僅僅是幾條線條即能清楚的表示出圖片中的物件。這篇論文(Coherent Line Drawing’ by Kang et al, Proc. NPAR 2007)提出一個全自動的方法，可以將相片轉換成簡單、高品質的線條畫風格圖片。 輸入一張影像，即可產生出一張線條藝術風格畫。 Implementation本篇論文的方法主要流程如下： 由輸入影像產生邊緣向量流場 反覆的精煉、平滑化邊緣向量流場 藉由邊緣向量流場的資訊，對原圖套用高斯差以產生特徵線條 整個方法的流程圖： 由原圖產生邊緣向量流場，再藉由流場資訊套用高斯差產生線條畫。 Edge Tangent Flow由於線條畫的特性希望是能夠保留原圖重要的特徵邊緣，所以對應的邊緣向量流場(Edge Tangent Flow)必須滿足幾個條件： 可以描述重要邊緣的流向。 鄰近的向量必須要平滑(避免鄰近的向量方向差太多)，除非是角落。 重要的邊緣必須要維持它原本的方向。 Generate initial ETF而本篇產生 ETF 的方法是藉由反覆的平滑化流場來得到一個符合上述各個條件的 ETF。首先，藉由 原圖產生灰階梯度流場 (Gradient Vector Field)，再逆時鐘旋轉 90 度來 產生初始的邊 緣向量流場 (ETF)。 對原圖做適當模糊後即可取得灰階梯度向量(GVF)，再旋轉 90 度可得一個初始的 ETF。 Refining ETF再將這個初始的 ETF 做平滑化，方法如下： 其中， 是一個圓形 box filter function，落在外面的向量權重為零 為 magnitude weight function，用於確保重要的邊緣方向會被保留 為 direction weight function，用於使得鄰居的向量方向不會差距過大 則是當兩向量夾角過大時會反轉方向以確保夾角不會大於 90 度 透過以上的方法，只須要決定 kernel size 即可反覆平滑化 即可反覆平滑化 邊緣向量流場 直至夠平 滑為止 。 由左至右：原圖、用 GVF 得到之初始 ETF、經過一次平滑化、經過兩次平滑化。 kernel size=7 Line construction有了 ETF 之後就可以進入下一步：產生線條。比起一般邊緣偵測的方法如 Sobel、Canny 等等固定 kernel size，這篇論文的方法則是使用 flow-based kernel ，也就是 kernel 會沿著流場有著不一樣的形狀。 (a)原圖、(b)ETF、(c)沿著流場的 kernel。[1] Flow-based Difference-of-Gaussians根據 flow-based kernel 來進行 Difference-of-Gaussians(DoG) ，藉此來找出足夠符合重要線條的像素們。 (d)kernel 詳細圖示、(e)高斯差示意圖。[1] 首先先對每個像素沿著 ETF 垂直方向做一維的 DoG，亦即對圖中 -T~T 做 DoG： 其中 就是高斯差的部分。 再來再沿著 -S~S 做一維的高斯加權： 這樣的方法除了可以使邊緣更有一致性(不會有太多短線)並且又可以抑制雜訊，使得產生出的結果很符合線條畫的特性。這即是這篇所提出的 Flow-based Difference of Gaussians(FDoG)。 Iterative FDoG filtering有時候做一次 FDoG 效果並不夠好，所以可以藉由反覆做 FDoG 來達到更良好的效果。要反覆套用 FDoG 也很容易，只要將原圖與 FDoG 的輸出疊合，然後以這新的圖片當作原圖再次套用一次 FDoG 即可。 藉由將結果疊合回原圖再做一次 FDoG，可以使的結果品質越來越好。 實作上也很簡單，就是把結果的黑色部分直接覆蓋在原圖然後拿這再做 FDoG。注意雖然原圖改變了，但是使用的 ETF 並沒有重新計算。 123456789101112/** * Superimposing the black edge pixels of the previous binary output * upon the original image */for (int y = 0; y &lt; originalImg.rows; y++) { for (int x = 0; x &lt; originalImg.cols; x++) { float H = result.at&lt;uchar&gt;(y, x); if (H == 0) { originalImg.at&lt;uchar&gt;(y, x) = 0; } }} Results基本上圖片在 512x512 左右的大小可以在 3 秒內產生結果，若平行化則即使是更大張的圖片也可達到 real time。 輸入影像。由左上至右下：蔣公、廟、燈塔、老鷹。 輸出結果。由左上至右下：蔣公、廟、燈塔、老鷹。 Source codeYou can find my implementation source code at githubOr download pre-build version here Screenshot of my system user interface References Image from ‘Coherent Line Drawing’ by Kang et al, Proc. NPAR 2007 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"opencv","slug":"opencv","permalink":"https://ssarcandy.tw/tags/opencv/"},{"name":"rendering","slug":"rendering","permalink":"https://ssarcandy.tw/tags/rendering/"},{"name":"paper","slug":"paper","permalink":"https://ssarcandy.tw/tags/paper/"}]},{"title":"Panorama image stitching","slug":"panorama-image-stitching","date":"2017-05-25T17:28:09.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/05/25/panorama-image-stitching/","link":"","permalink":"https://ssarcandy.tw/2017/05/25/panorama-image-stitching/","excerpt":"全景照(panorama) 現在已普遍存在手機的相機軟體中，是一種可以拍攝數張照片然後接在一起使整個視野變更大的方法，全景照常比較常見的是往同一方向拍攝的，但也是有上下左右都全景的那種(Photo Sphere)。以一般往同方向拍攝的一系列照片來說，要合出一張完整的全景照理論上就是將這一系列的照片重疊的部分對齊就可以了。但實際上還是需要經過一系列的方法，才能接合出比較良好的全景照。","text":"全景照(panorama) 現在已普遍存在手機的相機軟體中，是一種可以拍攝數張照片然後接在一起使整個視野變更大的方法，全景照常比較常見的是往同一方向拍攝的，但也是有上下左右都全景的那種(Photo Sphere)。以一般往同方向拍攝的一系列照片來說，要合出一張完整的全景照理論上就是將這一系列的照片重疊的部分對齊就可以了。但實際上還是需要經過一系列的方法，才能接合出比較良好的全景照。 Warp images to cylinder由於360全景影像的特性，是利用相機旋轉拍攝出環場影像，所以在做偵測特徵點或將照片接起來之前，必須先投影到圓柱體上才會有比較好的效果。 內圈的黑線分別代表六張照片的成像平面，紅線則是要投影到的圓柱面。[1] 如同上圖所示，真正拍到的影像是內圈的黑線，分別代表六張照片，但是要接起來必須重新投影到外圈紅色才會接得上。 右邊的圖木紋的直線都變成有個曲率的彎線。 這是我將影像投影至圓柱前後的差別，可以看見右邊的圖木紋的直線都變成有個曲率的彎線，這就是投影之後造成的差異。 Feature detection要找出兩兩照片中的特徵點才好找出照片間的相對位置，所以首先要找出特徵點，這邊我實作的是 Harris corner detector。 左:用棋盤圖來當作測試資料；右: corner response image 這是利用 Harris corner 找出的 corner response image，可以看到交叉點的值都比較高(紅色點)，而這些就是應該要挑出來的特徵點。 Feature descriptorDescriptor 是用來描述特徵點的東西，通常是以高維度的向量來表示，我則是使用 Harris corner 算出 corner response image 之後，將最高的 1% 的點抓出來，以一個 5x5 的 window 來表示成 25 維的向量。 以周邊像素的值當作 descriptor。[1] 這是一個 3x3 window 所表示的9維向量，實作上我是使用 5x5。 找出來以後，我還做了一些處理來減少 feature points 的數目: 只挑選corner response 最高的 1% 切掉上下邊緣(因為投影到圓柱之後會有黑邊) 做 local suppression 由左至右分別是: 前 1% 的features、切掉上下邊緣、以一個 window size 掃過整張圖，每個 window 中只保留最強的 feature。 減少 feature 數目最主要的原因是因為希望加快運算速度。 Feature matching再來就是match出兩兩圖片中對應的點，我用的方法很簡單，就是將兩個影像中找出的 feature descriptors 互相算出向量距離，找出對應的組合。 123456for i in range(len(descriptor1)): min_distance = inf for j in range(len(descriptor2)): distance = ((descriptor1[i]-descriptor2[j])**2).sum() if distance &lt; min_distance: min_distance = distance 以 psudo-code 來表示的話就是兩個 for loop 一個一個算距離找出最小的那組合。是最暴力最直覺的方式 XDDD 其實我也有嘗試對 descriptors 建立 KD-tree 來加速整個 matching 的過程，但是由於 descriptor 是個 25 維的向量，使用 KD-tree 幾乎沒有速度上的優勢，所以最後還是只用這種最單純的算法。另外，為了達到更好以及更快的 matching 效果，我有定一個假設:「前後兩張影像的垂直差異不會過大」，雖說這假設並不一定成立，但是在 360 環場影像的狀況下還算符合。 兩張影像中對應的點，以紅色連線來表示。 Find best warp model using RANSAC再來就是要找出最好的 translation model，進而移動影像來讓兩張影像中相同的部分疊在一起，這邊我用 RANSAC 來找出最佳的 translation model。RANSAC 基本精神就是隨機挑兩個點的相對位置當作最好的 translation model，然後讓所有 matched pair 都套用這個 model，再來計算有多少個 pair 套用這 model 之後也是有落在可接受的範圍之內。這樣的過程重複跑個幾百次，就有很高的機率可以找出最佳的 translation model。 另外，有時候找出的 matched pair 其實並沒有很多，這時候比起使用隨機選點的方式，直接窮盡所有組合相對快上許多。 Stitch image with Blending有了兩張圖的相對位置之後就可以來連接兩張影像，但是直接連起來會有很明顯的縫，所以需要做blending。我實作了兩種blend method: Linear blend Linear blend with constant width Linear blend 是最簡單的方式，重疊的部分的顏色由兩張影像加權平均得出，加權的比重如下圖所示，x軸位置接近哪張影像則權重就比較高。 x軸位置接近哪張影像則權重就比較高 不過這樣會有一個問題，如果重疊的區域有棵樹在動，那這樣blend就會出現鬼影的問題。所以我就改成第二種方法，Linear blend with constant width，先找出兩張影像重疊部分的中心線，以此左右取個固定寬度再做 linear blend，這樣就能避免鬼影的問題。 只在兩張影像重疊部分的中心線左右一點點區間做 linear blend 以下是分別用兩種方法所接合的圖，可以看到用第一種方法的左邊的樹明顯糊糊的，右邊的則幾乎沒有鬼影。 左:第一種方法，可看到樹葉糊糊的；右:第二種方法，沒有鬼影現象。 End to end alignment and Crop完成所有影像接合之後，會有一些上下的誤差，這時候可以直接將誤差平均分配給每張影像，就可以得出一張比較平的圖片。 原始的接起來的影像，可以看到這一張有嚴重的上下飄移。 接完以後的原始影像 把誤差平均分配給大家，可以變成比較平的影像。 經過 end to end alignment 修正之後的圖 最後再把上下的黑邊切除掉，就可以得出一張完整的影像。 裁減掉黑邊的圖。 Results這邊展示幾個結果，一張綜覽全貌、一張是原始尺寸（可拖曳） 大雪山登山口。 台大總圖後面。 Testing image from internet Testing image from internet window.onload = function() { var panoramas = document.getElementsByClassName('pano'); Array.prototype.map.call(panoramas, function(pano) { Ps.initialize(pano); }); }; References Image from Digital Visual Effects(NTU) slides h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"opencv","slug":"opencv","permalink":"https://ssarcandy.tw/tags/opencv/"},{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"}]},{"title":"Migrate mail server to Gmail - migration strategy","slug":"migrate-to-gmail-migration-strategy","date":"2017-04-29T14:56:05.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/04/29/migrate-to-gmail-migration-strategy/","link":"","permalink":"https://ssarcandy.tw/2017/04/29/migrate-to-gmail-migration-strategy/","excerpt":"上一篇講了如何使用 google 提供的 migration tool 來遷移郵件，但需要知道大家的密碼才能用 IMAP 存取郵件資料。一般來說即使是 root 也是看不到密碼的，就算能也只是看到加密過後的密碼。而本篇要紀錄的就是我們遷移策略，包含我們怎麼繞過(?)密碼這關。","text":"上一篇講了如何使用 google 提供的 migration tool 來遷移郵件，但需要知道大家的密碼才能用 IMAP 存取郵件資料。一般來說即使是 root 也是看不到密碼的，就算能也只是看到加密過後的密碼。而本篇要紀錄的就是我們遷移策略，包含我們怎麼繞過(?)密碼這關。 概況我們的伺服器是透過 LDAP 集中管理身分認證資訊，大致架構如下: 伺服器架構: 由 LDAP 來統一管理身分資訊。 除了一般工作站以外，郵件伺服器也同樣是利用 LDAP 的資訊來登入，簡單來說就是使用者只須要記得一個密碼，就可以登入各工作站以及郵件伺服器。 繞過密碼想來想去最後想到兩種方法，一是從 LDAP 下手，一是從郵件伺服器下手。 從 LDAP 下手雖然 root 看不到大家的密碼，但是能直接更改別人的密碼 XD所以呢，我們可以複製出一個假的 LDAP server，就專給郵件伺服器用。 複製一個 LDAP，專門給郵件伺服器使用。 要複製一個 LDAP 並不困難，只需要安裝好 LDAP 之後把資料 dump 過來就完成了。創造好一個 fake LDAP 之後就要來改改郵件伺服器的設定，讓他改用 fake LDAP 的資訊來做身分認證。 首先要先更改 /etc/hosts ，把 LDAP 對應的 ip address 改對(如果有的話)，如果沒有用 /etc/hosts 的話，則記得要更改 roundcube 設定: 更改 roundcube ldap 1$ vim /etc/roundcube/main.inc.php 把 hosts 改成 fake LDAP: 123456789101112131415$config[&#x27;ldap_public&#x27;][&#x27;public&#x27;] = array( &#x27;name&#x27; =&gt; &#x27;Public LDAP Addressbook&#x27;, &#x27;hosts&#x27; =&gt; array(&#x27;fake LDAP&#x27;), &#x27;port&#x27; =&gt; 389, &#x27;user_specific&#x27; =&gt; false, &#x27;base_dn&#x27; =&gt; &#x27;ou=public,ou=rcabook,dc=localhost&#x27;, &#x27;bind_dn&#x27; =&gt; &#x27;cn=rcuser,ou=rcabook,dc=localhost&#x27;, &#x27;bind_pass&#x27; =&gt; &#x27;rcpass&#x27;, &#x27;filter&#x27; =&gt; &#x27;(objectClass=inetOrgPerson)&#x27;, &#x27;groups&#x27; =&gt; array( &#x27;base_dn&#x27; =&gt; &#x27;&#x27;, &#x27;filter&#x27; =&gt; &#x27;(objectClass=groupOfNames)&#x27;, &#x27;object_classes&#x27; =&gt; array(&quot;top&quot;, &quot;groupOfNames&quot;), ),); 詳細可以看 roundcube 說明文件。 弄好之後記得要刷新 nslcd: 1$ /etc/init.d/nslcd force-reload # maybe need to reload 接下來就是要更改大家密碼啦~由於郵件伺服器現在是使用 fake LDAP 驗證資訊了，所以這時候更改密碼只會改到 fake LDAP 上的資訊，而不影響大家登入使用其他工作站。也就是說使用這種方法的壞處只有在遷移的過程中不能登入原郵件伺服器。不過由於收發信件都已經轉移去 gmail 了，暫時不能登入原郵件伺服器其實影響應該不會太大了。 更改 ldap user password 12345$ ldappasswd -h &lt;ldap_ip&gt; -x -ZZ \\ -D &quot;cn=admin,dc=your_base_dn&quot; \\ &quot;user_distinguished_name&quot; \\ -s &lt;new_password&gt; \\ -w &lt;ldap_admin_password&gt; 以上是更改一個人 LDAP 密碼的方式，那要批次更改全部人的密碼可以這樣: 先去 /var/spool/mail/ 找誰有郵件帳戶，然後組合成 ldap dn ，輸出到一個檔案: 1234$ ls /var/spool/mail/ -l \\ | grep rw \\ | awk &#123;&#x27;print $3&#x27;&#125; \\ | xargs -I &#123;&#125; echo &quot;uid=&#123;&#125;,ou=people,dc=cmlab,dc=csie,dc=ntu,dc=edu,dc=tw&quot; &gt; ~/mail_all.txt 再組合出改密碼的指令並輸出到 change_password.sh 12$ cat mail_all.txt \\ | xargs -I &#123;&#125; echo &quot;ldappasswd -x -ZZ -D cn=admin,dc=your_base_dn &#123;&#125; -s &lt;new_password&gt; -w &lt;ldap_password&gt;&quot; &gt; change_passwd.sh 然後就可以: 1$ ./change_password.sh 從 Mail Account 下手從 LDAP 下手的基本上就可以解決密碼的問題，不過就是會在遷移的期間讓大家無法登入看舊信件(因為大家密碼被改掉了)，雖說影響應該不大，不過依舊是有影響的。 另一個繞過密碼的方式是從郵件帳戶下手，先創建一個假帳號，再把信件都複製到假帳號那邊，就可以用假帳號存取那個人的信件。 假設有個帳號 ssarcandy，實際方法如下: Add a fake user into ldap, i.e. ssarcandy_fake Create home dir for it, change the owner to ssarcandy_fake Copy /var/spool/mail/ssarcandy to /var/spool/mail/ssarcandy_fake, and chage owner Copy ~ssarcandy/mail/ to ~ssarcandy_fake/mail/ if it exist, and chage owner 寫成 script 大致如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/bin/bash # $1=new fake user -&gt; i.e. ssarcandy_fake# $2=correspond user -&gt; i.e. ssarcandy PASSWORD=&#x27;some_password&#x27;HOME_DIR_BASE=&#x27;/data/mail_migrate_tmp&#x27;LDAP_BASE=&#x27;base dn&#x27;LDAP_DN=&#x27;ldap admin dn&#x27;LDAP_PASS=&#x27;ldap admin password&#x27;LDAP_HOST=&#x27;ip address&#x27; echo &quot;dn: uid=$1,ou=people,$LDAP_BASEuid: $1sn: lastnamecn: complete namemail: $1@cmlab.csie.ntu.edu.twobjectClass: personobjectClass: organizationalPersonobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: topobjectClass: shadowAccountuserPassword: &#123;crypt&#125;12345shadowMax: 99999shadowWarning: 7loginShell: /bin/bashuidNumber: 6577gidNumber: 1000homeDirectory: $HOME_DIR_BASE/$1&quot;&gt; tmp.ldif # Create fake accountldapadd -h $LDAP_HOST -ZZ -x -w $LDAP_PASS -D $LDAP_DN -f tmp.ldif # Change fake account passwordldappasswd -h $LDAP_HOST -ZZ -x \\ -w $LDAP_PASS -D $LDAP_DN \\ &quot;uid=$1,ou=people,$LDAP_BASE&quot; -s $PASSWORD # Create home dir for fake accountmkdir -p $HOME_DIR_BASEmkdir $HOME_DIR_BASE/$1 # Copy mails to fake account.cp /var/spool/mail/$2 /var/spool/mail/$1cp -r $(eval echo ~$2)/mail $(eval echo ~$1) # Change owner of copied mailschown 6577:1000 /var/spool/mail/$1chown -R 6577:1000 $HOME_DIR_BASE/$1 這就是另一個繞過密碼的方式，這方式的優點是可以不影響使用者的帳戶，也就是在遷移過程中依舊可以登入舊郵件伺服器。但也是有缺點的，就是必須複製一份使用者的 mails，在遷移期間會浪費兩倍的容量。 遷移策略我們要遷移的郵件說多不多，但也不太少，以容量來說大概 600 GB 左右。不過空間上稍微尷尬一點，只剩 50 GB 可以使用。所以基本上是沒辦法讓大家都使用第二種方法(創假帳號並複製郵件)，先使用 fake LDAP 轉移大部分人會是比較適當的方式。 以時間軸的方式來呈現整個遷移的過程，大致如下: 設定 MX record信件收發轉至 Gmail建立 fake LDAP 讓 mail server 使用開始遷移大部分使用者，遷移期間使用者無法登入原郵件伺服器(因密碼不同)。讓 mail server 使用原 LDAP使用者可以正常登入原郵件伺服器。針對重要使用者建立假帳號開始轉移那些信件特多的使用者，使用建立假帳號的方式。善後完成轉移，刪除假帳號。 開始遷移信件的時間點是在已將 MX record 設定完，收發功能已轉至 gmail 之後才開始的。我們基本上分兩階段轉移信件，第一階段是用 fake LDAP 的方式轉移大部分的使用者；第二階段則是用假帳號的方式轉移最重要(信件特多)的使用者。 首先我們先對使用者的信件用量做分群，找出哪些人用量相似、哪些人用量特多等等。可以利用 dovecot 的 admin 工具查詢: 123# 查詢 ssarcandy 的總信件數目$ doveadm search -u ssarcandy ALL | wc -l14990 把大家的郵件數目分佈畫成圖表: 帳戶郵件分佈圖 可以看到基本上大部分人都沒甚麼信，只有少數人有超大量的信，而這些少數人會是遷移的 bottleneck，所以應該要挑出來用假帳號的方式轉移信件，才能避免讓原郵件伺服器不能登入的時間拉太長。 另外，由於 G Suite 遷移工具是一批一批遷移的，所以同一批最好大家的郵件數目都要差不多，才可以減少總體等待時間。 我們用以上的遷移策略，總共耗時約十天完成大家的所有信件遷移。當初預估一周真是太天真了阿QQ .timeline,.timeline li{padding:1em 0}.timeline{position:relative;width:660px;margin:20px auto 0;list-style-type:none}.timeline:before{position:absolute;left:50%;top:0;content:' ';display:block;width:6px;height:100%;margin-left:-3px;background:#505050;background:-moz-linear-gradient(top,rgba(80,80,80,0) 0,#505050 8%,#505050 92%,rgba(80,80,80,0) 100%);background:-webkit-gradient(linear,left top,left bottom,color-stop(0,rgba(30,87,153,1)),color-stop(100%,rgba(125,185,232,1)));background:-webkit-linear-gradient(top,rgba(80,80,80,0) 0,#505050 8%,#505050 92%,rgba(80,80,80,0) 100%);background:-o-linear-gradient(top,rgba(80,80,80,0) 0,#505050 8%,#505050 92%,rgba(80,80,80,0) 100%);background:-ms-linear-gradient(top,rgba(80,80,80,0) 0,#505050 8%,#505050 92%,rgba(80,80,80,0) 100%);background:linear-gradient(to bottom,rgba(80,80,80,0) 0,#505050 8%,#505050 92%,rgba(80,80,80,0) 100%);z-index:5}.direction-l,.direction-r{width:300px;position:relative}.timeline li:after{content:\"\";display:block;height:0;clear:both;visibility:hidden}.direction-l{float:left;text-align:right}.direction-r{float:right}.flag-wrapper{position:relative;display:inline-block;text-align:center}.flag{position:relative;display:inline;background:#eee;padding:6px 10px;border-radius:5px;font-weight:600;text-align:left}.direction-l .flag{-webkit-box-shadow:-1px 1px 1px rgba(0,0,0,.15),0 0 1px rgba(0,0,0,.15);-moz-box-shadow:-1px 1px 1px rgba(0,0,0,.15),0 0 1px rgba(0,0,0,.15);box-shadow:-1px 1px 1px rgba(0,0,0,.15),0 0 1px rgba(0,0,0,.15)}.direction-r .flag{-webkit-box-shadow:1px 1px 1px rgba(0,0,0,.15),0 0 1px rgba(0,0,0,.15);-moz-box-shadow:1px 1px 1px rgba(0,0,0,.15),0 0 1px rgba(0,0,0,.15);box-shadow:1px 1px 1px rgba(0,0,0,.15),0 0 1px rgba(0,0,0,.15)}.direction-l .flag:before,.direction-r .flag:before{position:absolute;top:50%;right:-36px;content:' ';display:block;width:12px;height:12px;margin-top:-10px;background:#eee;border-radius:10px;border:4px solid #ff5050;z-index:10}.direction-l .flag:after,.direction-r .flag:after{content:\"\";position:absolute;top:50%;height:0;width:0;margin-top:-8px;pointer-events:none}.direction-r .flag:before{left:-36px}.direction-l .flag:after{left:100%;border:solid transparent;border-left-color:#eee;border-width:8px}.direction-r .flag:after{right:100%;border:solid transparent;border-right-color:#eee;border-width:8px}.time-wrapper{display:inline;line-height:1em;font-size:.66666em;color:#fa5050;vertical-align:middle}.direction-l .time-wrapper{float:left}.direction-r .time-wrapper{float:right}.time{display:inline-block;padding:4px 6px;background:#eee}.desc{margin:1em .75em 0 0;line-height:1.5em}.direction-r .desc{margin:1em 0 0 .75em}@media screen and (max-width:660px){.timeline{width:100%;padding:4em 0 1em}.timeline li{padding:2em 0}.direction-l,.direction-r{float:none;width:100%;text-align:center}.flag-wrapper{text-align:center}.flag{background:#eee;z-index:15}.direction-l .flag:before,.direction-r .flag:before{position:absolute;top:-30px;left:calc(50% + 3px);content:' ';display:block;width:12px;height:12px;margin-left:-9px;background:#eee;border-radius:10px;border:4px solid #ff5050;z-index:10}.direction-l .flag:after,.direction-r .flag:after{content:\"\";position:absolute;left:50%;top:-8px;height:0;width:0;margin-left:-8px;border:solid transparent;border-bottom-color:#eee;border-width:8px;pointer-events:none}.time-wrapper{display:block;position:relative;margin:4px 0 0;z-index:14}.direction-l .time-wrapper,.direction-r .time-wrapper{float:none}.desc{position:relative;margin:1em 0 0;padding:1em;background:#f5f5f5;-webkit-box-shadow:0 0 1px rgba(0,0,0,.2);-moz-box-shadow:0 0 1px rgba(0,0,0,.2);box-shadow:0 0 1px rgba(0,0,0,.2);z-index:15}.direction-l .desc,.direction-r .desc{position:relative;margin:1em 1em 0;padding:1em;z-index:15}}@media screen and (min-width:400px ?? max-width:660px){.direction-l .desc,.direction-r .desc{margin:1em 4em 0}}","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"}]},{"title":"Migrate mail server to Gmail - using migration tool","slug":"migrate-to-gmail-using-migration-tool","date":"2017-04-23T11:36:51.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/04/23/migrate-to-gmail-using-migration-tool/","link":"","permalink":"https://ssarcandy.tw/2017/04/23/migrate-to-gmail-using-migration-tool/","excerpt":"最近 CMLab 終於申請到 G Suite for Education 了～也就是我們不用再維護自己的 mail server 了～灑花～ 但這也就又多了一件事：Migration…我們要做的事情有以下幾件： 讓 MX record 指到新的位置 遷移群組 遷移舊信 其中 2, 3 並非必須做的，但為了讓大家可以無痛轉移，我們才決定要搬信以及同步群組。1, 2 十分容易，就照著說明就可以完成，真的不會的話還可以打給 Google 救援 XD而本篇要紀錄的是 遷移舊信 的部分，這是其中最麻煩也最繁瑣的部分…","text":"最近 CMLab 終於申請到 G Suite for Education 了～也就是我們不用再維護自己的 mail server 了～灑花～ 但這也就又多了一件事：Migration…我們要做的事情有以下幾件： 讓 MX record 指到新的位置 遷移群組 遷移舊信 其中 2, 3 並非必須做的，但為了讓大家可以無痛轉移，我們才決定要搬信以及同步群組。1, 2 十分容易，就照著說明就可以完成，真的不會的話還可以打給 Google 救援 XD而本篇要紀錄的是 遷移舊信 的部分，這是其中最麻煩也最繁瑣的部分… 設定資料遷移工具G Suite 中有提供資料遷移的工具，其中包含遷移舊信件，他的方法是透過 IMAP 下載 dovecot 信件再匯入至 Gmail. 遷移設定中，要設定以下幾個東西： 原郵件伺服器類型 → 我們的例子要選 其他電子郵件伺服器 原本的 IMAP server 管理者帳號 → 這好像沒甚麼重要的，只是他會收到遷移報告書 Password mismatch這邊可能會遇到問題(帳號密碼不正確&#x2F;password mismatch?)，可以查看 /var/log/mail.log 中的詳細錯誤訊息： 12Mar 16 19:00:54 cml2 dovecot: auth-worker(10594): pam(ssarcandy@cmlab.csie.ntu.edu.tw,173.194.90.100):pam_authenticate() failed: Authentication failure (password mismatch?) 這是因為這邊使用 full email address 去登入 IMAP server，而原本我們 dovecot 的設定是只要打 user name 就好(不用加 @domain)，才會造成帳密不正確的問題。 為了迎合 G suite 的格式，去原郵件伺服器更改 dovecot 設定： 1234$ vim /etc/dovecot/conf.d/10-auth.conf # 找到 auth_username_format# 反註解並改成 auth_username_format = %n 其中 auth_username_format 是指登入的 username 格式，預設是只有 @ 前面的部分。由於 G suite 那邊就是要用 full address 登入，故這邊改成 %n ，亦即捨棄 @ 後面的東西，只留下 @ 前面的部分 。 改完之後記得重啟 dovecot 12$ service dovecot restart[ ok ] Restarting IMAP/POP3 mail server: dovecot. 設定遷移範圍下一步是要設定要遷移的時間、資料夾等，以我們的例子而言： 時間選到 cmlab 創立以來的全部 ～(遷移！全都遷移！ 資料夾則選擇忽略幾個特定資料夾，像是 virus-mail , trash-mail, spam-mail 等等 完整的資料遷移設定 選取遷移的使用者接下來可以直接上傳一個 csv 檔，其中包含： 原郵件伺服器帳號 原郵件伺服器密碼 對應的 gmail 帳號 上傳時有個潛規則，csv 檔不可以超過 500 行，超過它就會說未知錯誤(對，這是我們試出來的… 另外，在遷移時有幾個常見的錯誤： 驗證失敗錯誤(18017)：驗證失敗。這表示所提供的原郵件伺服器帳號、密碼不正確，也就是可能這個人不存在原郵件伺服器，或者提供的密碼不正確。 如果原郵件伺服器的驗證是像我們一樣透過 LDAP 的話，可以用 ldapsearch 確認這人在不在： 1$ ldapsearch -x &quot;uid=??,dc=base_dn&quot; 連線至來源郵件伺服器時發生錯誤錯誤(18002)：連線至來源郵件伺服器時發生錯誤。這錯誤是對應到 roundcube 的 連線到 imap 伺服器失敗 錯誤，有兩個可能： 因為那個人沒有家目錄 uid mismatch → /var/spool/mail/username 的擁有者是錯誤的 會發生沒有家目錄的情況在我們的狀況而言就是那個人已被停權，那就根本不用遷移了。而 uid mismatch 則是去 /var/spool/mail/username 更改成正確的擁有者就好了。 信箱錯誤。無法開啟資料夾或郵件錯誤(18006)：信箱錯誤。無法開啟資料夾或郵件。這應該是屬於暫時性的錯誤，晚一點再重試看看。 Google 其實有提供一份錯誤一覽表 → 錯誤一覽表 (雖然有些有寫跟沒寫一樣… 怎麼得到大家的密碼我想看到這邊應該會發現一個問題：遷移的 csv 檔其中一欄要提供大家的密碼，這是即使是有 root 權限也沒辦法知道的資訊欸！雖然我們最後還是有想到辦法，不過那過程也很繁瑣，就留到下一次介紹吧…","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"}]},{"title":"High Dynamic Range Imaging","slug":"High-Dynamic-Range-Imaging","date":"2017-04-15T17:34:44.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2017/04/15/High-Dynamic-Range-Imaging/","link":"","permalink":"https://ssarcandy.tw/2017/04/15/High-Dynamic-Range-Imaging/","excerpt":"高動態範圍成像（英語：High Dynamic Range Imaging，簡稱HDRI或HDR），在電腦圖形學與電影攝影術中，是用來實現比普通數點陣圖像技術更大曝光動態範圍（即更大的明暗差別）的一組技術。高動態範圍成像的目的就是要正確地表示真實世界中從太陽光直射到最暗的陰影這樣大的範圍亮度。 – from wikipedia 由於一般數位相機的影像就是每個 pixel 8 bits，能夠表現的能量範圍就是這麼窄(過暴就全白，太暗就全黑)。然而真實世界的能量範圍是非常廣的，為了能夠在 0~255 之間表現出最豐富的細節(亮部即暗部的細節)，才有所謂的 HDR 技術。HDR Image 可以從多張不同曝光時間的照片中組合出來，而整個流程大致包含了: 影像對齊 計算出真實能量分佈 把影像壓縮回低動態範圍成像(一般螢幕才能顯示)","text":"高動態範圍成像（英語：High Dynamic Range Imaging，簡稱HDRI或HDR），在電腦圖形學與電影攝影術中，是用來實現比普通數點陣圖像技術更大曝光動態範圍（即更大的明暗差別）的一組技術。高動態範圍成像的目的就是要正確地表示真實世界中從太陽光直射到最暗的陰影這樣大的範圍亮度。 – from wikipedia 由於一般數位相機的影像就是每個 pixel 8 bits，能夠表現的能量範圍就是這麼窄(過暴就全白，太暗就全黑)。然而真實世界的能量範圍是非常廣的，為了能夠在 0~255 之間表現出最豐富的細節(亮部即暗部的細節)，才有所謂的 HDR 技術。HDR Image 可以從多張不同曝光時間的照片中組合出來，而整個流程大致包含了: 影像對齊 計算出真實能量分佈 把影像壓縮回低動態範圍成像(一般螢幕才能顯示) Image Alignment拍攝時，即使有用腳架固定，也無法確保每張照片拍到的都完全沒有晃到，如果晃到的話，到時候的結果就會糊糊的。所以，第一步就是要先對其各張影像。我們可以以第一張照片為標準，其餘的照片都想辦法對齊他。 Naïve Alignment最 naïve 的演算法，就是在一個範圍內移動找出差值最小的offset，pseudo-code如下: 12345678offset = (0, 0)difference = infinityfor y in [min..max]: for x in [min..max]: diff = find_diff(img[0], offset(img[i], (y, x)) if diff &lt; difference: difference = diff offset = (y, x) 如何訂 min, max 值是個難題，太大會效率差，太小可能會沒找最好的解。不過依據我的經驗，拍攝的時候有搭配腳架以及快門線的話，其實誤差差不多都在 5 pixels 之間，所以可以把 Median Threshold BitmapMedian Threshold Bitmap Alignment[1] 演算法，是利用金字塔的方式(每層圖片都為上一層的四倍)從最小的開始比對，在九個鄰居內做移動，累計各方向的誤差選擇最小的方向移動之，再向上傳遞到兩倍的圖再做一次。基本上每一張都是去對齊第一張。 實作的流程大略如下: 產生 binary-threshold image這邊的閾值由影像的中位數值決定。 我實作的 MTB 所產生的 binary-threshold image 產生 exclude mask由於那些太接近閾值的像素有可能會造成誤差，故將太接近閾值的像素標示出來，在比對時就直接跳過不比對。假設要忽略的是 閾值 ± 10 ，可以用 opencv 的 cv2.inRange()達成 1mask_img = cv2.inRange(img, median - 10, median + 10) 這是用於跳過過於接近閾值的像素的 MASK 由最小至最大的順序比對影像差異在每一個層級中，都是往九個鄰居移動，看哪個最小，再往上傳遞繼續做這個比對。由於影像都是 binary image，所以要比對影像差異挺容易的:影像差異 = img1 XOR img2 AND mask越小層級的移動的權重越大。 Construct HDR由於一般數位相機的影像就是每個 pixel 8 bits，所以每台數位相機其實都有自己對應的 response curveResponse curve 是在決定接收到多少能量該轉成多少的值(此值是 [0~255])。那我就可以透過多張不同曝光時間的影像來反推出這個 response curve，有了 response curve 之後就可以進一步算出真實能量分佈圖。 Solving response curve我建立 HDR 影像的方法為 1997 Debevec[2] 的方法。由於論文很佛心的有提供 MatLab Code，所以我就直接拿他為基底改寫成 python 程式碼，並利用 numpy 提供的 np.linalg.lstsq(A, b) 解 的 解。 我將 RGB channel 分別計算 response curve，並全部畫在一圖表上檢視，以下是我做的幾個例子。 另外，Debevec 論文[2]所提及的 (控制 response curve smoothness 程度)，並不好掌握最好的值，不過依據實驗 值大約在 30~50 就還不錯。 關於 smaple 點的方式，我試了兩種: Random 取 50 個點好處是很直覺且容易實作，但我發現做出來的 response curve 有時會有很不一樣的結果，比較不可靠。以下圖為例，可以看到在綠色的 channel 就有比較怪異的結果，藍色在頂部也是怪怪的。 Random sampling 50 points Response curve 把圖片縮成 10x10，全部拿去算由於縮小圖片基本上還是能保有圖片的特徵(特亮的、特暗之類的)，所以做出來的效果也比較好一點，左圖是縮放到 10x10 的影像，右為 response curve: Shrink original img to 10x10 and use all pixels Response curve 所以最後我採用第二種方法。 Construct radiance map再藉由 Debevec 論文[2]上 Equation(6) 所提的 construct radiance map:透過剛剛產生的 response curve 帶入公式來得到 radiance map。 其中 函式就是剛剛產生的 response curve。有了 radiance map之後，就可以套用假色來顯示出真實能量分佈圖，以下是我做的幾的例子(這邊的值都是log value) Tone mapping到此已經重建出 Radiance map 了，也就是已經知道真實能量分佈了，但這能量範圍太廣，沒辦法直接顯示到一般的顯示器上(0~255)所以需要再把這樣高動態範圍成像壓回低動態範圍成像，不過這壓縮的方式有其學問，如何才能讓低動態範圍成像看起來像高動態範圍成像就是 Tone mapping 在做的事。 根據不同的 Cases，最適合的演算法不見得相同，所以就是要看情況決定。這邊展示幾個 Tone mapping 之後的結果: Result galleries Living room, tone map algroithm: gdc, Camera: Nikon D5000 Taipei city, tone map algroithm: drago, Camera: Nikon D5000 Street, tone map algroithm: drago, Camera: Nikon D5000 Street at night, tone map algroithm: ashikhmin, Camera: Nikon D5000 References Fast, Robust Image Registration for Compositing High Dynamic Range Photographs from Handheld Exposures, G. Ward, JGT 2003 Recovering High Dynamic Range Radiance Maps from Photographs, Paul E. Debevec, Jitendra Malik, SIGGRAPH 1997 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; } 雜談 結果說真的還是有不少人工的感覺，可能我太廢了吧…QQ Latex 配 markdown 要注意該死的 _，在 Hexo 中底線會先被視為斜體字，所以在 Latex 中底線前面要用跳脫字元。 Hexo 的 markdown 真 robust! 愛怎亂搞都會跟預期中的一樣，也可以亂加 html 語法～(望向 github… 這篇文章照片真多，竟然要下載 30MB 之多，超不 friendly der~","categories":[],"tags":[{"name":"opencv","slug":"opencv","permalink":"https://ssarcandy.tw/tags/opencv/"},{"name":"paper","slug":"paper","permalink":"https://ssarcandy.tw/tags/paper/"},{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"}]},{"title":"Setup x11 forwarding on Debian","slug":"Setup-x11-forwarding-on-Debian","date":"2017-03-19T18:03:08.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2017/03/19/Setup-x11-forwarding-on-Debian/","link":"","permalink":"https://ssarcandy.tw/2017/03/19/Setup-x11-forwarding-on-Debian/","excerpt":"有時候會需要在 server 上使用有 GUI 介面的程式，像是我們會把 matlab 安裝在運算能力很好的工作站上供大家使用。但透過 ssh 連上工作站的話介面會是 terminal，使用上就會比較不便。 Linux 的 GUI 運作模式，其中 X-client 負責程式的運作 X-server 負責畫面的顯示 所以只要把工作站上的圖形顯示丟到本地端(windows&#x2F;mac)的 X-server，就可以顯示出來了。本篇將介紹如何設定才能使 Debian Server 的 GUI 程式的畫面顯示到 client 的電腦上。","text":"有時候會需要在 server 上使用有 GUI 介面的程式，像是我們會把 matlab 安裝在運算能力很好的工作站上供大家使用。但透過 ssh 連上工作站的話介面會是 terminal，使用上就會比較不便。 Linux 的 GUI 運作模式，其中 X-client 負責程式的運作 X-server 負責畫面的顯示 所以只要把工作站上的圖形顯示丟到本地端(windows&#x2F;mac)的 X-server，就可以顯示出來了。本篇將介紹如何設定才能使 Debian Server 的 GUI 程式的畫面顯示到 client 的電腦上。 Debian Server安裝 X-server如果 server 一開始就是灌沒有桌面環境的，現在就要安裝一下: 1$ apt-get install xserver-xorg-core 也可以裝個 gvim 來測試。 1$ apt-get install vim-gtk 啟用 X11Forwardingserver 這邊，必須要允許 ssh 的連線 forward 這些圖形介面的資訊到 client 端，所以需要去 /etc/ssh/ssh_config 中設定: 1234$ vim /etc/ssh/ssh_config # add this line:X11Forwarding yes 設定好記的重啟 sshd 服務: 1$ service sshd reload Client Setup本機端也是要有相應的設定才能正確地接收 server forward 過來的圖形介面，Mac 十分容易，而Windows 的設定比較麻煩。 Mac打開你的 terminal，用 ssh 連線至主機: 1234$ ssh -X user@example.com # on remote server$ gvim # open vim GUI version for testing 不過最近 Mac 已不再內建 X11[1]，所以如果你系統版本高於 Sierra，則必須下載 XQuartz。 Windows在 windows 上需要安裝 X-server 才能使用 x11 forwarding，我推薦使用 Xming。下載並安裝好以後，確認通知列有出現 Xming 的圖示。 並且要再更改 server 上的 ssh_config : 12345$ vim /etc/ssh/ssh_config # add these two linesX11DisplayOffset 10X11UseLocalhost yes 記得重啟 sshd service 接下來使用 PuTTy 來連線: Session &gt; Host Name: 輸入 server ip Connection &gt; SSH &gt; X11: Enable X11 forwarding → 打勾 x display location: localhost:0 連上之後，可以試試輸入以下指令: 12$ echo $DISPLAYlocalhost:10.0 如果看到如上的回傳，就表示一切正常。 試試 gvim ，就會跳出小巧可愛的視窗: 透過 X11-forwarding 讓 server 上的 gvim 顯示到本機(windows) References Mac 已不再隨附 X11 SSH X11 Forwarding Installing&#x2F;Configuring PuTTy and Xming Setup X11 Forwarding over SSH on Debian Wheezy h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"}]},{"title":"防止 mail server 大量寄信的手段","slug":"prevent-mail-server-sending-spam","date":"2017-02-26T13:14:07.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2017/02/26/prevent-mail-server-sending-spam/","link":"","permalink":"https://ssarcandy.tw/2017/02/26/prevent-mail-server-sending-spam/","excerpt":"最近這幾周都一直被 cmlab 的 mail server 霸凌，覺得難過…為了解決 mail server 時不時會大量寄出信件的問題，嘗試了許多方法，終於得到一點點平靜…本篇整理了我們最近嘗試的方法及一些工具，做個紀錄…","text":"最近這幾周都一直被 cmlab 的 mail server 霸凌，覺得難過…為了解決 mail server 時不時會大量寄出信件的問題，嘗試了許多方法，終於得到一點點平靜…本篇整理了我們最近嘗試的方法及一些工具，做個紀錄… 流量分析及監控先介紹兩個好工具， pflogsumm mailgraph 方便監控以及分析 mail server 的狀況: pflogsummpflogsumm 是個可以把 mail.log 整理成一份比較好閱讀的報告，可以一目瞭然這時段內共收發多少信、誰寄最多信、誰收最多信等等的資訊。用法也很簡單: 123456789101112131415161718192021222324$ apt-get install pflogsumm # install$ pflogsumm /var/log/mail.log -d today # generate today&#x27;s report Postfix log summaries for Feb 26 Grand Totals------------messages 659 received 1047 delivered 135 forwarded 0 deferred 2 bounced 3285 rejected (75%) 0 reject warnings 0 held 0 discarded (0%) 15378k bytes received 20065k bytes delivered 142 senders 88 sending hosts/domains 156 recipients 15 recipient hosts/domains 這邊只列出 report 的一部份，還有很多有用的資訊，可以自己試試看。 mailgraph這是一個視覺化圖表呈現 mail server 狀態的工具，顯示整個時間軸收發了多少信之類的資訊，介面大概長這樣: mailgrapgh 網頁統計資訊 這是以網頁來呈現的，原理就是每幾分鐘就會去整理 mail.log 中的資訊，然後產生圖表再呈現在網頁上。安裝流程如下: 12$ apt-get install rrdtool mailgraph$ dpkg-reconfigure mailgraph 然後會問你三個問題 Should Mailgraph start on boot? &lt;– Yes Logfile used by mailgraph: &lt;– /var/log/mail.log Count incoming mail as outgoing mail? 這要看你是否有安裝一些過濾器 (amavisd 之類的)，有的話就選 NO，反之則選 YES 再來就是把對應檔案搬到 web server 下面 123$ mkdir /var/www/mailgrapgh # create a folder for mailgraph$ cp -p /usr/lib/cgi-bin/mailgraph.cgi /var/www/mailgraph $ cp -p /usr/lib/cgi-bin/mailgraph.css /var/www/mailgraph 然後就可以連上 http://yorurdomain.com/mailgraph/mailgraph.cgi 查看結果。 看 mail.log 揪出亂寄者通常 mail server 寄信量暴增都是因為有使用者在大量寄信，無論是真人在搞鬼或是有程式在惡意寄信，都應該視情況直接封鎖他寄信的功能。 找 nrcpt 過高者1$ cat mail.log | egrep &#x27;nrcpt=[1-9][0-9]&#x27; nrcpt 是 number of recipients 的縮寫，nrcpt 很大表示這封信要寄給很多人，合不合理還是要看情況，但至少由此下手比較能夠找到搞鬼的人。 看 pflogsumm 誰寄太多信12345678910$ pflogsumm /var/log/mail.log -d today Senders by message count------------------------ 1060 xxx@cmlab.csie.ntu.edu.tw 32 xxx@cmlab.csie.ntu.edu.tw 12 xxx@cmlab.csie.ntu.edu.tw 6 xxx@cml0.csie.ntu.edu.tw 2 xxx@cmlab.csie.ntu.edu.tw 2 xxx@ntu.edu.tw 找到誰很詭異寄很多信之後，就加入 Postfix 的 sender 黑名單中限制寄信。(記得更新 .db 檔) 在 Postfix main.cf 中加入 sender-access 的黑名單: 123smtpd_sender_restrictions = check_sender_access hash:/etc/postfix/sender-access, ... 123456$ vim sender-access # add this line to disable sender function# xxx@cmlab.csie.ntu.edu.tw REJECT sorry, we don&#x27;t provide smtp service for you. $ postmap sender-access # update .db file 設定內收外寄流量控制有時候單靠黑名單其實有點治標不治本，因為今天鎖了 A 明天可能是 B 在大量寄信，所以若能設定每個使用者一小時允許的寄信量才可以防止這類大量寄信的事情發生。 Postfix 並沒有相關的設定可以設，必須依靠別的程式，而看起來最好的就是 Policyd 了我之前已經有一篇是 安裝 Policyd 並設定外寄 Quota 了，可以直接參考那篇。 不過有幾點是需要注意的，我一開始設定是設定成「若超過寄信額度，則延後寄信(DEFER)」，我之所以這樣設是不想讓使用者的信沒寄出去，只是讓他晚一點寄出去。但這樣就又可能造成某人大量寄信結果因為超過額度所以全部都塞在 mail queue 中….崩潰…. 所以我現在就直接設成「若超過寄信額度，則直接拒絕寄信(REJECT)」，這樣比較乾脆～ 設定重送間隔、生命週期有時候信件會寄不出去(可能是對方容量滿了、網路問題等等、或是對方伺服器黑名單我們的信)，Postfix 預設是有重送機制的，但假設信件一直重送而且又一直寄不出去那 mail queue 就會累積越來越多信，最後就會有超大寄信流量，而這時候就會被學校限制 IP 了。 Postfix 重送的相關設定預設值都相當長，像是一封信他能夠存活在 mail queue 中的時間竟然是五天，這也表示如果有一封信寄不出去的話 Postfix 會鍥而不捨的連試五天….. 以下列出 Postfix 有關於重送的設定及其預設值，想看每個的詳細說明可以看官方文件 12345minimal_backoff_time (default: 300s)maximal_backoff_time (default: 4000s)queue_run_delay (default: 300s)maximal_queue_lifetime (default: 5d)bounce_queue_lifetime (default: 5d) 根據我們自己 mail server 的用量，我最後將設定改為如下，這樣應該就可以避免 queue 中塞滿寄不出去的信的狀況了。 12345minimal_backoff_time = 10mmaximal_backoff_time = 30mqueue_run_delay = 10mbounce_queue_lifetime = 2hmaximal_queue_lifetime = 2h 最近搞 mail server 真的大崩潰，連帶系上跟者我們一起崩潰 Sorry…QQ果然還是盡快轉移到 G Suite 好了…..","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"}]},{"title":"From React to React Native","slug":"from-react-to-react-native","date":"2017-02-02T03:04:00.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/02/02/from-react-to-react-native/","link":"","permalink":"https://ssarcandy.tw/2017/02/02/from-react-to-react-native/","excerpt":"接觸 React 其實也一段時間了，總是嚷嚷想做個自己的 Project 但始終沒有動手開始做。最近趁著寒假終於用 React 做了個網頁小遊戲，之後也改寫成 React Native 做出 Android and iOS 的原生 app，順便把 android 版上架到 Google play 上～(iOS app store 費用太高付不起…)算是完成幾個長久以來的小小目標(?) 用 React Native 做出 Android/iOS 原生 app","text":"接觸 React 其實也一段時間了，總是嚷嚷想做個自己的 Project 但始終沒有動手開始做。最近趁著寒假終於用 React 做了個網頁小遊戲，之後也改寫成 React Native 做出 Android and iOS 的原生 app，順便把 android 版上架到 Google play 上～(iOS app store 費用太高付不起…)算是完成幾個長久以來的小小目標(?) 用 React Native 做出 Android/iOS 原生 app 說真的寫好 React 版本以後要改寫成 React Native 還挺容易的，基本上要改的只有介面的部分，對應到程式碼大概就是每個 Component 中的 render()，當然還有 CSS 要改成 react-native 的 style，Animation 也不能用 CSS 來做了，這也是比較麻煩的地方。 檔案結構React 版跟 React Native 版的檔案結構其實幾乎都是差不多的，可以看下圖的對應： 左圖是 React 的檔案結構；右圖是 React Native 的檔案結構。 最大的差異是 style/ 資料夾不見了，這是因為 React Native 的 style 我都寫在 components 檔案裡面了。React 版本的 index.js 是進入點。其他檔案基本上都維持一樣的結構、React 版本定義的 components 在 react-native 版中都依舊存在。 左為 React 版；右為 React Native 版，元件完全一致。 改寫 render() 邏輯可以重用，要改的只有渲染的部分。 我在改寫的時候，反正第一步就是把 &lt;div&gt; 通通改成 &lt;View&gt; ，把包住文字的 &lt;span&gt;、&lt;div&gt; 改成 &lt;Text&gt; ，這樣大概就完成一半了吧(?)剩下的一半就是找找最適合的 native component ，這些可以上官方文件尋找。 改寫 Style 看似像 CSS，但又沒這麼好用，比較像是閹割版的 CSS。 React Native 的 style 是個 javascript 的物件，大概有七成可以跟 CSS 直接對應，寫法就是原本 CSS 改成 camlCase 寫，如果要寫得像 css 的 class 的話還要使用 StyleSheet.create() : 1234567891011// css.name &#123; font-size: 20px;&#125; // react nativeconst styles = StyleSheet.create(&#123; name: &#123; fontSize: 20 &#125;&#125;); 套用樣式的寫法則是直接用 style=&#123;Object&#125; ，若要套用複合樣式，則在 style 中放一個 object array(順序有差): 1234&lt;View&gt; &lt;Text style=&#123;styles.red&#125;&gt;just red&lt;/Text&gt; &lt;Text style=&#123;[styles.red, styles.bigblue]&#125;&gt;red, then bigblue&lt;/Text&gt;&lt;/View&gt; Animation原本 React 版的動畫全都是用 css 做出來的，但這些在 React Native 中就沒辦法用了，官方有提供 Animation 相關的 API，但說實在的真的有點難搞。後來我是用某大神寫的 library react-native-animatable，比起官方提供的更好用。 Divide and Conquer 改寫要一部份一部份改比較容易，一次要全改只會要你命！ 從 React 改成 React Native 雖然好像不用費很大的功夫，但是如果想要一次到位全部改好其實還是很困難的。我這個 Project 已經是很小規模的了(約 500 多行)，但第一次想一次到位時我花了一兩個小時還是連 build 都沒辦法成功。所以果然還是一部份一部份改起來比較輕鬆。以我的例子而言，是先改 &lt;StatusPanel/&gt; ，因為只要會倒數就好嘛，單純了自然就比較好寫；再來改 &lt;ArrowKey/&gt; 最後才是改 &lt;QuestionList/&gt; 跟 &lt;Question/&gt; ，這樣分批改前前後後大概只花兩個小時就全部搞定了。 一些坑以及一些筆記進入點註冊程式就好React Native 官方 example 的主程式都是寫在 index.android.js (或 .ios.js ) 裡面，個人感覺是可以把主邏輯拆出來，進入點只負責註冊程式就好: 12345import &#123; AppRegistry &#125; from &#x27;react-native&#x27;;import colorblocksRN from &#x27;./src/app&#x27;; // 主程式 component // 進入點只做 register 的動作AppRegistry.registerComponent(&#x27;colorblocksRN&#x27;, () =&gt; colorblocksRN); iOS 樣式有差即使用一模一樣的 code，iOS 跑起來樣式跟 android 的還是有差別，這我也不知道為甚麼，反正最後記得針對 iOS 的樣式再修改修改。 不能 react-native run-ios假設遇到這問題，首先先檢查 port 8081 有沒有被佔用了: 12$ lsof -n -i4TCP:8081$ kill &lt;pid&gt; 如果問題沒排除，試試 1sudo react-native run-ios","categories":[],"tags":[{"name":"react","slug":"react","permalink":"https://ssarcandy.tw/tags/react/"},{"name":"react native","slug":"react-native","permalink":"https://ssarcandy.tw/tags/react-native/"}]},{"title":"Light Field Camera","slug":"pbrt-light-field-camera","date":"2017-01-18T11:04:29.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2017/01/18/pbrt-light-field-camera/","link":"","permalink":"https://ssarcandy.tw/2017/01/18/pbrt-light-field-camera/","excerpt":"一般傳統相機都是先對焦好之後拍攝照片，而往往會有些照片事後才發現竟然沒對好焦，甚是可惜。而光場相機(light field camera)，有別於一般傳統相機，是可以記錄相機內部的光線傳輸方向等信息的相機；光場相機就是比傻瓜相機還傻瓜的相機，允許再拍攝後根據拍攝者的需要再重新聚焦到任意的位置光場相機可以做到先拍攝，後對焦這種神奇的事情。 光場相機其實是在相機主鏡頭後面加了一層微透鏡陣列，讓原本聚焦的光再次分散到各個感光點上，如圖一： 圖一：在成像平面前加一個微透鏡的陣列。微透鏡陣列的平面在這裡是 st 平面。在微透鏡陣列後面的感光元件上，每一個像素對應著 uv 平面上一個區域射到此像素對應的微透鏡上光強的和。圖源[1]","text":"一般傳統相機都是先對焦好之後拍攝照片，而往往會有些照片事後才發現竟然沒對好焦，甚是可惜。而光場相機(light field camera)，有別於一般傳統相機，是可以記錄相機內部的光線傳輸方向等信息的相機；光場相機就是比傻瓜相機還傻瓜的相機，允許再拍攝後根據拍攝者的需要再重新聚焦到任意的位置光場相機可以做到先拍攝，後對焦這種神奇的事情。 光場相機其實是在相機主鏡頭後面加了一層微透鏡陣列，讓原本聚焦的光再次分散到各個感光點上，如圖一： 圖一：在成像平面前加一個微透鏡的陣列。微透鏡陣列的平面在這裡是 st 平面。在微透鏡陣列後面的感光元件上，每一個像素對應著 uv 平面上一個區域射到此像素對應的微透鏡上光強的和。圖源[1] 光場相機每個像素紀錄的則是只有一條光線。由於光場相機這樣的設計，所以光場相機所拍出的原始影像很特別，會是一格一格的，如圖二所示。 圖二：光場相機所拍出的原始影像是一格一格的。 這是因為微透鏡陣列對主透鏡聚集的光再成像的緣故，每一格都是對應的微透鏡的成像。事實上，可以把每個微透鏡都當作一個小小針孔相機，如圖三(c)所示。 另外，藉由對這些微透鏡影像的重新排列，可以生成不同視角的影像。具體來說，就是挑選每個微透鏡的同一格像素，組合出一張長寬分別等同於微透鏡在長邊以及寬邊的數量的影像。而一張光場相機原始影像能夠產生多少不同視角的影像，端看每個微透鏡後面對應到的像素有幾個。 圖三：(a)把每個微透鏡相同位置的像素取出來合成一張圖，可以得出不同視角的影像。(b)由光場影像得出的所有視角一覽。(c)每個微透鏡可以視為一針孔相機。 數位重對焦我以之前做的真實相機系統為基礎，在主透鏡焦距上放置微透鏡陣列，藉此來模擬光場相機的硬體設備。光場相機原始影像紀錄了四維的光線資訊， 這個函式代表從主透鏡的 2D 點 射到微透鏡的 2D 點 的光線能量，利用這個資訊，就可以進行數位重對焦。關於如何進行數位重對焦，可以看圖四。 圖四：在 s' 平面重新聚焦，即是讓所有光錐都落在 s' 平面上，而對於數位重對焦而言，必須透過蒐集ｓ平面上的資訊來達成。圖源[2] 原始焦距的比例。假設要重對焦影像至 平面，可以由已知算出： 不過由於光線能量函式是 ，所以必須把 改寫成 的形式： 所以對於一個重對焦影像的像素而言，能量可以這樣求得： 其實應該要寫成積分形式才是真實狀況，但是由於真正得到的資料是離散的(像素)，所以用加總的就可以了。 結果我分成兩塊實作: 模擬光場相機成像，這部分是以 pbrt 為基礎再加新的 class 來實作。 利用光場相機的原始影像來做數位重對焦，這部分就是另外用 python 來做。 這樣做也使得我在驗證時可以分別驗證，讓實作上除錯較為容易。 模擬光場相機的部分，我也是用之前的場景檔為基礎，加以更改後來測試我的模擬效果。微透鏡的數目越多能使之後數位重對焦的影有更高的畫質，而微透鏡的大小則關係著能夠有多少不同的視角（亦即重對焦能力的高低）。 圖五：(a)微透鏡大小較大，但數位重對焦以後會是比較低畫質的影像。(b) 微透鏡大小較小，數位重對焦以後會是比較高畫質的影像。 圖六：數位重對焦結果。左上、右上、左下、右下分別對焦點為：遠、中、近、更近。四張皆裁切掉上方黑色區塊。 由圖六可以看出焦距的改變影響清楚的部分，雖然其實沒有十分明顯，但還是可以看到右上角的圖清楚的部分是紅龍的背部及尾部，而左下清楚的則是藍龍的背部。 另外，利用光場相機的原始影像來做數位重對焦的部分除了拿我自己產生出的光場相機影像來試驗以外，我也利用網路[5]直接尋找了一張光場相機的影像，並直接拿來當作我數位重對焦的測試資料，這是九個不同字母分別在不同的距離處。可以看見成功的對焦到不同的字母上。 圖七：(a)從網路[5]上獲得的第三方光場相機原始影像。(b)用(a)來測試我實作的數位重對焦，可以看見成功的對焦到不同的字母上。 雜談 其實自己模擬出來的光場影像做數位重對焦的時候效果一直不太理想，搞不懂是哪邊有問題QQ 不過用別人的光場影像做數位重對焦很成功阿，肯定是 pbrt 那邊我寫錯了什麼… 多虧有 CMLab 的眾多強力工作站，讓我免於跑一張圖就要浪費十幾個小時的時間… 查資料一直看到 Lytro 的開箱文…. markdown + latex 還真多要注意的，markdown 的_會先被轉成&lt;em&gt;然後像是\\sum _{u}的 latex 就會變成\\sum &lt;em&gt;{u}… References Light Field Photography with a Hand-held Plenoptic Camera, R Ng, M Levoy, M Brédif, G Duval, M Horowitz, P Hanrahan CS348b Project： Light Field Camera Simulation, Zahid Hossain, Adam Spilfogel Backery, Yanlin Chen Fourier Slice Photography, Ren Ng, ACM Transactions on Graphics, July 2005 The (New) Stanford Light Field Archive 光場相機原理及仿真實現 LYTRO Light Field Camera 原理解析 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"rendering","slug":"rendering","permalink":"https://ssarcandy.tw/tags/rendering/"},{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"},{"name":"pbrt","slug":"pbrt","permalink":"https://ssarcandy.tw/tags/pbrt/"}]},{"title":"MSAuto - 玩遊戲也要自動化","slug":"MSAuto","date":"2016-12-25T13:12:48.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2016/12/25/MSAuto/","link":"","permalink":"https://ssarcandy.tw/2016/12/25/MSAuto/","excerpt":"最近 Facebook messanger 推出了一系列小遊戲，大部分其實都是無腦遊戲，但扯到互相比分總是會變得很激烈…. 其中有個遊戲很特別 EverWing，他是可以升級主角的射擊遊戲，也就是我可以偷練再去挑戰別人，然後分數就會很高~起初我偷練到七等左右去挑戰 David 老師，沒想到他一下就超越我了，後來又浪費我好幾個小時才超越他…。後來跑去挑戰 ball 他們，費盡千辛萬苦打到 3204 分，結果後來才發現他們都在直接發 request 作弊…。","text":"最近 Facebook messanger 推出了一系列小遊戲，大部分其實都是無腦遊戲，但扯到互相比分總是會變得很激烈…. 其中有個遊戲很特別 EverWing，他是可以升級主角的射擊遊戲，也就是我可以偷練再去挑戰別人，然後分數就會很高~起初我偷練到七等左右去挑戰 David 老師，沒想到他一下就超越我了，後來又浪費我好幾個小時才超越他…。後來跑去挑戰 ball 他們，費盡千辛萬苦打到 3204 分，結果後來才發現他們都在直接發 request 作弊…。 有圖有真相，沒作弊打 3204 分 既然他們都這樣玩，我也沒在客氣的，直接發個強一點的分數: 8萬分。 直接發 request 獲得八萬分 後來還發現 github 上竟然有自動練等的專案 neverwing ，做得如此完整，真的是讓我開眼界了哈哈。 以上是啟發我做個自動解 MSA 任務的程式的起因。 認識我的人應該都知道我一直有在玩一個手機遊戲 Metal Slug Attack (MSA)，基本上我會玩也只是因為這是小時候玩越南大戰機台的回憶。不過有鑒於這遊戲每日任務實在是有點麻煩又耗時，所以我就想弄成自動化。 首先還是要先知道這遊戲發 request 發到哪、怎麼發、順序是甚麼、怎麼認證使用者資訊。基本上這些想知道可能要架 Proxy + Postman + 手機 root 才行了。幸好有個好東西 Charles 可以輕易的架 Proxy 讓手機的 http request 都先通過電腦再到 remote server。 知道怎麼發之後，就來開始試試吧！MSA 這遊戲有很多模式，其中一個是 TREASURE HUNT，每隔幾個小時可以領一次獎品這樣(個人覺得很沒意義的模式…)，而我也是從這模式開始下手。 TREASURE HUNT 一般而言我頂多一天進一次遊戲，那像是這種 CD 3 小時的就很浪費(原本一天可以領 8 次的)，我寫的程式邏輯挺簡單的: 每十分鐘檢查一次，冷卻好了就領獎並且重新開始搜尋。 這用 nodejs request 加上 crontab 設定一下就可以達成~ 這樣我每天都可以多好多體力 XD 接下來挑戰下個模式: COMBAT SCHOOL ，這就是每天可以打電腦三次然後會給你獎品(也是滿沒意義的模式…) COMBAT SCHOOL 在做這模式的自動化時卡滿久的，有個 POST request 的 x-www-form-urlencoded data 長這樣: 123&quot;cover=2&amp;deck_no=3&amp;stage_id=706&amp;training_id=1&quot; +&quot;&amp;unit_ids[]=15&amp;unit_ids[]=16&amp;unit_ids[]=17&amp;unit_ids[]=18&amp;unit_ids[]=19&amp;unit_ids[]=21&amp;unit_ids[]=84&amp;unit_ids[]=82&amp;unit_ids[]=271&amp;unit_ids[]=340&quot; +&quot;&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&amp;unit_level[]=50&quot; 可以發現有一大堆 key 都是一樣的( unit_ids[] , unit_level[] )，這用 nodejs request 似乎辦不到，所以我最後決定寫 shell script 直接用 curl 發 request ，再用 cronjob 設定每天幫刷一下關。恩，就成功了。 再挑戰下一個模式: P.O.W RESCUE ，每天可以打 20 關電腦，會給你一些獎品(怎麼每個模式都大同小異XD)。這是我覺得最浪費時間的模式了，因為一天要打 20 次。 P.O.W RESCUE 這邏輯稍微複雜一點，因為每次挑戰的 stage_id 都不一樣，要先 GET 下一關 stage_id ，而且又有那種 nodejs request 不能發的 request。解法兩種: 全部用 shell script 寫。 用 nodejs 寫，要發特殊 request 就另外執行 shell script。 基於我其實不太會寫 shell script，我決定採用 2 的混和寫法。利用 nodejs 判斷下一關 stage_id 再傳給 shell script 發 request。在 nodejs 中要執行 shell 可以這樣: 123456const cp = require(&#x27;child_process&#x27;);const job = (str, option) =&gt; &#123; return cp.execSync(str, &#123; cwd: __dirname &#125;).toString();&#125;; job(&#x27;/bin/bash script.sh&#x27;); 這模式我並沒有設定 cronjob，因為我有時候想自己玩 XD。 而其他模式我現階段沒打算自動化，不然就根本都不是我在玩了… 寫這些東西其實也讓我學到一些新東西以及一些小技巧，這也算是玩 MSA 給我的收穫嗎？哈哈其實我 code 寫得亂七八糟，但管他的可以用就好~ 最後還是附上 code: MSAuto 是說這應該算是作弊喔，抓到會被 ban 吧?不過其實被 ban 也算是一種解脫吧?","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"},{"name":"automation","slug":"automation","permalink":"https://ssarcandy.tw/tags/automation/"}]},{"title":"安裝 Policyd 並設定外寄 Quota","slug":"policyd","date":"2016-12-23T16:15:40.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2016/12/23/policyd/","link":"","permalink":"https://ssarcandy.tw/2016/12/23/policyd/","excerpt":"最近遇到一個問題，有人利用 cmlab 的電子郵件發送垃圾郵件，導致許多外部郵件伺服器將我們加入黑名單。因此，我們認為有必要設定一個寄件上限來防止這種情況。雖然 Postfix 本身已經具有許多可設定的功能，包括防止垃圾郵件等，但它無法設定寄件的配額，所以我們打算嘗試使用 Policyd。 Policyd（又名 Cluebringer）是一個類似於中間件的軟體，它可以為郵件伺服器設定一些規則。這可以幫助我們更靈活地管理郵件流量，提高郵件伺服器的安全性和效率。通過使用 Policyd，我們可以制定更細緻的郵件策略，有效地對抗垃圾郵件和其他濫用行為。","text":"最近遇到一個問題，有人利用 cmlab 的電子郵件發送垃圾郵件，導致許多外部郵件伺服器將我們加入黑名單。因此，我們認為有必要設定一個寄件上限來防止這種情況。雖然 Postfix 本身已經具有許多可設定的功能，包括防止垃圾郵件等，但它無法設定寄件的配額，所以我們打算嘗試使用 Policyd。 Policyd（又名 Cluebringer）是一個類似於中間件的軟體，它可以為郵件伺服器設定一些規則。這可以幫助我們更靈活地管理郵件流量，提高郵件伺服器的安全性和效率。通過使用 Policyd，我們可以制定更細緻的郵件策略，有效地對抗垃圾郵件和其他濫用行為。 安裝在 cluebringer 2.0 以前的版本不支援 IPv6，所以基本上只能從官網下載最新版，又，官網安裝說明充滿錯誤，我在弄得時候十分不開心….，所以決定自己整理安裝流程。 下載並解壓縮12$ wget http://download.policyd.org/v2.0.14/cluebringer-v2.0.14.zip$ unzip cluebringer-v2.0.14.zip 初始化資料庫在 database/ 下，執行這段 shell script 1234for i in core.tsql access_control.tsql quotas.tsql amavis.tsql checkhelo.tsql checkspf.tsql greylisting.tsqldo ./convert-tsql mysql $idone &gt; policyd.sql 這邊產出的 .sql 會有語法錯誤，用 vim 開啟並下 :%s/TYPE=innondb/ENGINE=innondb/g 指令修改全部。 建立新資料庫並匯入 policyd.sql : 12$ mysql -u root -p -e &#x27;CREATE DATABASE policyd&#x27;$ mysql -u root -p policyd &lt; policyd.sql 複製檔案到該放的地方123$ cp -r cbp /usr/local/lib/cbpolicyd-2.1/$ cp cbpadmin /usr/local/bin/$ cp cbpolicyd /usr/local/sbin/ 啟動1$ /usr/bin/perl /usr/local/sbin/cbpolicyd --config /etc/cluebringer.conf 如果啟動時遇到類似: you may need to install the Mail::SPF module 等等 error，就安裝這個: sudo aptitude install libmail-spf-perl 要查看是否有啟動成功，可以下 ps aux | grep policyd 指令。要查看 port 10031 是否有在 listen，可以下 netstat -pln | grep :10031 指令檢查。 設定 Postfix 使用 Policyd去 Postfix config 檔設定 check_policy_service : 123456789101112smtpd_sender_restrictions = ..., check_policy_service inet:127.0.0.1:10031 smtpd_recipient_restrictions = ..., check_policy_service inet:127.0.0.1:10031, permit_mynetworks, permit smtpd_end_of_data_restrictions = check_policy_service inet:127.0.0.1:10031 在 smtpd_recipient_restrictions 中，check_policy_service 需要在 permit_mynetworks 上面才有用。若要設定外寄 Quota 的話則 smtpd_sender_restrictions 也要加上 check_policy_service。 另外提醒註解不要亂放，會讓設定檔整個壞掉….可以透過 postconf 指令來列出真正 postfix 吃到的設定值 設定 Policyd Web UIpolicyd 有提供一個 web 的設定介面，讓我們比較方便設定 policyd。 複製解壓縮檔裡的 webui/ 到 web server 12$ cp -r webui /var/www/$ vim /var/www/webui/include/config # 填上該填的資訊 需要把擁有者改成 www-data 12$ chown -R webui$ chgrp -R webui 就可以直接連上 web 介面: http://your.domian/webui/ 這 web 介面預設不用登入，大家都可以隨意更改，所以必須利用其他方式加個密碼保護。 這邊是用 lighthttpd 設定密碼 123$ cd /etc/lighthttpd$ vim pwd # user:password$ vim lighthttpd.conf 詳細可以參考: Lighttpd setup a password protected directory (directories) 設定 Rate Limit到這邊就簡單了，藉由 web 介面按按按鈕就可以設定各種 Quota，詳細可參考這篇圖文教學:How To Configure Rate Limit Sending Message on PolicyD 驗證想確定是不是有成功，可以去 mySQL &gt; policyd &gt; quota_tracking 查看是不是真的有在追蹤大家的流量。 或者也可以看 mail.log ，會有流量的 log 資訊，包含還剩多少用量等等資訊。 123$ tail /var/log/mail.log | grep cbpolicydJan 20 00:02:05 cml2 cbpolicyd[32562]: module=Quotas, mode=update, reason=quota_update,policy=6, quota=3, limit=4, track=Sender:xxx, counter=MessageCount, quota=1.00/200 (0.5%) References Policyd-Installing Postfix + Centos + Policyd V2 + MySQL Policyd(Cluebringer) installation How To Configure Rate Limit Sending Message on PolicyD Lighttpd setup a password protected directory (directories) h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"}]},{"title":"pbrt - 用多點光源模擬環境光","slug":"pbrt-mediancut-environment-light","date":"2016-12-17T17:45:08.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2016/12/17/pbrt-mediancut-environment-light/","link":"","permalink":"https://ssarcandy.tw/2016/12/17/pbrt-mediancut-environment-light/","excerpt":"環境中總有一些背景光，像是太陽光、遠處大樓窗戶反光之類的，可以看成一整片不均勻分布的光源，有些地方亮；有些地方暗。pbrt 中是用 important sampling 來渲染環境光，不過，其實也可以把環境光轉換成一堆點光源來計算。","text":"環境中總有一些背景光，像是太陽光、遠處大樓窗戶反光之類的，可以看成一整片不均勻分布的光源，有些地方亮；有些地方暗。pbrt 中是用 important sampling 來渲染環境光，不過，其實也可以把環境光轉換成一堆點光源來計算。 不同的環境光會讓場景中物件有不同的渲染結果[1] Median Cut Alogrithm直接來看一張圖: 用 median-cut algorithm 把環境光轉成多點光源[2] 事實上這方法很簡單，就是將整張 environment light probe 切成 x 塊相同能量的區塊(x 是多少開心就好~)，整個邏輯可以分成幾步驟: 初始狀態 &#x3D; 整張 env light probe 影像（陣列中只有一個區域） 對於每個區域，沿著長邊切成兩塊一樣能量的區塊 如果區塊少於 x ，則繼續做 2. 把點光源放在每個區域中的能量重心處 計算能量重心能量重心計算其實就是 x, y 軸分別加權平均（權重為點能量）點能量計算方式可以根據 RGB channel 加權平均來計算eg. Y &#x3D; 0.2125R + 0.7154G + 0.0721B 之類的加權法 計算區塊能量直覺的想法其實就是這區塊中所有點能量和就是這區塊能量了。不過在做 Median-cut 時期時會不斷需要計算區塊的能量，用這種暴力解會使效能突破天際的差。這邊可以利用預先計算 environment light probe 的能量 Sum Area Table(簡稱 SAT)，概念很簡單:一開始先維護一個能量累積的二維陣列，每個點的值就是其左上角區塊的能量和，藉由這個陣列可以使獲得任意區塊能量的時間複雜度為 O(1)。 這張圖說明了若要獲得紅色區塊能量和，只需查表四次，計算 D-B-C+A 即可得出。 結果渲染出來的結果其實跟原本的環境光還是有些差別，畢竟是用多點光源來模擬環境光，本來就不會一模一樣。 Median cut algorithm Important sampling(original) 不過在效能方面，用多點光源模擬所需要的運算時間大概都只有原本算法的 30% ，可以說是大幅加速了! 雜談 這次中間卡了一周跑去 SIGGRAPH ASIA，由於我懶沒帶電腦去，所以也就稍微趕了一點…(幸好這次比較簡單一點點…哈哈) 我學乖了，用 cmlab 工作站跑 pbrt 整個就是舒爽，Enter 按下去差不多就算完了~ GA 累積人次快突破 5000 瀏覽了，不過發現還真的有人在看我的文章還是覺得很特別~哈哈 之前 group meeting 座前面學長一直在看我的文章 Realistic camera in pbrt，害我注意力都被吸走了 XD References 原圖來自 Physically Based Rendering, Second Edition 原圖來自 A Median Cut Algorithm for Light Probe Sampling, SIGGRAPH 2006 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"rendering","slug":"rendering","permalink":"https://ssarcandy.tw/tags/rendering/"},{"name":"pbrt","slug":"pbrt","permalink":"https://ssarcandy.tw/tags/pbrt/"}]},{"title":"用 Facebook 聊天機器人當通知系統","slug":"cml-fb-bot","date":"2016-11-17T11:57:17.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2016/11/17/cml-fb-bot/","link":"","permalink":"https://ssarcandy.tw/2016/11/17/cml-fb-bot/","excerpt":"CMLab 有二十幾台 unix work stations 供大家使用(據說 CMLab 的工作站比資工系工作站還要好…)。雖然這些機器設備都很強悍，但還是有時候會出現某台機器掛掉，或是 CPU, Memory, Swap 用量過高之類的事件。若有這類事發生就必須去處理，不然一直讓機器維持在高負載很容易就死掉了。實驗室有個 網站 可以查看機器及時狀態，什麼機器有什麼問題一看就知道，超方便的。 但是不上去看就不會知道有沒有狀況，所以我就決定做個有狀況發生就通知我的 Facebook chat bot。","text":"CMLab 有二十幾台 unix work stations 供大家使用(據說 CMLab 的工作站比資工系工作站還要好…)。雖然這些機器設備都很強悍，但還是有時候會出現某台機器掛掉，或是 CPU, Memory, Swap 用量過高之類的事件。若有這類事發生就必須去處理，不然一直讓機器維持在高負載很容易就死掉了。實驗室有個 網站 可以查看機器及時狀態，什麼機器有什麼問題一看就知道，超方便的。 但是不上去看就不會知道有沒有狀況，所以我就決定做個有狀況發生就通知我的 Facebook chat bot。 爬網頁最簡單的方式莫過於把網站的內容爬下來，如果有高負載或死機就通知我。我用 nodejs request 來完成這件事。 1234const request = require(&#x27;request&#x27;);request(&#x27;https://www.cmlab.csie.ntu.edu.tw/status/&#x27;, function (error, response, body) &#123; console.log(body) // Show the HTML&#125;); facebook-chat-apifacebook-chat-api 可以很容易的操作聊天室相關的行為，我利用這套件來達成通知自己。將爬下來的網頁內容，找出掛掉的機器，配合 facebook-chat-api 就可以用 fb 通知有機器掛掉了。 123456789101112131415161718192021222324// require modules let deadMachines = [];request(&#x27;http://www.cmlab.csie.ntu.edu.tw/status/&#x27;, function (err, res, body) &#123; const $ = cheerio.load(body); const dead = $(&#x27;.dead&#x27;); for (let i = 0; i &lt; dead.length; i++) &#123; const machineId = dead.eq(i).text(); deadMachines.push(machineId); &#125; // all machines are fine~ if (!deadMachines.length) return; // login to facebook and send msg to unix manager loginFacebook(config.account, function (err, api) &#123; if (err) return console.error(err); api.sendMessage(`$&#123;deadMachines.toString()&#125; is dead.`, /* channel_id */, function (err) &#123; console.log(`$&#123;deadMachines.toString()&#125; is dead.`); &#125;); &#125;);&#125;); 透過 Facebook 聊天室告知我有機器掛掉了。 定時檢查這隻程式並不是個持續監控的程式，只能算是個 script 而已。所以我用 crontab 設定排程定時檢查機器是不是有狀況。利用 unix 指令 crontab -e 編輯排程。 12# 每小時執行程式0 * * * * node index.js crontab 是吃 localtime，系統現在是甚麼時間可以藉由 timedatectl 查看: 12345678$ timedatectl Local time: Thu 2016-11-17 22:15:13 CST Universal time: Thu 2016-11-17 14:15:13 UTC RTC time: n/a Time zone: Asia/Taipei (CST, +0800) Network time on: yesNTP synchronized: yes RTC in local TZ: no 另外，如果設定了新的 Time Zone，則必須重啟 crontab service 才會讓他吃到正確的時間。 1$ /etc/init.d/cron restart 截圖直到現在其實已經達成目標了，但是只講句「某某機器掛了。」似乎還是差強人意，為了防範未然，應該要把高負載的機器也告知一下，提早處理以免機器死掉。想想感覺把整個網站截圖下來傳給我不是最快嗎。恩，那就這樣弄吧！ 多虧廣大 nodejs 套件開發者，這件事可以很容易地利用 webshot 達成。 截下整個表格傳給我。 Callback to Promise爬網頁、截圖、傳訊息都是非同步的動作，寫起來就變成 Callback Hell 了。真的是醜爆了… 1234567891011// Scrap webrequest(&#x27;url&#x27;, function (err, res, body) &#123; /* do something with &#x27;body&#x27; */ // Screenshot webshot(body, &#x27;tmp.png&#x27;, options, function (err) &#123; // login to facebook and send msg loginFacebook(config.account, function (err, api) &#123; // send message &#125;); &#125;);&#125;); 利用 bluebird，可以把 Callback 神奇的轉為 Promise，並且用 es6 的語法改寫，就可以大幅改善程式碼的易讀性～ 1234567891011request(&#x27;url&#x27;) .then(body =&gt; &#123; /* do something with &#x27;body&#x27; */ // Screenshot return webshot(body, &#x27;tmp.png&#x27;, options); &#125;) .then(() =&gt; loginFacebook(config.account)) // login to facebook and send msg .then(api =&gt; &#123; // send message &#125;) .catch(err =&gt; console.error(err)); 完整的程式碼: https://github.com/SSARCandy/cml-status-fb-notify","categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"automation","slug":"automation","permalink":"https://ssarcandy.tw/tags/automation/"},{"name":"nodejs","slug":"nodejs","permalink":"https://ssarcandy.tw/tags/nodejs/"}]},{"title":"Realistic camera in pbrt","slug":"pbrt-realistic-camera","date":"2016-11-09T05:14:41.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2016/11/09/pbrt-realistic-camera/","link":"","permalink":"https://ssarcandy.tw/2016/11/09/pbrt-realistic-camera/","excerpt":"Ray-tracing 中的相機(眼睛)是所有光束的起點，從相機成像平面出發的光束如果能夠經由折射、反射等等最終到達光源的那些「存活」的光束，才對最終的影像有影響的光束。這種與現實物理相反的設計(從光源發出光並追蹤那些存活到相機成像平面的光束)是為了減少計算量。 ray-tracing 中，光束是從相機射出來的。[1]","text":"Ray-tracing 中的相機(眼睛)是所有光束的起點，從相機成像平面出發的光束如果能夠經由折射、反射等等最終到達光源的那些「存活」的光束，才對最終的影像有影響的光束。這種與現實物理相反的設計(從光源發出光並追蹤那些存活到相機成像平面的光束)是為了減少計算量。 ray-tracing 中，光束是從相機射出來的。[1] 實作真實相機光學系統相機的光學系統決定了最終成像的樣貌，比如廣角、魚眼、正交投影等等樣貌，pbrt-v2 中實做了最常見的幾個，包含了 perspective camera, orthographic camera，原始程式碼在 src/camera 之下。 實際上的相機的光學系統通常都包含了多個透鏡，以此來達成比較複雜的成像(或是減少透鏡的像差)。 真實相機通常都是由多片透鏡組成的光學系統。[2] 上圖是描述光學系統各個透鏡面的相關參數，從成像平面出發的 eye-ray 會在這個光學系統折射多次之後才會進入場景中，而這些光束又會跟場景的物件相交、反射等等，就如同我之前寫的這篇 改善 pbrt 中的 heightfield shape，在做一樣的事。 所以，要模擬真實相機的光學系統，其實就是幾個步驟： 在成像平面上與第一個透鏡面上各取一點，相連產生 eye-ray 找出 eye-ray 與第一個透鏡面的交點，沒交點就結束 找出折射後的新 eye-ray 重覆 2, 3 直到 eye-ray 離開光學系統進入場景為止 以 pseudo-code 表示，大概可以寫成這樣： 123456789101112131415filmP = random point on filmlensP = random point on first lensray.o = filmPray.d = Vector(lensP - originP) for l in lens: # from rear to front intersectionP = l.intersect(ray) if not intersectionP: return 0 newDirection = l.refractRay(ray) if not newDirection: return 0 ray.d = newDirection 其實也可以借助一些除錯工具來視覺化光束的折射行為，這邊我使用 vdb 來畫出整個光學系統並且追蹤光束的折射行為。(關於如何利用 vdb 除錯，可以參考 vdb - Debugging visual programs) pbrt 模擬從成像平面中點發出光束，經過多次折射直至離開相機鏡頭。 除此之外，也需要計算每條 ray 的權重，根據論文[2]所說是如下公式： : 出射瞳面積: 最後透鏡與成像平面的距離: 光束與成像平面法向量夾角 結果我嘗試渲染大張一點的圖並且讓每個像素的採樣夠多次，希望能夠讓結果圖漂亮一點。代價就是一張圖要跑好幾個小時…… dobule-gauss 50mm with 512 samples per pixel(1024*1024) wide 22mm with 512 samples per pixel(1024*1024) telephoto 250mm with 512 samples per pixel(1024*1024) fisheye 10mm with 512 samples per pixel(1024*1024) 加速每個透鏡面基本上是以部份的球面來模擬，要求交點很容易，因為 src/shape/shpere.cpp 已經實做求交點了，可以直接創一個 Sphere object ，然後再 s.Intersect(r, &amp;thit, &amp;rayEpsilon, &amp;dg) 來取得交點。 這樣寫簡單易懂，三兩下解決求交點這件事，但其實效能並不好，因為只為了求一個交點卻建構了一個完整的 shpere shape ，稍微有點浪費…… 改善方式可以藉由複製貼上 Shpere::Intersect() 並加以改寫，就可以省去這樣的 overhead實際上效能大約差 2 倍。 各種坑 RasterToCamera在 GenerateRay() 中，sample.imageX sample.imageY 是在 Raster space 上，需轉至 Camera space，Raster/Camera space 之間關係如圖(省略 z 軸)，可以看出來轉換方式大概就是：上下翻轉、平移、縮小。 Raster/Camera space 之間關係。 Ray minT, maxT在 GenerateRay() 中產生的 ray 記得參數要給好給滿，之前因為沒給值到 min/maxT 導致結果有坑坑巴巴的破圖… Total reflection我使用 Snell’s law 計算折射後的新方向，在公式中，若發生全反射時會讓其中一項產生虛數，程式就整個爆炸了。所以記得要注意別把負數開根號了。 References 圖片取自維基百科 Ray-tracing A Realistic Camera Model for Computer Graphics, SIGGRAPH 1995 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"rendering","slug":"rendering","permalink":"https://ssarcandy.tw/tags/rendering/"},{"name":"pbrt","slug":"pbrt","permalink":"https://ssarcandy.tw/tags/pbrt/"}]},{"title":"vdb - Debugging visual programs","slug":"debug-using-vdb","date":"2016-10-13T12:34:53.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2016/10/13/debug-using-vdb/","link":"","permalink":"https://ssarcandy.tw/2016/10/13/debug-using-vdb/","excerpt":"有時候在寫 openGL 或者是類似 pbrt 這樣有牽涉到三維空間的程式的時候總是很難除錯…雖然可以設斷點看看變數內容有沒有問題，但說實在的其實這樣看有時候根本看不出所以然，這樣還是難以除錯。 vdb 是一個解決這樣問題的工具，它提供了很多常見的畫線、畫點等等函式，最重要的是他的易用性，可以在幾乎不更動程式碼的狀態下就完成偵錯。","text":"有時候在寫 openGL 或者是類似 pbrt 這樣有牽涉到三維空間的程式的時候總是很難除錯…雖然可以設斷點看看變數內容有沒有問題，但說實在的其實這樣看有時候根本看不出所以然，這樣還是難以除錯。 vdb 是一個解決這樣問題的工具，它提供了很多常見的畫線、畫點等等函式，最重要的是他的易用性，可以在幾乎不更動程式碼的狀態下就完成偵錯。 Usage其實在 vdb README 已經說的滿清楚的了，這邊在整理一下: 下載 vdb-win.zip 或 vdb-osx.tar.gz 把 vdb.h include 進你要偵錯的程式 執行 vdb.exe 或 ./vdb 在要偵錯的程式中，可以在任何地方插入 vdb_line() 等他所提供的函式 Example以 pbrt 為例，由於結果都是一張渲染完成的影像，說實在的這真的很難 debug….那利用 vdb 這工具能有甚麼好處呢？ 藉由他，可以很容易地畫出像是物體的樣子以及他的 Bounding Box，如圖： heightfield object and its bounding box 或者是當你在算 Vertex normal 時，不知道到底算的對不對，也可以直接畫出來： Showing Normals on vertices 我個人覺得 vdb 對我最大的幫助是 object space 以及 world space 之間的關係了，坐標系一個沒弄好就會讓渲染的結果差異甚大，完全摸不著頭緒到底發生什麼事情….. 藉由 vdb 實際畫出各個 object 時候，很容易就發現 object&#x2F;world space 之間的 bug Bounding box 沒有正確的轉換到 object space，導致跑到怪怪的地方 Some tips其實上面說的很簡單，但事實上我剛開始嘗試使用 vdb 時遇到了超多的問題，這邊就來記錄一下： Header files order#include &quot;vdb.h&quot; 要擺在哪裡？一般來說當然越上面越好，但還是有例外的： heightfield.cpp12345#include &quot;stdafx.h&quot;#include &quot;vdb.h&quot;#include &quot;shapes/heightfieldImproved.h&quot;#include &quot;shapes/trianglemesh.h&quot;#include &quot;paramset.h&quot; 以上面的例子來說，#include &quot;vdb.h&quot;必須放在 #include &quot;stdafx.h&quot;後面。stdafx.h是來做 precompiled headers 用的，所以置於他之前的 include 都會被忽略，就會造成找不到 vdb_line() 之類的錯誤。 Use ONE threadvdb 不支援多執行緒，如果在偵錯的程式是多執行緒的話，有可能會導致畫在 vdb 上的東西不正確(漏畫)。解決方法就是在偵錯時使用單一執行緒。 Using winsock instead of winsock2這問題應該是只在 Windows 會發生，並且在要偵錯的程式中使用到 windows.h。vdb 中用到了 winsock2.h，這是跟 socket 有關的東西，然而 windows.h 中也有 include 一個叫 winsock.h 的東西。winsock.h, winsock2.h 有很多函式都各自有宣告，而同時引用兩者會造成 redefinition 錯誤。 解決方法是把 vdb.h 中，winsock2.h改成winsock.h。詳細可查看 stackoverflow 這篇","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"pbrt","slug":"pbrt","permalink":"https://ssarcandy.tw/tags/pbrt/"}]},{"title":"改善 pbrt 中的 heightfield shape","slug":"pbrt-heightfield","date":"2016-10-10T13:14:41.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2016/10/10/pbrt-heightfield/","link":"","permalink":"https://ssarcandy.tw/2016/10/10/pbrt-heightfield/","excerpt":"pbrt 是一個基於物理的 ray-tracing libarary，他可以拿來產生接近現實的真實場景，據說 IKEA 的型錄都是用類似方法產生的，而不是真的把產品擺出來拍照。 哈哈 據說 IKEA 型錄的圖都是渲染出來的[1]","text":"pbrt 是一個基於物理的 ray-tracing libarary，他可以拿來產生接近現實的真實場景，據說 IKEA 的型錄都是用類似方法產生的，而不是真的把產品擺出來拍照。 哈哈 據說 IKEA 型錄的圖都是渲染出來的[1] 關於 pbrt 與 ray-tracingRay-tracing 說穿了就是在模擬自然界光線的運作，我們之所以看的到東西，其實就是因為光線打到物體並反射到我們眼睛，這也是為甚麼在無光的地方會伸手不見五指(因為沒有任何光打到手指並反射到眼中)。至於電腦要怎麼模擬這件事，大致來說是光線從光源出發，途中遇到障礙物就要算交點，有交點就要根據材質特性反射，反射之後就是一條新的光線，就繼續做交點測試直到進到眼睛中(或直到能量遞減完畢)。可以看出，ray-tracing 最重要的大概就是與物件算交點了，因為 ray 會一直做交點測試，所以與物件的交點測試必須要夠快才行，不然就會算到天荒地老…. pbrt 除了最基本的 triangleMesh 以外，還實作很多其他一些常見的 shape(球體、圓柱體、圓形、heightfield…)，這其實是拿來給 api 使用的，一般人可以寫 pbrt 專用的描述檔來描述一個場景中的物體、光源等等，再藉由 pbrt 的程式來渲染出整張影像。 pbrt 中的各種 shape，有些是會先轉成 triangleMesh(對三角形求交點應該是圖學中最基礎的了，如果原本物件太複雜通常就會先把它拆成三角形組合再來算)；而有些是有實作對 shape 交點測試的。由於把物件轉成 triangleMesh 其實就硬是多一個步驟了啊，如果可以與 shape 直接求交點，那速度上當然會大躍進~ 實作 heightfield 交點測試Heightfield 其實就是平面但是有高低差，也就是說，對每個 而言只會有一個 值。算是個滿單純的 shape。Heightfield 也是原本就有實作的一種 shape，是直接用 Refine() 來把形狀轉為 triangleMesh 再做交點測試的。 如果能夠跳過三角化而直接與 heightfield 做交點測試，可能可以比較快喔？參考一下別的 shape，包含球體、圓柱體等實作的交點測試的方法都是在幾何意義上直接求交點，也就是算數學求解哈哈但 heightfield 似乎是沒辦法從幾何意義上直接解了，需要用更暴力的方法 這邊我是使用 DDA(digital differential analyzer) 來做交點測試，這東西其實原本是拿來畫線的演算法，因為實際上的線是連續的，但是呈現在電腦上卻必須以 pixel 為單位呈現。而這邊與 heightfield 的交點測試就是將 DDA 擴展至三維空間中(多了Z軸)。 2D-DDA 邏輯。[2] 可以看到其實可以在一開始就算出x, y要走多少會到下一個 pixel，這些都是定值，也讓遍歷整個 Pixel-Grid 變得很容易，而 3D-DDA 就只是再加入 z 軸的資訊，並且每一個 pixel 變成 voxel。3D-DDA 這樣的方式其實在 pbrt 裡面已有實作，是來作為加速結構用途，但是由於 heightfield 本身特性(對每個 而言只會有一個 值)，我們可以讓 Voxel 的高度等於 heightfield 的高度，如此一來就可以讓3D結構的 heightfield 套用 2D-DDA 了！耶~~~ 建好 DDA 需要的資訊後，接下來就是要實作 Ray 交點測試了，在遍歷 Voxel 的過程中，需要針對這個 Voxel 做交點測試，如果有交點就結束了；沒有就到下個 Voxel。而關於每個 Voxel 的交點測試其實也是滿單純的，在設計 DDA 的結構時，除了讓 Voxel 高等於 heightfield 高，可以變成 2D-DDA 以外，讓 Voxel 的寬等於一個單位的 x 及 y 也是有很大的好處的，如下圖: 從正上方看下來的 heightfield 樣子，數值為對應 z 值。讓 Voxel 寬度等於一格寬有好處。 依據這樣的設計，每次在做 Voxel 交點測試時，可以知道這 Voxel 中就是包含兩個三角形；也就是說分別對這兩個三角形做交點測試就好了~~ 根據這樣的算法，就可以得出與原本直接三角化的做法一模一樣的結果: 用直接求交點的方式取代原本先做三角化的方法。 平滑化看看上圖的結果，看得出都是一面一面的三角形面，這是因為同一個面上所有點都是一樣的法向量，所以反射角度也都一樣，自然就成這副德性。如果要做平滑化的話就必須內插三角形內部的點的法向量，使得三角面反射光會看起來滑順一點。 三角化後每個點 M 都有六個鄰居。 這邊我是直接平均法向量來達成這樣的效果，由於三角化之後每個點會有六個鄰居；點 M 的鄰居有 TL、T、R、BR、B、L 六點，點 M 的法向量可以藉由任意兩向量外積得出。 那我就平均一下六個法向量來當作真正的法向量，以 M 為原點，可算出平均法向量為: 這樣子改進後，就可以讓結果變這樣: 平滑化的結果 浮點數精度問題(10/22 更新)做完平滑化之後，感覺海好像怪怪的歐….一開始其實我還沒察覺，過這麼久才發現這問題… 很顯然只有海有這樣的問題，八成是因為海的 值差距太小，計算法向量時的誤差。用這樣的思維去追查程式後，發現我原本在算六個法向量總和後有做 Normalize(sumOfNormals)，這步驟造成 值起伏太小的海的計算誤差….把 Normalize() 拔掉之後就正常了～ 修正浮點數精度問題後的結果 加速很可惜的是我做完以後，速度沒有想像中的快速，反而比原本的還慢了兩倍以上….嗚嗚嗚….稍微看看別的 shape 的交點測試，其實有很多時候會先跟 Bbox(Bounding Box) 做測試，因為與 Bbox 交點是容易很多的，如果與 Bbox 無交點就也不用繼續做下去了。在 Voxel 交點測試中，也應當先與整個 Voxel 做測試，確定有交點再去試裡面的兩個三角形，這樣就可以省下很大量的做白工。 但這樣做完還是不夠好，所以我利用 CPU profiling 來測試我的程式的瓶頸到底在哪裡….利用這樣的檢測，我陸續做了幾次優化: 原因及改善方法 Intersect() 與未最佳化時比較 未最佳化。 90,274 100.0% 發現 ObjectBound() 很慢，改在 Heightfield construction 時就先存 minZ, maxZ。 82,731 91.6% 發現在算 Voxel BBox 使用的 Bbox(Union(Bbox, Bbox)) 超爆慢，改用 Bbox(Point, Point)。 53,408 59.1% 發現有不必要的坐標系轉換，由於在 Voxel 交點測試中會先測與 Voxel Bbox 交點，所以最好直接給他已轉好的 Ray(Object Space)。 46,922 51.9% 由於 Bbox(Point, Point) 建構時都要重新判斷 min max，改用先建構空的 Bbox 再直接給值(pmin, pmax)省去建構時間。 36,768 40.7% 把用不到的 Bbox 給 Voxel 交點測試中再利用，節省 Construction Time(避免重新 relocate 記憶體位置以及建構空 Bbox 的時間)。 32,742 36.2% Intersect() 欄的數字是指 CPU 採樣時落在這函式的總樣本數，越多表示執行時間越長 測試的是 landsea-2.pbrt，並且使用 –ncores 1 以減少多執行緒的誤差 經過幾次最佳化後，成功壓低執行時間(單位為秒)，效能比較如下: 效能比較。[3] 雜談說真的這大概是我做過數一數二難的作業了，而且竟然只是作業一….其實寫的過程也不是我自己想到的，老師也有給提示，甚至網路上其實根本有答案….(不見得是最佳解就是了)為了這作業我甚至還上 Stack overflow 問了人生第一個問題 哈哈(雖然問題跟演算法沒關係)只能說不愧是 Stanford 的題目囉？ 註: [1] 可以看這篇介紹 IKEA 渲染型錄 [2] 原圖來自 Physically Based Rendering, Second Edition [3] 執行時間用 bash 內建 time 指令來量測","categories":[],"tags":[{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"rendering","slug":"rendering","permalink":"https://ssarcandy.tw/tags/rendering/"},{"name":"pbrt","slug":"pbrt","permalink":"https://ssarcandy.tw/tags/pbrt/"}]},{"title":"一些 Python 筆記","slug":"python-note","date":"2016-09-09T16:28:57.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2016/09/09/python-note/","link":"","permalink":"https://ssarcandy.tw/2016/09/09/python-note/","excerpt":"最近的專案需要用到 OpenCV，官方有提供 C++ 以及 Python 的版本。我以前都用 C++，這次想說來換換口味使用 Python 好了，如果用的順手以後就都這樣搭配著用(Python + OpenCV)。說是這樣說，但其實我對 Python 根本一竅不通，從來沒在比較大的專案中使用過，所以新手如我自然就遇到很多坑(?)","text":"最近的專案需要用到 OpenCV，官方有提供 C++ 以及 Python 的版本。我以前都用 C++，這次想說來換換口味使用 Python 好了，如果用的順手以後就都這樣搭配著用(Python + OpenCV)。說是這樣說，但其實我對 Python 根本一竅不通，從來沒在比較大的專案中使用過，所以新手如我自然就遇到很多坑(?) 我這次專案是 Python 2.7 + OpenCV 3.1。安裝 OpenCV 一直都是很麻煩的事情，C++ 的免不了要自己 build，詳細的方法在我之前寫的另一篇有教學；而 Python 安裝 OpenCV 稍微簡單一點，把 cv2.pyd 放到 C:\\Python27\\Lib\\site-packages 就可以了 [1]。在 Mac&#x2F;Linux 上更簡單，可以使用 conda [2] 來幫你安裝。 好，接下來就是用到現在所做的一些筆記～ Coding-stylePython 的 Coding-style 基本上就是參考 PEP8，簡結一下重點: 縮排用空格 一行不要超過 80 個字元，超過就換行。 除了 ClassName 以外，一律用 snake_case 為命名規則。 文件首行要加上編碼，一般 utf-8 就是加上這行 # -*- coding: utf-8 -*- DivisionPython2 的除法很特別，竟然只除到整數，意思就是 5 / 2 = 2，不過這其實就跟 C++ 一樣嘛，強制轉型一下就好: 123print(5 / 2) # 會無條件捨去小數點，印出 2print(5 / 2.0) # 2.5print(5 / float(2)) # 2.5 不過這樣好麻煩，有沒有一勞永逸的方法？有！就是用神奇的 __future__ 12# 在文件開頭處加上這行from __future__ import division future 是 Python2 很特別的東西，感覺上就是抓 Python3 的功能來用，而這邊 import division 就是使除法的行為與 Python3 一致，詳細運作原理可以參考 stackoverflow 上的說法: The internal difference is that without that import, / is mapped to the __div__() method, while with it, __truediv__() is used. Class網路上有許多宣告 Class 的教學，不過好像大部分的都不對或過時了。在 Python2 中，Class 宣告方式如下： class.py123456789101112131415class Foo(object): def __init__(self): self.bar1 = &#x27;hello&#x27; self.bar2 = &#x27;world&#x27; def hello_world(self): print(self.bar1 + &#x27; &#x27; + self.bar2) def __private_func(self): print(&#x27;I am private function&#x27;) foo = Foo() # new 一個新的 Foofoo.hello_world() # 不用帶任何參數foo.__private_func() # 無法呼叫，會噴 Error: AttributeError: &#x27;Foo&#x27; object has no attribute &#x27;__private_func&#x27; 第一行的 object 是必須的。 __init__ 這個 function 也是必須的，這是 Class Constructor。 每個 function 的第一個參數必須放 self，這與 C++ Class 中的 this 相似，基本上就是拿來存取自己用的。 例如 hello_world() 中就有存取 bar1 跟 bar2，而在呼叫時 self 會被跳過。 如果要寫 private method [3]，就在 function name 前加上雙底線 __，如同上面的第 9 行處。 Path檔案路徑也是一個滿麻煩的事情，假設要讀一個檔案，一開始可能會這樣寫： 1cv2.imread(&#x27;../data/foo.jpg&#x27;) # cv2.imread 是 opencv 的讀圖函式 這樣當然可以讀的到，但當你換了一個工作環境，像是 Windows，這樣寫就炸了～(因為 Window 路徑是用反斜線 \\)Python 提供了個好工具 os.path.join()，簡單來說就是幫忙處理斜線。所以上面那個例子可以改寫成這樣： 12import oscv2.imread(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;, &#x27;foo.jpg&#x27;)) 這樣的寫法就可以順利地在各平台運作～！ Function name and Variable name每個程式語言都有保留字，像是 for, while, if 之類的都是常見的保留字，而 Python 也不例外，你可以在這邊看到全部的保留字。而通常 function name 也跟保留字一樣不能當作變數名稱 [4]。特別的是 Python 允許變數名稱與 function 名稱一樣，像是： 123sum = sum([1,2,3,4,5]) # sum() is built-in function for pythonprint(sum) # 15print(sum([1,2,3,4,5])) # 15 上面的例子可以看到，Python 能夠清楚的分辨這個 sum 是指變數還是內建函式。javascript 就不行： 123btoa = btoa(&#x27;hello&#x27;) // btoa() is built-in function for javascriptconsole.log(btoa) // aGVsbG8=console.log(btoa(&#x27;hello&#x27;)) // Uncaught TypeError: btoa is not a function 可以發現在第三行就噴錯了，因為 btoa 在第一行已經被蓋掉了，後面要用 btoa() 就會以為你是指那個變數，而變數當然不是 function 囉。 The Zen of PythonPython 作者 Tim Peters 把一首詩藏在 import this 中， Beautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren’t special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one– and preferably only one –obvious way to do it.Although that way may not be obvious at first unless you’re Dutch.Now is better than never.Although never is often better than right now.If the implementation is hard to explain, it’s a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea – let’s do more of those! 寫 Python 就是要追求乾淨、易讀、簡單，這也是我這幾周使用 Python 所感覺到的。再引用 David 老師所言， 寫 Python 就像是在寫 pseudo-code 一樣爽！ References cv.pyd 可以在 build 好的 opencv 資料夾中找到。 OpenCV 不能用 pip 安裝，而 conda 是類似 pip 的 Python 套件管理軟體。 Python 是沒有 private function 的，只是在 runtime 藉由更改 function name 來達到這樣的效果，詳細可以參考這篇。 函式名子不一定不能當作變數名稱，在 C&#x2F;C++ 中會有 Compile-time Error，在 javascript 中是可以的，但是會覆蓋其內容。 h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"},{"name":"opencv","slug":"opencv","permalink":"https://ssarcandy.tw/tags/opencv/"},{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"}]},{"title":"談談 vim plugin-manager","slug":"vim-plugin-manager","date":"2016-08-17T03:02:10.000Z","updated":"2025-10-22T00:51:26.606Z","comments":true,"path":"2016/08/17/vim-plugin-manager/","link":"","permalink":"https://ssarcandy.tw/2016/08/17/vim-plugin-manager/","excerpt":"我用過了幾乎所有有名的 vim plugin-manager，包含 Pathogen, Vundle 以及比較新的 vim-plug。而以時間序來說也是 Pathogen -&gt; Vundle -&gt; vim-plug 先來談談用過這三個分別的感想好了：","text":"我用過了幾乎所有有名的 vim plugin-manager，包含 Pathogen, Vundle 以及比較新的 vim-plug。而以時間序來說也是 Pathogen -&gt; Vundle -&gt; vim-plug 先來談談用過這三個分別的感想好了： Pathogen簡單好用，與其說是 plugin-manager，個人覺得比較像是個純粹的 run-time loader，沒有什麼其他的功能。但已十分好用，要新增什麼 plugin，只需把 plugin 的資料夾放在 bundle/ 底下就完工了！刪除也是，直接砍掉 bundle/ 底下對應資料夾就OK Vundle目前應該是這三者中 github stars 最多的。plugin 安裝方式是在 .vimrc 中寫你要的 plugin name 1Plugin &#x27;tpope/vim-fugitive&#x27; 然後在 vim 中打 :PluginInstall 就會幫你安裝。 這樣的好處是你裝過什麼一目了然，而且到新環境要重新設置的時候也很方便，直接:PluginInstall就完成了。(如果用 Pathogen 就必須自己把要用的 plugins clone 下來。) vim-plug我目前在使用的 plugin-manager ，給我的感覺就是 Vundle 的加強版。新增 plugin 的方式跟 Vundle 很像(只是關鍵字不同)，都是在 .vimrc 中寫你要的 plugin name 1Plug &#x27;tpope/vim-fugitive&#x27; 安裝也是跟 Vundle 差不多，關鍵字不一樣而已 (:PlugInstall) 比較厲害的是 vim-plug 可以 on-demand loading！像是 vim-go(強大的 Golang Dev-plugin)，這種只有在寫 golang 時候才要的 plugin，就應該只在副檔名是.go的時候載入就好；NERDTree 也是，有時候只是打開一個檔案要編輯而已，用不著這個套件，只有當真的觸發開啟 NERDTree 的時候再載入就好。這些 vim-plug 都可以設定(設定方式詳見 readme)，大幅提升 vim 開啟速度～ 效能我從 Pathogen 換到 Vundle 是為了可以很容易的在新環境設定好 vim，而從 Vundle 換到 vim-plug 則是為了他的 on-demand loading。 所以說到底效能差多少？其實我並不是個重度 plugins 使用者，有在用的 plugins 大概 20 個吧。所以從 Vundle 換到 vim-plug 說實在並沒有顯著效能差異。不過還是可以看看別人的實驗 不同 plugin-manager 的開啟速度(plug = vim-plug)。圖片出自 vim-plugins-and-startup-time 不難發現，有 on-demand loading 的 plugin-manager 開啟速度會快不少，以圖中為例大概都可以快上個 30% ！如果有興趣，也可以自己試試 vim 的開啟速度，可以用以下的方法測量。 1$ vim --startuptime vim.log 詳細開啟資訊都會寫入 vim.log References vim-plugins-and-startup-time vim.tw h1+ol { list-style-type: none; counter-reset: list-counter; padding-left: 0; } h1+ol li { position: relative; padding-left: 3em; } h1+ol li::before { counter-increment: list-counter; content: \"[\" counter(list-counter) \"] \"; position: absolute; left: 0; width: 25px; text-align: right; }","categories":[],"tags":[{"name":"vim","slug":"vim","permalink":"https://ssarcandy.tw/tags/vim/"}]},{"title":"用 Travis CI 自動部屬 hexo 到 GitHub","slug":"hexo-auto-deploy","date":"2016-07-28T16:51:43.000Z","updated":"2025-10-22T00:51:26.605Z","comments":true,"path":"2016/07/28/hexo-auto-deploy/","link":"","permalink":"https://ssarcandy.tw/2016/07/28/hexo-auto-deploy/","excerpt":"其實 hexo 作者 TC 已經有發過一篇文章在講這個主題了，也講得很清楚了，基本上矇著眼睛照做就行了。而這篇主要是再補充幾個細節。","text":"其實 hexo 作者 TC 已經有發過一篇文章在講這個主題了，也講得很清楚了，基本上矇著眼睛照做就行了。而這篇主要是再補充幾個細節。 SSH KEY矇著眼照做那篇，到這行: 1$ travis encrypt-file ssh_key --add 這邊會幫你上傳 ssh_key 到 Travis 上， --add 這個 flag 可以幫你插入解密指令到 .travis.yml 的 before_install 。不過這 flag 真的很機車，會把你的 .travis.yml 排版全搞亂，順便把註解刪光光！建議不要加 --add 自己手動插入解密指令，排版就不會亂掉。而且用 Windows 的人會在解密文件時莫名失敗，所以只能用 Mac 或 Unix 環境做這件事(File decryption fails on Windows)，超雷… 另外，如果因某種天災人禍導致忘記或沒辦法用指令插入解密指令，還是可以上 Travis 上的設定中看到環境變數名稱。 repository > more options 可以設定、看到 Travis 的環境變數 USE SSHTravis 是用 GitHub 的 Deploy key 來存取 repository 的，關於如何產生以及設定 Deploy key 都照著 TC 那篇文章做就可以了。如果不幸在hexo deploy時遇到錯誤如下: 12remote: Invalid username or password.fatal: Authentication failed for &quot;....&quot; 那可以檢查一下 hexo 的_config.yml deploy的部分，要用 ssh 的形式設定 repository _config.yml1234deploy: type: git repo: git@github.com:SSARCandy/ssarcandy.github.io.git branch: master SETTING UP .travis.yml我這個網站結構如下: [develop] -&gt; default branch，我在這條 branch 新增文章、修改樣式等等[master] -&gt; 放 static-files，也就是 hexo generate 出來的東東 讓 Travis 自動部屬時，Clone 的是 develop branch， 經過hexo generate後推到master branch 上，為了避免 forced-update，在.travis.yml中需要再設定一下。 附上我的 .travis.yml，基本上跟 TC 那篇 87% 像啦… title: .travis.yml123456789101112131415161718192021222324252627282930313233language: node_jsnode_js: - &quot;4&quot; before_install: # Decrypt the private key - openssl aes-256-cbc -K $encrypted_d7634ff77415_key -iv $encrypted_d7634ff77415_iv -in .travis/ssh_key.enc -out ~/.ssh/id_rsa -d # Set the permission of the key - chmod 600 ~/.ssh/id_rsa # Start SSH agent - eval $(ssh-agent) # Add the private key to the system - ssh-add ~/.ssh/id_rsa # Copy SSH config - cp .travis/ssh_config ~/.ssh/config # Set Git config - git config --global user.name &quot;ssarcandy&quot; - git config --global user.email ssarcandy@gmail.com # Install Hexo - npm install hexo -g # Clone the repository - git clone https://github.com/SSARCandy/ssarcandy.github.io .deploy_git # My static-files store on master branch - cd .deploy_git &amp;&amp; git checkout master - cd .. script: - hexo generate - hexo deploy branches: only: - develop 比較需要解說的是這段 1234# Clone the repository- git clone https://github.com/SSARCandy/ssarcandy.github.io .deploy_git- cd .deploy_git &amp;&amp; git checkout master- cd .. .deploy_git 是 hexo 會產生的資料夾，用於紀錄 git history，不過由於每次 clone 都是全新的，所以每次 .deploy_git 也都會是新的，這會導致每次更新都會是 forced-update。所以，複製一份 repo (at master branch)，並改名叫.deploy_git就是為了讓新產生出的靜態檔案可以有之前的 git history，就可以避免 forced-update。","categories":[],"tags":[{"name":"automation","slug":"automation","permalink":"https://ssarcandy.tw/tags/automation/"},{"name":"hexo","slug":"hexo","permalink":"https://ssarcandy.tw/tags/hexo/"}]},{"title":"自動分享每日天文圖(APOD)到 Dcard","slug":"bot-battle","date":"2016-07-26T16:16:45.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2016/07/26/bot-battle/","link":"","permalink":"https://ssarcandy.tw/2016/07/26/bot-battle/","excerpt":"原本其實只是要做個 Slack bot，因為我懶得每天都上 APOD 官網看…乾脆做個 bot 每天都會在中午 12:30 發今日的圖跟說明到 Slack channel 裡～","text":"原本其實只是要做個 Slack bot，因為我懶得每天都上 APOD 官網看…乾脆做個 bot 每天都會在中午 12:30 發今日的圖跟說明到 Slack channel 裡～ 自動發文到 slack channel 裡，然後還隨機搭配內容農場風格的一句話。 不過除了我自己以外好像都沒什麼人捧場，幾乎都被無視 QQDavid老師 還表示很煩要我關掉 (明明一天才 Po 一次啊啊啊 ಠ_ಠ 所以我只好忍痛取消這個 Slack bot，改成直接用 Direct message 傳給 David老師 XD 結果 David老師 也做了個每半小時就發名言佳句給我的 bot，害我只好也把我的 bot 改成每十分鐘隨機發天文圖給他，看誰比較耐得住煩～ 每半小時發給我一句名言佳句，真是世界煩。 當然這種互相毀滅的 bot 大戰持續沒多久就在雙方讓步之下都停止了….. — 以上都是前言 — 即使 Slack 上都沒人要鳥我，我還是秉持著推廣基礎天文教育的偉大理念，認為優質的每日天文圖的不應該被埋沒，要讓這樣優質的內容給更多人接觸到！最後突然想到：「不然乾脆發在 Dcard 廢版好了，那邊最多人在看了而且不受版規限制」可是懶人如我是不可能每天上去 Po 文的，所以又寫了個 bot 每天中午戳 Dcard API，發優質天文圖到廢版(廢文？ 發廢文到廢版 結果意外反應還不錯？還有人想要我整理系列文，不愧是廢版版友，就是有眼光！ 大家都很捧場，覺得溫馨 ╰(〞︶〝) ╯ 不知道要過多久才會有人會發現這都是 bot 在發文 XDD –最後自己推銷一下，由於 APOD 只有 NASA 有提供官方 API，除了英文以外其他語言的子站(像是中文版)都沒辦法簡單的取得內容，所以我寫了一個 npm 套件，專門處理取得指定日期、指定語言的 APOD 內容。Github: https://github.com/SSARCandy/node-apodDemo: http://ssarcandy.tw/node-apod/demo.html","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"},{"name":"automation","slug":"automation","permalink":"https://ssarcandy.tw/tags/automation/"}]},{"title":"Setting up OpenCV using Cmake GUI","slug":"Setting-up-OpenCV-using-Cmake-GUI","date":"2016-07-22T16:46:45.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2016/07/22/Setting-up-OpenCV-using-Cmake-GUI/","link":"","permalink":"https://ssarcandy.tw/2016/07/22/Setting-up-OpenCV-using-Cmake-GUI/","excerpt":"Build OpenCV Download OpenCV and Cmake Build opencv with cmake Cmake configuration Press configure, choose visual studio 2015, finish Then press generate","text":"Build OpenCV Download OpenCV and Cmake Build opencv with cmake Cmake configuration Press configure, choose visual studio 2015, finish Then press generate Open OpenCV.sln under build/ Build it using Debug, Release Right click > build right click &gt; build switch to Release mode and build again [Windows] Setting up environment variable add &lt;opencv&gt;/bin into PATH Add new environment variable add new env named OpenCV_DIR, value as &lt;opencv&gt;/build it may need logout to apply setting, you can check it by echo %PATH%, echo %OpenCV_DIR% Build with EXTRA MODULES In step 2. Build opencv with cmake, press configure Set up OPENCV_EXTRA_MODULES_PATH to proper path(&lt;opencv_contrib&gt;/modules) Set OPENCV_EXTRA_MODULES_PATH as opencv_contrib/modules Press configure again, then generate To see more details instructions, see opencv_contrib README Travis.yml exampletitle: travis.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061language: - cpp sudo: required compiler: - gcc # building opencv is time consuming, caching it is bettercache: apt: true ccache: true directories: - opencv-3.1.0 - opencv_contrib-3.1.0 install: # OpenCV dependencies - Details available at: http://docs.opencv.org/trunk/doc/tutorials/introduction/linux_install/linux_install.html - sudo apt-get install -y build-essential - sudo apt-get install -y cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev - sudo apt-get install -y python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev # Download v3.1.0 .zip file and extract. - curl -sL https://github.com/Itseez/opencv/archive/3.1.0.zip &gt; opencv.zip - unzip -n opencv.zip &gt; log1 # Download EXTRA MODULES and extract. - curl -sL https://github.com/Itseez/opencv_contrib/archive/3.1.0.zip &gt; opencv_contrib.zip - unzip -n opencv_contrib.zip &gt; log2 # Create a new &#x27;build&#x27; folder. - cd opencv-3.1.0 - mkdir -p build - cd build # if Makefile is cached, then skip cmake opencv # Set build instructions for Ubuntu distro. - cat Makefile &gt; l1 || cmake -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-3.1.0/modules CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_OPENGL=ON .. # if Makefile is cached, then skip make opencv # Run &#x27;make&#x27; with four threads. - cat Makefile &gt; l2 ||make -j5 &gt; make_log # Install to OS. - sudo make install # Add configuration to OpenCV to tell it where the library files are located on the file system (/usr/local/lib) - sudo sh -c &#x27;echo &quot;/usr/local/lib&quot; &gt; /etc/ld.so.conf.d/opencv.conf&#x27; - sudo ldconfig - echo &quot;OpenCV installed.&quot; # We need to return to the repo &quot;root&quot; folder, so we can then &#x27;cd&#x27; into the C++ project folder. - cd ../../ - ls -al script: - cmake CMakeLists.txt - make - &quot;./a.out &gt; log1.txt&quot;","categories":[],"tags":[{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"},{"name":"opencv","slug":"opencv","permalink":"https://ssarcandy.tw/tags/opencv/"},{"name":"cmake","slug":"cmake","permalink":"https://ssarcandy.tw/tags/cmake/"}]},{"title":"Travian 對人類行為的影響","slug":"Travian-Article-Submission","date":"2008-10-28T14:00:43.000Z","updated":"2025-10-22T00:51:26.604Z","comments":true,"path":"2008/10/28/Travian-Article-Submission/","link":"","permalink":"https://ssarcandy.tw/2008/10/28/Travian-Article-Submission/","excerpt":"Travian 會改變一個人的各種行為。 我曾經研究過一個玩 Travian 的同學，但才研究一兩個星期，自己也就跑去玩了 痾… 研究結果是： Travian 確實會影響一個人的行為並且發現影響程度大小依序是──腦子思維模式 &gt; 記憶有關事物 &gt; 算數方面 &gt; 做人處事道理 或許有些人不這麼認為，所以我將要依影響程度大小來解釋：","text":"Travian 會改變一個人的各種行為。 我曾經研究過一個玩 Travian 的同學，但才研究一兩個星期，自己也就跑去玩了 痾… 研究結果是： Travian 確實會影響一個人的行為並且發現影響程度大小依序是──腦子思維模式 &gt; 記憶有關事物 &gt; 算數方面 &gt; 做人處事道理 或許有些人不這麼認為，所以我將要依影響程度大小來解釋： 我只是想放 Travian 的廣告圖。 腦子思維模式在這裡我將舉出4個滿爆笑的實例來解釋，思維模式大大的被 Travian 影響！ 第一件事有一天，老師感嘆我們中午剩菜剩飯太多，於是開始訓話：「地球上的資源有限，你們浪費一口飯，世界上就有一個人因為沒有這一口飯而餓死！」【洞】說：「對阿！就算地球有１５０個農田＋３個雙米綠洲＋開金，也沒辦法養活六十四億的人阿。你們還一直浪費糧？！」我說：「可以去搶別人糧阿ＸＤ」【洞】馬上回話說：「搶誰？！地球上的人都沒資源拉！難道你要去搶火星阿」我：「痾痾痾，服了你…」 第二件事當歷史老師講述羅馬的時候說：「當時人民會選出兩個參議員，但實權則是在元老院中…」我無意間就說出一句話：「哇！兩隻參議員耶！好強阿」【洞】回說：「有甚麼了不起！文明點不夠就跟屁一樣！」我：「你中的TR毒一定比我嚴重…」 第三件事我們國文老師叫我們一天寫一則名言佳句，有一次……【洞】說：「今天我寫『己所不欲，勿施於人』好了」然後他就邊寫邊說：「我今天打算車掉ＸＸＸ，你認為呢？」「隨便你啦～」就這樣…到了隔天【洞】：「阿！我的名言佳句被圈起來了！？難道『己所不欲，勿施於人』不是名言佳句？」然後他就給我看我仔細看發現：「你根本亂寫嘛！這甚麼東東？『己所不欲，勿車別人』？！」 第四件事有一次老師正在談澳洲的人文地形：「在澳洲，羊是比人多的，羊比人大約是 5：1，然後…」【洞】說：「酷耶！ㄟ～不要玩Ｓ３了啦！去澳洲伺服器啦～一個人可以分到５隻羊耶！」我：「&amp;$#%@+%」(OS：你瘋了嗎?！你被 TR 洗腦了啦！) 這四件事，已經充分的顯示出，TR 對人的思維模式影響不可忽視@@ 記憶有關事物我相信大家都學過外國史吧?！那各位應該也都知道西羅馬、東羅馬帝國和日耳曼民族(條頓)吧。西元476年，日耳曼民族南下滅了西羅馬帝國，在西羅馬帝國的位子又建立了許多大大小小的王國，許多人讀到這兒就頭痛，因為有一大堆王國要背，但我跟【洞】卻不知不覺的牢牢記住了，汪達爾、東哥德、西哥德、法蘭克和盎格魯克遜人。沒甚麼特殊原因，只是因為我們Travian裡種族選的都是條頓！！ 算數方面有一天的數學小考考勾股定律，【洞】居然拿滿分！ (通常我的分數 =【洞】的分數乘 2 分之 3)我問他：「怎麼突然考滿分?」【洞】說：「因為你說我中 TR 毒很深阿！勾股定律當然會很熟練阿！」於是他開始講解：當你要計算一個座標到另一個座標的直線距離，這時就會用到上課所學的『勾股定律』，套入公式： 輕而易舉的就算出來了！然後又繼續說：等加速度公式 Travian 也用的到阿！你看喔：當你要出門，你又在 24 小時產兵中，怕自己米不夠吃 (而且是越吃越快 = = ，因為是24小時產兵…) 這時候又可以套入等加速度公式： 答案又馬上出來了！ 「酷耶！！那這樣就不用ＰＬＵＳ了，問你就好啦～」不只這樣呢！我們公民課學到的『機會成本』概念也用的到喔！於是又開始講解：當你登入之後，想必就是想管管自己的村莊吧！然而，想要把你的村莊管理好，就要做出對的選擇，每每登入，總是會有很多種綠色的按鈕，這時你就必須做出選擇，因為在同一個時刻裡，每個村都只能點一種東西。「那羅馬呢？！」我插嘴說「羅馬不算啦！他們一個人的手一定不只２隻！！讓我說完嘛！！」然後……所以這時就要利用『機會成本』的概念，若你想要全力造兵，那你就失去按資源田的機會；反之，若你想按資源田，那你造兵的數目就會變少，考慮哪個機會成本比較小，然後再選出你最想要點的那個！ 做人處事方面我曾經擔任過 s3 的一個小盟盟主，每天收發的信件都超過20封，有聯防的啦、佯攻計劃阿、討論戰術阿，或只是單純聊天打屁。如果不知道做人處事的方法，一直以為盟主可以高高在上的話……。這時【洞】接下去說：「如果你真以為盟主可以高高在上的話…當你需要幫助時，我跟其他人也一定會助你一臂之力──幫忙『落井下石』！」所以…有謙虛的態度加上時常上聯盟論壇連絡大家感情，辦事效率也就會事半功倍，而這也才是個好的聯盟。 這樣說明過後，我相信那些不相信 Travian 會影響一個人的行為的人，也都相信了吧！？","categories":[],"tags":[{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"}]}],"categories":[],"tags":[{"name":"unix","slug":"unix","permalink":"https://ssarcandy.tw/tags/unix/"},{"name":"note","slug":"note","permalink":"https://ssarcandy.tw/tags/note/"},{"name":"trashtalk","slug":"trashtalk","permalink":"https://ssarcandy.tw/tags/trashtalk/"},{"name":"automation","slug":"automation","permalink":"https://ssarcandy.tw/tags/automation/"},{"name":"c++","slug":"c","permalink":"https://ssarcandy.tw/tags/c/"},{"name":"cmake","slug":"cmake","permalink":"https://ssarcandy.tw/tags/cmake/"},{"name":"cloud","slug":"cloud","permalink":"https://ssarcandy.tw/tags/cloud/"},{"name":"docker","slug":"docker","permalink":"https://ssarcandy.tw/tags/docker/"},{"name":"slurm","slug":"slurm","permalink":"https://ssarcandy.tw/tags/slurm/"},{"name":"nodejs","slug":"nodejs","permalink":"https://ssarcandy.tw/tags/nodejs/"},{"name":"hexo","slug":"hexo","permalink":"https://ssarcandy.tw/tags/hexo/"},{"name":"javascript","slug":"javascript","permalink":"https://ssarcandy.tw/tags/javascript/"},{"name":"python","slug":"python","permalink":"https://ssarcandy.tw/tags/python/"},{"name":"pytorch","slug":"pytorch","permalink":"https://ssarcandy.tw/tags/pytorch/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://ssarcandy.tw/tags/machine-learning/"},{"name":"paper","slug":"paper","permalink":"https://ssarcandy.tw/tags/paper/"},{"name":"opencv","slug":"opencv","permalink":"https://ssarcandy.tw/tags/opencv/"},{"name":"rendering","slug":"rendering","permalink":"https://ssarcandy.tw/tags/rendering/"},{"name":"react","slug":"react","permalink":"https://ssarcandy.tw/tags/react/"},{"name":"react native","slug":"react-native","permalink":"https://ssarcandy.tw/tags/react-native/"},{"name":"pbrt","slug":"pbrt","permalink":"https://ssarcandy.tw/tags/pbrt/"},{"name":"vim","slug":"vim","permalink":"https://ssarcandy.tw/tags/vim/"}]}